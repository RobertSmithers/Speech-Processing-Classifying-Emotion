{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries\n",
    "\n",
    "For any issues running these modules, use python -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data, separate into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Building Train/Test Data ---\n",
      "Test data is 0.21% of the overall data\n",
      "Train has 1200 files.\n",
      "Test has 240 files.\n",
      "Split is 5:1 train:test\n"
     ]
    }
   ],
   "source": [
    "from fractions import Fraction\n",
    "import random\n",
    "\n",
    "path = \"./data/audio_speech_actors_01-24/\"\n",
    "actors = os.listdir(path)\n",
    "\n",
    "# We need to categorize the data files according to their emotion. Since the dataset is labelled by emotion (which is encoded into their filenames), we need to break that down\n",
    "# Filename identifiers:\n",
    "# Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
    "#\n",
    "# Vocal channel (01 = speech, 02 = song).\n",
    "#\n",
    "# Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
    "#\n",
    "# Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n",
    "#\n",
    "# Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n",
    "#\n",
    "# Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
    "#\n",
    "# Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n",
    "\n",
    "# We are only focusing on the emotion, so we categorize by the third number (01-08)\n",
    "# according to the dataset's site\n",
    "mapping = {1:\"neutral\", 2:\"calm\", 3:\"happy\", 4:\"sad\", 5:\"angry\", 6:\"fearful\", 7:\"disgust\", 8:\"surprised\"}\n",
    "\n",
    "def load_data(path, return_train_test=False, test_percentage=0.20):\n",
    "    # 1 slot for each of the emotions\n",
    "    emot = []\n",
    "    paths = []\n",
    "    train_test_labels = []\n",
    "    \n",
    "    # Custom making our train/test split\n",
    "    test_threshold = int(len(os.listdir(path)) * test_percentage + 1)    # (At least) 20% of data reserved for testing (important that we do this by actor to prevent data leakage)\n",
    "    print(\"Test data is {:0.2f}% of the overall data\".format(1 / (len(os.listdir(path))/test_threshold)))\n",
    "    \n",
    "    # Make a list of actors so we can shuffle order (last few actors will not always be test data each time we load data)\n",
    "    actors = []\n",
    "    for directory in os.listdir(path):\n",
    "        actors.append(directory)\n",
    "    random.shuffle(actors)\n",
    "    \n",
    "    count = 0\n",
    "    data_label = \"test\"\n",
    "    for directory in actors:\n",
    "        count += 1\n",
    "        if (count == test_threshold):\n",
    "            data_label = \"train\"\n",
    "        files = os.path.join(path, directory)\n",
    "        for file in os.listdir(files):\n",
    "            em_num = int(file.split(\"-\")[2])\n",
    "            emot.append(em_num)\n",
    "            train_test_labels.append(data_label)\n",
    "            paths.append(path + directory + \"/\" + file)\n",
    "    \n",
    "    tts = pd.DataFrame(train_test_labels, columns=[\"train_test\"])\n",
    "    ems = pd.DataFrame(emot, columns=['emotion']).replace(mapping)\n",
    "    pths = pd.DataFrame(paths, columns = [\"path\"])\n",
    "    data_file = pd.concat(\n",
    "        [\n",
    "            tts.reset_index(drop=True),\n",
    "            ems.reset_index(drop=True),\n",
    "            pths.reset_index(drop=True)\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    \n",
    "    if return_train_test:\n",
    "        return get_train_test(data_file)\n",
    "    return data_file\n",
    "\n",
    "def get_train_test(data):\n",
    "    grouped = data.groupby(data.train_test)\n",
    "    train = grouped.get_group(\"train\")\n",
    "    test = grouped.get_group(\"test\")\n",
    "    return train, test\n",
    "\n",
    "print(\"--- Building Train/Test Data ---\")\n",
    "data = load_data(path)\n",
    "train, test = get_train_test(data)\n",
    "train_size = train.train_test.value_counts().train\n",
    "test_size = test.train_test.value_counts().test\n",
    "print(\"Train has\", train_size, \"files.\")\n",
    "print(\"Test has\", test_size, \"files.\")\n",
    "\n",
    "ratio = Fraction(train_size, test_size)\n",
    "print(\"Split is\", str(ratio.numerator)+\":\"+str(ratio.denominator), \"train:test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# for m in mapping:\n",
    "#     for i in train[m]:\n",
    "#         X.append(i)\n",
    "#         y.append(m)\n",
    "\n",
    "def gen_mfccs(data, NUM_MFCCs=13):\n",
    "    mfccs = pd.DataFrame(columns=['mfccs'])\n",
    "    \n",
    "    # Get mfccs from each audio file\n",
    "    count=0\n",
    "    for i, j in data.iterrows():\n",
    "        for item in j.items():\n",
    "            if item[0] == 'path':\n",
    "                # Sample rate and duration taken from the kaggle dataset description\n",
    "                file, sample_rate = librosa.load(item[1], res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)\n",
    "                sample_rate = np.array(sample_rate)\n",
    "                mfcc = np.mean(librosa.feature.mfcc(y=file, sr=sample_rate, n_mfcc=NUM_MFCCs), axis=0)\n",
    "                mfccs.loc[count] = [mfcc]\n",
    "                count+=1\n",
    "                break\n",
    "    \n",
    "    # Gen list of mfccs as a dataframe to **manually** concatenate onto data\n",
    "#     mfccs = pd.DataFrame(mfccs, columns = [(\"mfcc_\" + str(num)) for num in range(len(mfccs[0]))])\n",
    "#     data = pd.concat(\n",
    "#         [\n",
    "#             data.reset_index(drop=True),\n",
    "#             mfccs.reset_index(drop=True)\n",
    "#         ],\n",
    "#         axis=1\n",
    "#     )\n",
    "\n",
    "    # Add on these mfccs to the data DataFrame\n",
    "    return pd.concat([data.reset_index(drop=True), pd.DataFrame(mfccs[\"mfccs\"].values.tolist())], axis=1)\n",
    "        \n",
    "data = gen_mfccs(data)\n",
    "data = data.fillna(0)\n",
    "train, test = get_train_test(data)\n",
    "\n",
    "# Save train + test DataFrame file as a csv\n",
    "data.to_csv(\"extracted_data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     train_test    emotion                                               path  \\\n",
      "240       train  surprised  ./data/audio_speech_actors_01-24/Actor_09/03-0...   \n",
      "241       train  surprised  ./data/audio_speech_actors_01-24/Actor_09/03-0...   \n",
      "242       train      angry  ./data/audio_speech_actors_01-24/Actor_09/03-0...   \n",
      "243       train    fearful  ./data/audio_speech_actors_01-24/Actor_09/03-0...   \n",
      "244       train    fearful  ./data/audio_speech_actors_01-24/Actor_09/03-0...   \n",
      "...         ...        ...                                                ...   \n",
      "1435      train       calm  ./data/audio_speech_actors_01-24/Actor_02/03-0...   \n",
      "1436      train    neutral  ./data/audio_speech_actors_01-24/Actor_02/03-0...   \n",
      "1437      train       calm  ./data/audio_speech_actors_01-24/Actor_02/03-0...   \n",
      "1438      train      happy  ./data/audio_speech_actors_01-24/Actor_02/03-0...   \n",
      "1439      train      happy  ./data/audio_speech_actors_01-24/Actor_02/03-0...   \n",
      "\n",
      "              0          1          2          3          4          5  \\\n",
      "240  -58.592861 -58.231144 -55.635338 -55.269249 -56.136497 -57.637722   \n",
      "241  -60.614769 -60.333462 -60.059963 -59.069801 -58.659504 -58.257420   \n",
      "242  -70.285873 -70.389305 -70.548294 -70.548294 -70.548294 -70.444122   \n",
      "243  -71.479057 -71.082336 -66.279167 -62.661621 -64.825493 -71.925446   \n",
      "244  -43.615314 -43.374554 -43.336418 -44.040428 -43.847435 -45.292446   \n",
      "...         ...        ...        ...        ...        ...        ...   \n",
      "1435 -65.707649 -65.707649 -63.114719 -61.518997 -61.097141 -63.424599   \n",
      "1436 -63.052151 -63.052151 -63.052151 -63.052151 -63.052151 -63.052151   \n",
      "1437 -63.831219 -63.831219 -63.831219 -63.350342 -63.392937 -63.535973   \n",
      "1438 -59.211617 -58.687519 -59.094669 -59.007286 -57.840801 -58.106220   \n",
      "1439 -50.220425 -50.220425 -50.399052 -50.745525 -51.077095 -50.710781   \n",
      "\n",
      "              6  ...        206        207        208        209        210  \\\n",
      "240  -58.218235  ... -56.591068 -58.085934 -53.917545 -52.794960 -53.616901   \n",
      "241  -57.767776  ... -56.123306 -56.572598 -54.151264 -51.804245 -54.117874   \n",
      "242  -68.731781  ... -50.354404 -52.920853 -53.241062 -49.917316 -52.488419   \n",
      "243  -68.969780  ... -57.389214 -59.056606 -55.987522 -57.580513 -56.954727   \n",
      "244  -41.461220  ... -47.613213 -47.598083 -49.032173 -50.849789 -49.472263   \n",
      "...         ...  ...        ...        ...        ...        ...        ...   \n",
      "1435 -63.720066  ... -38.301201 -39.792141 -40.613159 -41.209202 -41.439201   \n",
      "1436 -63.052151  ... -54.385399 -54.003391 -55.091404 -55.259026 -54.581646   \n",
      "1437 -61.527138  ... -39.106331 -40.272667 -40.879623 -38.603230 -37.423103   \n",
      "1438 -58.139366  ... -50.112942 -56.381630 -56.171310 -53.489872 -55.346500   \n",
      "1439 -49.991970  ... -49.845463 -49.453140 -49.776066 -50.220425 -50.220425   \n",
      "\n",
      "            211        212        213        214        215  \n",
      "240  -52.776859 -53.655525 -52.729752 -51.152920 -51.091022  \n",
      "241  -57.471909 -54.496956 -53.170071 -56.810947 -61.619038  \n",
      "242  -52.368870 -52.123867 -52.528561 -49.973579 -49.818020  \n",
      "243  -54.964417 -58.056065 -57.141010 -55.498817 -56.980305  \n",
      "244  -47.971760 -48.797470 -46.976933 -46.494694 -48.626308  \n",
      "...         ...        ...        ...        ...        ...  \n",
      "1435 -43.994286 -49.399620 -50.591599 -49.144051 -48.705654  \n",
      "1436 -54.561687 -59.238106 -60.861343 -54.298275 -53.452309  \n",
      "1437 -37.418514 -37.123837 -38.194134 -34.119850 -29.075195  \n",
      "1438 -55.563206 -53.164761 -52.805904 -53.068382 -51.151840  \n",
      "1439 -50.220425 -50.220425 -50.220425 -50.220425 -50.220425  \n",
      "\n",
      "[1200 rows x 219 columns]\n"
     ]
    }
   ],
   "source": [
    "def get_x_y(data):\n",
    "    rev_mapping = {emotion: num for num, emotion in mapping.items()}\n",
    "    x, y = [], []\n",
    "    for i, j in data.iterrows():\n",
    "        col = (label for label in j.items() if label[0] == 'emotion')\n",
    "        for item in col:\n",
    "            y.append(rev_mapping[item[1]])\n",
    "        count = 0\n",
    "        xs = []\n",
    "        for k in j.items():\n",
    "            if count > 5:\n",
    "                xs.append(k[1])\n",
    "            count += 1\n",
    "        x.append(xs)\n",
    "    return x, y\n",
    "\n",
    "# The numbered columns are mfccs\n",
    "print(train)\n",
    "train_x, train_y = get_x_y(train)\n",
    "test_x, test_y = get_x_y(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest frequency baseline: 0.13333333333333333\n",
      "Random baseline 0.15833333333333333\n",
      "Random uniform baseline 0.1375\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# train a dummy classifier to make predictions based on the most_frequent class value\n",
    "frequent_dummy_classifier = DummyClassifier(strategy=\"most_frequent\")\n",
    "frequent_dummy_classifier.fit(train_x, train_y)\n",
    "\n",
    "print(\"Highest frequency baseline:\", frequent_dummy_classifier.score(test_x, test_y))\n",
    "\n",
    "# train a dummy classifier to make predictions based on the class values\n",
    "stratified_dummy_classifier = DummyClassifier(strategy=\"stratified\")\n",
    "stratified_dummy_classifier.fit(train_x,train_y)\n",
    "\n",
    "print(\"Random baseline\", stratified_dummy_classifier.score(test_x, test_y))\n",
    "\n",
    "# train a dummy classifier to make predictions based on uniform selection\n",
    "uniform_dummy_classifier = DummyClassifier(strategy=\"uniform\")\n",
    "uniform_dummy_classifier.fit(train_x,train_y)\n",
    "\n",
    "print(\"Random uniform baseline\", uniform_dummy_classifier.score(test_x, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "If we want to transform the data in any way, we can do it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 213, 1)\n",
      "[[-55.26924896]\n",
      " [-56.1364975 ]\n",
      " [-57.63772202]\n",
      " [-58.21823502]\n",
      " [-58.39059067]\n",
      " [-58.59286118]\n",
      " [-58.59286118]\n",
      " [-58.59286118]\n",
      " [-58.59286118]\n",
      " [-58.59286118]\n",
      " [-58.59286118]\n",
      " [-58.59286118]\n",
      " [-58.57069397]\n",
      " [-58.59286118]\n",
      " [-58.53364944]\n",
      " [-57.45916367]\n",
      " [-58.20681381]\n",
      " [-58.59286118]\n",
      " [-58.54962158]\n",
      " [-58.44428253]\n",
      " [-58.59286118]\n",
      " [-58.34177017]\n",
      " [-58.17399597]\n",
      " [-58.59286118]\n",
      " [-58.59286118]\n",
      " [-58.59286118]\n",
      " [-58.59286118]\n",
      " [-58.59286118]\n",
      " [-58.59286118]\n",
      " [-58.59286118]\n",
      " [-58.59286118]\n",
      " [-56.78277206]\n",
      " [-52.91753769]\n",
      " [-51.94740677]\n",
      " [-51.11194611]\n",
      " [-50.75716019]\n",
      " [-53.61134338]\n",
      " [-53.16996002]\n",
      " [-53.41305923]\n",
      " [-49.80541992]\n",
      " [-44.81842804]\n",
      " [-36.13505936]\n",
      " [-31.42490387]\n",
      " [-29.70433235]\n",
      " [-28.17225075]\n",
      " [-21.76544189]\n",
      " [-20.44127655]\n",
      " [-20.49786377]\n",
      " [-21.24540138]\n",
      " [-22.37733841]\n",
      " [-24.93726921]\n",
      " [-24.83314705]\n",
      " [-25.29724884]\n",
      " [-27.66734695]\n",
      " [-30.3040905 ]\n",
      " [-30.83553696]\n",
      " [-30.53357124]\n",
      " [-30.1076622 ]\n",
      " [-28.45379639]\n",
      " [-27.68619919]\n",
      " [-26.11016846]\n",
      " [-24.06843948]\n",
      " [-23.29185104]\n",
      " [-22.97426414]\n",
      " [-22.91091537]\n",
      " [-23.29369926]\n",
      " [-24.11113167]\n",
      " [-25.62074089]\n",
      " [-27.20458603]\n",
      " [-30.69836044]\n",
      " [-30.19683456]\n",
      " [-28.20200348]\n",
      " [-27.76084709]\n",
      " [-28.58658791]\n",
      " [-29.62780762]\n",
      " [-34.27480316]\n",
      " [-34.47084808]\n",
      " [-34.09559631]\n",
      " [-33.62306595]\n",
      " [-28.51081467]\n",
      " [-27.47575951]\n",
      " [-28.98387527]\n",
      " [-29.73434448]\n",
      " [-29.93706894]\n",
      " [-30.51677322]\n",
      " [-30.91258049]\n",
      " [-30.16601181]\n",
      " [-29.11582184]\n",
      " [-27.79250145]\n",
      " [-30.65960693]\n",
      " [-35.8958168 ]\n",
      " [-36.26296616]\n",
      " [-32.50107574]\n",
      " [-31.48924065]\n",
      " [-32.85398483]\n",
      " [-30.46927643]\n",
      " [-29.80096245]\n",
      " [-29.58486176]\n",
      " [-30.06069756]\n",
      " [-29.8205452 ]\n",
      " [-28.1976757 ]\n",
      " [-27.90062714]\n",
      " [-28.66022873]\n",
      " [-29.69180107]\n",
      " [-29.12197113]\n",
      " [-28.22946358]\n",
      " [-26.56978035]\n",
      " [-27.37262154]\n",
      " [-28.38821793]\n",
      " [-26.80965805]\n",
      " [-26.47854424]\n",
      " [-27.46169662]\n",
      " [-27.40564346]\n",
      " [-28.09276009]\n",
      " [-29.27122116]\n",
      " [-29.26212883]\n",
      " [-29.31707764]\n",
      " [-28.43973541]\n",
      " [-26.8520031 ]\n",
      " [-26.11234283]\n",
      " [-26.66616821]\n",
      " [-27.58326912]\n",
      " [-27.96344948]\n",
      " [-28.67022705]\n",
      " [-28.73861885]\n",
      " [-28.23840141]\n",
      " [-30.55397606]\n",
      " [-36.56975937]\n",
      " [-40.27465439]\n",
      " [-37.78233337]\n",
      " [-31.24190521]\n",
      " [-25.65262985]\n",
      " [-24.38403702]\n",
      " [-26.37097549]\n",
      " [-27.43458939]\n",
      " [-29.36914444]\n",
      " [-29.12826538]\n",
      " [-29.64076996]\n",
      " [-31.6534729 ]\n",
      " [-34.22315979]\n",
      " [-32.0420723 ]\n",
      " [-32.2847023 ]\n",
      " [-34.05449677]\n",
      " [-36.65526581]\n",
      " [-34.87837982]\n",
      " [-35.24872971]\n",
      " [-35.81252289]\n",
      " [-34.87495422]\n",
      " [-32.0893364 ]\n",
      " [-29.42144775]\n",
      " [-29.6575737 ]\n",
      " [-30.19783783]\n",
      " [-31.26803398]\n",
      " [-32.8904953 ]\n",
      " [-33.81732559]\n",
      " [-34.02460861]\n",
      " [-35.65185928]\n",
      " [-35.99028397]\n",
      " [-37.84857941]\n",
      " [-39.11403656]\n",
      " [-40.69405746]\n",
      " [-42.07405853]\n",
      " [-43.10799789]\n",
      " [-45.20723724]\n",
      " [-47.38492584]\n",
      " [-46.37003708]\n",
      " [-47.10435486]\n",
      " [-49.95981979]\n",
      " [-52.5369873 ]\n",
      " [-51.0246315 ]\n",
      " [-51.42388535]\n",
      " [-52.8120575 ]\n",
      " [-51.63890076]\n",
      " [-49.94741058]\n",
      " [-49.1204567 ]\n",
      " [-52.84441376]\n",
      " [-56.87042999]\n",
      " [-56.17404938]\n",
      " [-56.27108383]\n",
      " [-55.79882812]\n",
      " [-48.37899017]\n",
      " [-47.43638229]\n",
      " [-53.95317841]\n",
      " [-57.30133438]\n",
      " [-57.25919724]\n",
      " [-56.32641602]\n",
      " [-54.30596924]\n",
      " [-53.2433548 ]\n",
      " [-53.43675232]\n",
      " [-53.937603  ]\n",
      " [-54.92956924]\n",
      " [-56.11120605]\n",
      " [-52.178936  ]\n",
      " [-52.10535049]\n",
      " [-55.72647476]\n",
      " [-56.49064255]\n",
      " [-56.42974854]\n",
      " [-56.61427689]\n",
      " [-50.74659729]\n",
      " [-50.15155792]\n",
      " [-54.17067719]\n",
      " [-53.35348892]\n",
      " [-55.01298523]\n",
      " [-56.59106827]\n",
      " [-58.08593369]\n",
      " [-53.91754532]\n",
      " [-52.79496002]\n",
      " [-53.6169014 ]\n",
      " [-52.77685928]\n",
      " [-53.65552521]\n",
      " [-52.72975159]\n",
      " [-51.15291977]\n",
      " [-51.09102249]]\n",
      "\n",
      "(240, 213, 1)\n",
      "[[-52.20449448]\n",
      " [-52.50223541]\n",
      " [-52.7783165 ]\n",
      " [-52.98943329]\n",
      " [-53.0833931 ]\n",
      " [-53.36600876]\n",
      " [-53.46891403]\n",
      " [-50.59867859]\n",
      " [-51.40190125]\n",
      " [-51.10347748]\n",
      " [-46.06972504]\n",
      " [-41.91774368]\n",
      " [-40.98271942]\n",
      " [-39.81606293]\n",
      " [-39.26906967]\n",
      " [-37.01068497]\n",
      " [-38.57160187]\n",
      " [-40.67051315]\n",
      " [-40.63167191]\n",
      " [-40.3306694 ]\n",
      " [-42.35848236]\n",
      " [-43.15448761]\n",
      " [-42.41799164]\n",
      " [-42.8257103 ]\n",
      " [-46.86209488]\n",
      " [-46.63039017]\n",
      " [-49.61841202]\n",
      " [-51.06899261]\n",
      " [-49.02284622]\n",
      " [-47.67755508]\n",
      " [-46.3499527 ]\n",
      " [-45.73349762]\n",
      " [-47.84318161]\n",
      " [-49.35329437]\n",
      " [-49.49411392]\n",
      " [-47.95532227]\n",
      " [-48.1895752 ]\n",
      " [-45.96852875]\n",
      " [-45.0077858 ]\n",
      " [-46.3802948 ]\n",
      " [-44.39165497]\n",
      " [-32.77241898]\n",
      " [-24.32615471]\n",
      " [-20.27746773]\n",
      " [-18.72375488]\n",
      " [-19.41149712]\n",
      " [-22.6707592 ]\n",
      " [-22.66191673]\n",
      " [-22.63798904]\n",
      " [-21.63296509]\n",
      " [-21.10053635]\n",
      " [-22.64857292]\n",
      " [-24.26882553]\n",
      " [-24.67256737]\n",
      " [-25.5150547 ]\n",
      " [-26.69074631]\n",
      " [-28.26083374]\n",
      " [-29.72010994]\n",
      " [-27.70487785]\n",
      " [-27.24476051]\n",
      " [-23.78062057]\n",
      " [-23.26311302]\n",
      " [-22.4350605 ]\n",
      " [-22.20849991]\n",
      " [-21.84436989]\n",
      " [-21.40654755]\n",
      " [-21.7756443 ]\n",
      " [-22.21507454]\n",
      " [-22.9887867 ]\n",
      " [-25.84828758]\n",
      " [-27.40151215]\n",
      " [-23.71791267]\n",
      " [-24.16706657]\n",
      " [-26.17773628]\n",
      " [-27.33635139]\n",
      " [-29.31774139]\n",
      " [-29.33927536]\n",
      " [-26.70417976]\n",
      " [-20.08214951]\n",
      " [-18.57698441]\n",
      " [-20.06525993]\n",
      " [-20.37016869]\n",
      " [-19.38028717]\n",
      " [-21.49660301]\n",
      " [-22.38012695]\n",
      " [-21.82850266]\n",
      " [-21.26249313]\n",
      " [-21.27681541]\n",
      " [-20.55985641]\n",
      " [-21.52223587]\n",
      " [-23.37828445]\n",
      " [-24.22513008]\n",
      " [-23.11114311]\n",
      " [-23.04387474]\n",
      " [-21.35421371]\n",
      " [-22.09348488]\n",
      " [-22.71844292]\n",
      " [-22.23900032]\n",
      " [-19.34211922]\n",
      " [-17.34469986]\n",
      " [-16.99800301]\n",
      " [-16.36826897]\n",
      " [-15.98243332]\n",
      " [-15.8752594 ]\n",
      " [-16.5779686 ]\n",
      " [-17.85687256]\n",
      " [-19.73803902]\n",
      " [-21.16711807]\n",
      " [-21.16537857]\n",
      " [-21.44369698]\n",
      " [-21.82665062]\n",
      " [-21.49230957]\n",
      " [-20.09532547]\n",
      " [-20.29037857]\n",
      " [-21.49684525]\n",
      " [-22.90433502]\n",
      " [-22.60004044]\n",
      " [-23.33678436]\n",
      " [-22.5892849 ]\n",
      " [-18.44217682]\n",
      " [-18.36133194]\n",
      " [-21.24436569]\n",
      " [-22.40130234]\n",
      " [-24.10302734]\n",
      " [-25.66769409]\n",
      " [-26.16678238]\n",
      " [-25.58044434]\n",
      " [-27.2121048 ]\n",
      " [-28.87298584]\n",
      " [-28.65468407]\n",
      " [-27.66060638]\n",
      " [-26.28231621]\n",
      " [-23.33052826]\n",
      " [-22.74182129]\n",
      " [-22.56816483]\n",
      " [-22.2313633 ]\n",
      " [-24.55846786]\n",
      " [-25.74703789]\n",
      " [-25.27500153]\n",
      " [-27.97887802]\n",
      " [-27.7387867 ]\n",
      " [-28.15048409]\n",
      " [-27.23407173]\n",
      " [-29.39385796]\n",
      " [-32.11245346]\n",
      " [-32.94087219]\n",
      " [-35.10944366]\n",
      " [-35.4045372 ]\n",
      " [-36.19527054]\n",
      " [-38.30382538]\n",
      " [-39.88472366]\n",
      " [-38.57190323]\n",
      " [-39.3433609 ]\n",
      " [-44.55150986]\n",
      " [-43.57261658]\n",
      " [-41.30023956]\n",
      " [-40.31433487]\n",
      " [-44.33873749]\n",
      " [-45.09305191]\n",
      " [-44.86711121]\n",
      " [-45.27843857]\n",
      " [-45.59098053]\n",
      " [-47.00373077]\n",
      " [-47.12807846]\n",
      " [-44.58223343]\n",
      " [-45.10373688]\n",
      " [-47.53569031]\n",
      " [-50.20245743]\n",
      " [-47.81142426]\n",
      " [-45.7655983 ]\n",
      " [-45.5005722 ]\n",
      " [-45.3756752 ]\n",
      " [-47.42670441]\n",
      " [-47.59126663]\n",
      " [-48.2165184 ]\n",
      " [-51.44116974]\n",
      " [-49.71770859]\n",
      " [-46.47333145]\n",
      " [-48.12944031]\n",
      " [-48.23997116]\n",
      " [-47.84924698]\n",
      " [-49.43896484]\n",
      " [-49.30562592]\n",
      " [-48.8910675 ]\n",
      " [-47.194767  ]\n",
      " [-49.07958603]\n",
      " [-47.40952301]\n",
      " [-47.42061615]\n",
      " [-46.49908447]\n",
      " [-47.75415421]\n",
      " [-49.04636383]\n",
      " [-49.82714081]\n",
      " [-50.31256104]\n",
      " [-51.90095901]\n",
      " [-51.76489639]\n",
      " [-49.47477722]\n",
      " [-50.78200912]\n",
      " [-51.52988815]\n",
      " [-50.30321503]\n",
      " [-51.18655014]\n",
      " [-52.21738052]\n",
      " [-51.86761856]\n",
      " [-52.00451279]\n",
      " [-51.00328064]\n",
      " [-51.01125717]\n",
      " [-52.28501129]\n",
      " [-54.94282532]\n",
      " [-51.73024368]\n",
      " [-50.45843124]\n",
      " [-51.64995956]\n",
      " [-52.56546783]\n",
      " [-54.60706329]\n",
      " [-57.2851181 ]]\n"
     ]
    }
   ],
   "source": [
    "train_length = len(train_x[0])\n",
    "train_x = np.array(train_x).reshape(-1,train_length,1)\n",
    "print(np.shape(train_x))\n",
    "print(train_x[0])\n",
    "print()\n",
    "\n",
    "\n",
    "test_length = len(test_x[0])\n",
    "test_x = np.array(test_x).reshape(-1,test_length,1)\n",
    "print(np.shape(test_x))\n",
    "print(test_x[0])\n",
    "\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# le = LabelEncoder()\n",
    "\n",
    "\n",
    "# # \n",
    "# # Generates an int for each label\n",
    "# y=le.fit_transform(labels)\n",
    "\n",
    "# # Prints out each date with its int mapping\n",
    "# for c in list(le.classes_):\n",
    "#     print(le.transform([c])[0], c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 neutral\n",
      "[0 0 0 0 0 0 0 1] neutral\n",
      "7 neutral\n",
      "[0 0 0 0 0 0 0 1] neutral\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.utils import np_utils\n",
    "\n",
    "labels=[\"neutral\", \"calm\", \"happy\", \"sad\", \"angry\", \"fearful\", \"disgust\", \"surprised\"]\n",
    "\n",
    "train_y = [val-1 for val in train_y]\n",
    "train_y = [int(val) for val in train_y]\n",
    "print(train_y[0], labels[0])\n",
    "train_y=np_utils.to_categorical(train_y, num_classes=len(labels), dtype=np.int32)\n",
    "print(train_y[0], labels[0])\n",
    "\n",
    "test_y = [val-1 for val in test_y]\n",
    "print(test_y[0], labels[0])\n",
    "test_y=np_utils.to_categorical(test_y, num_classes=len(labels), dtype=np.int32)\n",
    "print(test_y[0], labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Flatten, Conv1D, Input, MaxPooling1D, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 213, 1)]          0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 201, 8)            112       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 201, 8)            32        \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 67, 8)             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 67, 8)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 57, 16)            1424      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 19, 16)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 19, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 11, 32)            4640      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 3, 32)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 3, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                6208      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 520       \n",
      "=================================================================\n",
      "Total params: 12,936\n",
      "Trainable params: 12,920\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "\n",
    "inputs = Input(shape=(train_length,1))\n",
    "\n",
    "#First Conv1D layer\n",
    "conv = Conv1D(8, 13, padding='valid', activation='relu', strides=1, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))(inputs)\n",
    "conv = BatchNormalization()(conv)\n",
    "conv = MaxPooling1D(3)(conv)\n",
    "conv = Dropout(0.3)(conv)\n",
    "\n",
    "#Second Conv1D layer\n",
    "conv = Conv1D(16, 11, padding='valid', activation='relu', strides=1, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))(conv)\n",
    "conv = MaxPooling1D(3)(conv)\n",
    "conv = Dropout(0.3)(conv)\n",
    "\n",
    "#Third Conv1D layer\n",
    "conv = Conv1D(32, 9, padding='valid', activation='relu', strides=1, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))(conv)\n",
    "conv = MaxPooling1D(3)(conv)\n",
    "conv = Dropout(0.3)(conv)\n",
    "\n",
    "#Flatten layer\n",
    "conv = Flatten()(conv)\n",
    "\n",
    "#Dense Layer 1\n",
    "conv = Dense(64, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))(conv)\n",
    "conv = Dropout(0.3)(conv)\n",
    "\n",
    "# conv = BatchNormalization()(conv)\n",
    "\n",
    "# Output layer\n",
    "outputs = Dense(len(labels), activation='softmax')(conv)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20, min_delta=0.0001) \n",
    "mc = ModelCheckpoint('best_model.hdf5', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 3.4769 - accuracy: 0.1135\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.10417, saving model to best_model.hdf5\n",
      "38/38 [==============================] - 0s 11ms/step - loss: 3.4507 - accuracy: 0.1192 - val_loss: 3.2794 - val_accuracy: 0.1042\n",
      "Epoch 2/500\n",
      "28/38 [=====================>........] - ETA: 0s - loss: 3.3094 - accuracy: 0.1228\n",
      "Epoch 00002: val_accuracy did not improve from 0.10417\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 3.3068 - accuracy: 0.1250 - val_loss: 3.1833 - val_accuracy: 0.0958\n",
      "Epoch 3/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 3.2413 - accuracy: 0.1364\n",
      "Epoch 00003: val_accuracy improved from 0.10417 to 0.12500, saving model to best_model.hdf5\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 3.2368 - accuracy: 0.1367 - val_loss: 3.1426 - val_accuracy: 0.1250\n",
      "Epoch 4/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 3.1982 - accuracy: 0.1321\n",
      "Epoch 00004: val_accuracy did not improve from 0.12500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 3.1891 - accuracy: 0.1350 - val_loss: 3.1185 - val_accuracy: 0.1250\n",
      "Epoch 5/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 3.1675 - accuracy: 0.1326\n",
      "Epoch 00005: val_accuracy improved from 0.12500 to 0.12917, saving model to best_model.hdf5\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 3.1584 - accuracy: 0.1383 - val_loss: 3.0981 - val_accuracy: 0.1292\n",
      "Epoch 6/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 3.1398 - accuracy: 0.1406\n",
      "Epoch 00006: val_accuracy improved from 0.12917 to 0.13333, saving model to best_model.hdf5\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 3.1326 - accuracy: 0.1450 - val_loss: 3.0780 - val_accuracy: 0.1333\n",
      "Epoch 7/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 3.1026 - accuracy: 0.1391\n",
      "Epoch 00007: val_accuracy improved from 0.13333 to 0.18333, saving model to best_model.hdf5\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 3.1034 - accuracy: 0.1450 - val_loss: 3.0582 - val_accuracy: 0.1833\n",
      "Epoch 8/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 3.0978 - accuracy: 0.1341\n",
      "Epoch 00008: val_accuracy improved from 0.18333 to 0.18750, saving model to best_model.hdf5\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 3.0939 - accuracy: 0.1325 - val_loss: 3.0411 - val_accuracy: 0.1875\n",
      "Epoch 9/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 3.0658 - accuracy: 0.1411\n",
      "Epoch 00009: val_accuracy improved from 0.18750 to 0.20833, saving model to best_model.hdf5\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 3.0653 - accuracy: 0.1392 - val_loss: 3.0242 - val_accuracy: 0.2083\n",
      "Epoch 10/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 3.0400 - accuracy: 0.1533\n",
      "Epoch 00010: val_accuracy did not improve from 0.20833\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 3.0448 - accuracy: 0.1442 - val_loss: 3.0083 - val_accuracy: 0.1958\n",
      "Epoch 11/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 3.0429 - accuracy: 0.1436\n",
      "Epoch 00011: val_accuracy improved from 0.20833 to 0.21250, saving model to best_model.hdf5\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 3.0348 - accuracy: 0.1450 - val_loss: 2.9935 - val_accuracy: 0.2125\n",
      "Epoch 12/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 3.0008 - accuracy: 0.1411\n",
      "Epoch 00012: val_accuracy did not improve from 0.21250\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 3.0044 - accuracy: 0.1367 - val_loss: 2.9775 - val_accuracy: 0.2125\n",
      "Epoch 13/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 2.9773 - accuracy: 0.1543\n",
      "Epoch 00013: val_accuracy improved from 0.21250 to 0.22083, saving model to best_model.hdf5\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.9756 - accuracy: 0.1583 - val_loss: 2.9617 - val_accuracy: 0.2208\n",
      "Epoch 14/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 2.9620 - accuracy: 0.1553\n",
      "Epoch 00014: val_accuracy did not improve from 0.22083\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.9613 - accuracy: 0.1542 - val_loss: 2.9457 - val_accuracy: 0.2042\n",
      "Epoch 15/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 2.9490 - accuracy: 0.1590\n",
      "Epoch 00015: val_accuracy did not improve from 0.22083\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.9510 - accuracy: 0.1575 - val_loss: 2.9290 - val_accuracy: 0.2042\n",
      "Epoch 16/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 2.9314 - accuracy: 0.1593\n",
      "Epoch 00016: val_accuracy did not improve from 0.22083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.9311 - accuracy: 0.1567 - val_loss: 2.9120 - val_accuracy: 0.2000\n",
      "Epoch 17/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 2.9110 - accuracy: 0.1621\n",
      "Epoch 00017: val_accuracy did not improve from 0.22083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.9129 - accuracy: 0.1633 - val_loss: 2.8954 - val_accuracy: 0.2125\n",
      "Epoch 18/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 2.9005 - accuracy: 0.1581\n",
      "Epoch 00018: val_accuracy did not improve from 0.22083\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.9016 - accuracy: 0.1583 - val_loss: 2.8784 - val_accuracy: 0.2042\n",
      "Epoch 19/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 2.8726 - accuracy: 0.1709\n",
      "Epoch 00019: val_accuracy did not improve from 0.22083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.8744 - accuracy: 0.1692 - val_loss: 2.8616 - val_accuracy: 0.2042\n",
      "Epoch 20/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 2.8641 - accuracy: 0.1645\n",
      "Epoch 00020: val_accuracy did not improve from 0.22083\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.8585 - accuracy: 0.1700 - val_loss: 2.8468 - val_accuracy: 0.2083\n",
      "Epoch 21/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 2.8384 - accuracy: 0.1673\n",
      "Epoch 00021: val_accuracy did not improve from 0.22083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.8432 - accuracy: 0.1675 - val_loss: 2.8295 - val_accuracy: 0.2083\n",
      "Epoch 22/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 2.8214 - accuracy: 0.1943\n",
      "Epoch 00022: val_accuracy did not improve from 0.22083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.8199 - accuracy: 0.1917 - val_loss: 2.8144 - val_accuracy: 0.2042\n",
      "Epoch 23/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 2.8094 - accuracy: 0.1847\n",
      "Epoch 00023: val_accuracy did not improve from 0.22083\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.8105 - accuracy: 0.1817 - val_loss: 2.7989 - val_accuracy: 0.2083\n",
      "Epoch 24/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 2.7980 - accuracy: 0.1915\n",
      "Epoch 00024: val_accuracy did not improve from 0.22083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.7962 - accuracy: 0.1942 - val_loss: 2.7848 - val_accuracy: 0.2042\n",
      "Epoch 25/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 2.7733 - accuracy: 0.1963\n",
      "Epoch 00025: val_accuracy did not improve from 0.22083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.7692 - accuracy: 0.1925 - val_loss: 2.7701 - val_accuracy: 0.1958\n",
      "Epoch 26/500\n",
      "29/38 [=====================>........] - ETA: 0s - loss: 2.7692 - accuracy: 0.1800\n",
      "Epoch 00026: val_accuracy did not improve from 0.22083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.7677 - accuracy: 0.1767 - val_loss: 2.7553 - val_accuracy: 0.1917\n",
      "Epoch 27/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 2.7470 - accuracy: 0.1996\n",
      "Epoch 00027: val_accuracy did not improve from 0.22083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.7349 - accuracy: 0.2075 - val_loss: 2.7418 - val_accuracy: 0.1917\n",
      "Epoch 28/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 2.7245 - accuracy: 0.1943\n",
      "Epoch 00028: val_accuracy did not improve from 0.22083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.7245 - accuracy: 0.2000 - val_loss: 2.7259 - val_accuracy: 0.2083\n",
      "Epoch 29/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/38 [=======================>......] - ETA: 0s - loss: 2.7330 - accuracy: 0.1885\n",
      "Epoch 00029: val_accuracy did not improve from 0.22083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.7242 - accuracy: 0.1900 - val_loss: 2.7124 - val_accuracy: 0.2125\n",
      "Epoch 30/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 2.7103 - accuracy: 0.1836\n",
      "Epoch 00030: val_accuracy did not improve from 0.22083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.7137 - accuracy: 0.1867 - val_loss: 2.6989 - val_accuracy: 0.2167\n",
      "Epoch 31/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 2.7030 - accuracy: 0.1884\n",
      "Epoch 00031: val_accuracy did not improve from 0.22083\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.6999 - accuracy: 0.1875 - val_loss: 2.6856 - val_accuracy: 0.2000\n",
      "Epoch 32/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 2.6812 - accuracy: 0.1680\n",
      "Epoch 00032: val_accuracy did not improve from 0.22083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.6776 - accuracy: 0.1733 - val_loss: 2.6737 - val_accuracy: 0.2208\n",
      "Epoch 33/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 2.6675 - accuracy: 0.1846\n",
      "Epoch 00033: val_accuracy did not improve from 0.22083\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.6755 - accuracy: 0.1792 - val_loss: 2.6593 - val_accuracy: 0.1958\n",
      "Epoch 34/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 2.6564 - accuracy: 0.2077\n",
      "Epoch 00034: val_accuracy did not improve from 0.22083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.6517 - accuracy: 0.2150 - val_loss: 2.6454 - val_accuracy: 0.1833\n",
      "Epoch 35/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 2.6220 - accuracy: 0.2070\n",
      "Epoch 00035: val_accuracy did not improve from 0.22083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.6256 - accuracy: 0.2017 - val_loss: 2.6315 - val_accuracy: 0.1792\n",
      "Epoch 36/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 2.6291 - accuracy: 0.2036\n",
      "Epoch 00036: val_accuracy did not improve from 0.22083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.6182 - accuracy: 0.2142 - val_loss: 2.6195 - val_accuracy: 0.1667\n",
      "Epoch 37/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 2.6074 - accuracy: 0.1979\n",
      "Epoch 00037: val_accuracy did not improve from 0.22083\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.6090 - accuracy: 0.1975 - val_loss: 2.6070 - val_accuracy: 0.1667\n",
      "Epoch 38/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 2.5951 - accuracy: 0.2017\n",
      "Epoch 00038: val_accuracy did not improve from 0.22083\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.5954 - accuracy: 0.1983 - val_loss: 2.5950 - val_accuracy: 0.1625\n",
      "Epoch 39/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 2.5868 - accuracy: 0.2148\n",
      "Epoch 00039: val_accuracy did not improve from 0.22083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.5858 - accuracy: 0.2192 - val_loss: 2.5830 - val_accuracy: 0.1792\n",
      "Epoch 40/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 2.5674 - accuracy: 0.2177\n",
      "Epoch 00040: val_accuracy did not improve from 0.22083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.5692 - accuracy: 0.2117 - val_loss: 2.5704 - val_accuracy: 0.1917\n",
      "Epoch 41/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 2.5667 - accuracy: 0.2070\n",
      "Epoch 00041: val_accuracy did not improve from 0.22083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.5766 - accuracy: 0.1917 - val_loss: 2.5603 - val_accuracy: 0.1708\n",
      "Epoch 42/500\n",
      "38/38 [==============================] - ETA: 0s - loss: 2.5482 - accuracy: 0.2142\n",
      "Epoch 00042: val_accuracy did not improve from 0.22083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.5482 - accuracy: 0.2142 - val_loss: 2.5501 - val_accuracy: 0.1792\n",
      "Epoch 43/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 2.5586 - accuracy: 0.2074\n",
      "Epoch 00043: val_accuracy did not improve from 0.22083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.5501 - accuracy: 0.2117 - val_loss: 2.5403 - val_accuracy: 0.1792\n",
      "Epoch 44/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 2.5268 - accuracy: 0.2016\n",
      "Epoch 00044: val_accuracy did not improve from 0.22083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.5316 - accuracy: 0.2000 - val_loss: 2.5293 - val_accuracy: 0.1917\n",
      "Epoch 45/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 2.5262 - accuracy: 0.2177\n",
      "Epoch 00045: val_accuracy did not improve from 0.22083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.5190 - accuracy: 0.2142 - val_loss: 2.5205 - val_accuracy: 0.2125\n",
      "Epoch 46/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 2.4944 - accuracy: 0.2026\n",
      "Epoch 00046: val_accuracy did not improve from 0.22083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.4958 - accuracy: 0.2100 - val_loss: 2.5089 - val_accuracy: 0.2083\n",
      "Epoch 47/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 2.5041 - accuracy: 0.2117\n",
      "Epoch 00047: val_accuracy did not improve from 0.22083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.4979 - accuracy: 0.2050 - val_loss: 2.4982 - val_accuracy: 0.2000\n",
      "Epoch 48/500\n",
      "37/38 [============================>.] - ETA: 0s - loss: 2.4948 - accuracy: 0.2179\n",
      "Epoch 00048: val_accuracy did not improve from 0.22083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.4963 - accuracy: 0.2167 - val_loss: 2.4901 - val_accuracy: 0.2167\n",
      "Epoch 49/500\n",
      "37/38 [============================>.] - ETA: 0s - loss: 2.4792 - accuracy: 0.2348\n",
      "Epoch 00049: val_accuracy did not improve from 0.22083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.4801 - accuracy: 0.2333 - val_loss: 2.4818 - val_accuracy: 0.2167\n",
      "Epoch 50/500\n",
      "37/38 [============================>.] - ETA: 0s - loss: 2.4538 - accuracy: 0.2171\n",
      "Epoch 00050: val_accuracy improved from 0.22083 to 0.22917, saving model to best_model.hdf5\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 2.4530 - accuracy: 0.2158 - val_loss: 2.4716 - val_accuracy: 0.2292\n",
      "Epoch 51/500\n",
      "29/38 [=====================>........] - ETA: 0s - loss: 2.4541 - accuracy: 0.2166\n",
      "Epoch 00051: val_accuracy did not improve from 0.22917\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.4613 - accuracy: 0.2183 - val_loss: 2.4621 - val_accuracy: 0.2167\n",
      "Epoch 52/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 2.4395 - accuracy: 0.2349\n",
      "Epoch 00052: val_accuracy did not improve from 0.22917\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.4373 - accuracy: 0.2342 - val_loss: 2.4531 - val_accuracy: 0.2083\n",
      "Epoch 53/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 2.4465 - accuracy: 0.2127\n",
      "Epoch 00053: val_accuracy did not improve from 0.22917\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.4357 - accuracy: 0.2217 - val_loss: 2.4442 - val_accuracy: 0.2167\n",
      "Epoch 54/500\n",
      "38/38 [==============================] - ETA: 0s - loss: 2.4290 - accuracy: 0.2325\n",
      "Epoch 00054: val_accuracy did not improve from 0.22917\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 2.4290 - accuracy: 0.2325 - val_loss: 2.4361 - val_accuracy: 0.2125\n",
      "Epoch 55/500\n",
      "29/38 [=====================>........] - ETA: 0s - loss: 2.4347 - accuracy: 0.2091\n",
      "Epoch 00055: val_accuracy did not improve from 0.22917\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.4360 - accuracy: 0.2083 - val_loss: 2.4286 - val_accuracy: 0.2292\n",
      "Epoch 56/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 2.4213 - accuracy: 0.2168\n",
      "Epoch 00056: val_accuracy did not improve from 0.22917\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.4223 - accuracy: 0.2183 - val_loss: 2.4194 - val_accuracy: 0.2292\n",
      "Epoch 57/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 2.3950 - accuracy: 0.2480\n",
      "Epoch 00057: val_accuracy did not improve from 0.22917\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.3964 - accuracy: 0.2417 - val_loss: 2.4115 - val_accuracy: 0.2292\n",
      "Epoch 58/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - ETA: 0s - loss: 2.3952 - accuracy: 0.2233\n",
      "Epoch 00058: val_accuracy improved from 0.22917 to 0.23333, saving model to best_model.hdf5\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 2.3952 - accuracy: 0.2233 - val_loss: 2.4043 - val_accuracy: 0.2333\n",
      "Epoch 59/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 2.3793 - accuracy: 0.2461\n",
      "Epoch 00059: val_accuracy improved from 0.23333 to 0.24583, saving model to best_model.hdf5\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 2.3790 - accuracy: 0.2500 - val_loss: 2.3968 - val_accuracy: 0.2458\n",
      "Epoch 60/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 2.3702 - accuracy: 0.2389\n",
      "Epoch 00060: val_accuracy improved from 0.24583 to 0.25833, saving model to best_model.hdf5\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 2.3780 - accuracy: 0.2317 - val_loss: 2.3889 - val_accuracy: 0.2583\n",
      "Epoch 61/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 2.3617 - accuracy: 0.2339\n",
      "Epoch 00061: val_accuracy did not improve from 0.25833\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.3602 - accuracy: 0.2358 - val_loss: 2.3827 - val_accuracy: 0.2583\n",
      "Epoch 62/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 2.3673 - accuracy: 0.2298\n",
      "Epoch 00062: val_accuracy did not improve from 0.25833\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.3604 - accuracy: 0.2308 - val_loss: 2.3743 - val_accuracy: 0.2542\n",
      "Epoch 63/500\n",
      "38/38 [==============================] - ETA: 0s - loss: 2.3543 - accuracy: 0.2292\n",
      "Epoch 00063: val_accuracy did not improve from 0.25833\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 2.3543 - accuracy: 0.2292 - val_loss: 2.3677 - val_accuracy: 0.2417\n",
      "Epoch 64/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 2.3449 - accuracy: 0.2344\n",
      "Epoch 00064: val_accuracy did not improve from 0.25833\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.3424 - accuracy: 0.2308 - val_loss: 2.3610 - val_accuracy: 0.2458\n",
      "Epoch 65/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 2.3293 - accuracy: 0.2529\n",
      "Epoch 00065: val_accuracy did not improve from 0.25833\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.3306 - accuracy: 0.2492 - val_loss: 2.3531 - val_accuracy: 0.2292\n",
      "Epoch 66/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 2.3371 - accuracy: 0.2454\n",
      "Epoch 00066: val_accuracy did not improve from 0.25833\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.3331 - accuracy: 0.2450 - val_loss: 2.3486 - val_accuracy: 0.2333\n",
      "Epoch 67/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 2.3289 - accuracy: 0.2396\n",
      "Epoch 00067: val_accuracy did not improve from 0.25833\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.3255 - accuracy: 0.2375 - val_loss: 2.3420 - val_accuracy: 0.2458\n",
      "Epoch 68/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 2.3202 - accuracy: 0.2472\n",
      "Epoch 00068: val_accuracy did not improve from 0.25833\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.3157 - accuracy: 0.2533 - val_loss: 2.3352 - val_accuracy: 0.2417\n",
      "Epoch 69/500\n",
      "29/38 [=====================>........] - ETA: 0s - loss: 2.3140 - accuracy: 0.2295\n",
      "Epoch 00069: val_accuracy did not improve from 0.25833\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.3102 - accuracy: 0.2258 - val_loss: 2.3318 - val_accuracy: 0.2500\n",
      "Epoch 70/500\n",
      "38/38 [==============================] - ETA: 0s - loss: 2.3036 - accuracy: 0.2508\n",
      "Epoch 00070: val_accuracy did not improve from 0.25833\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.3036 - accuracy: 0.2508 - val_loss: 2.3272 - val_accuracy: 0.2500\n",
      "Epoch 71/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 2.2879 - accuracy: 0.2228\n",
      "Epoch 00071: val_accuracy did not improve from 0.25833\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.2749 - accuracy: 0.2408 - val_loss: 2.3177 - val_accuracy: 0.2500\n",
      "Epoch 72/500\n",
      "38/38 [==============================] - ETA: 0s - loss: 2.2767 - accuracy: 0.2650\n",
      "Epoch 00072: val_accuracy did not improve from 0.25833\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.2767 - accuracy: 0.2650 - val_loss: 2.3093 - val_accuracy: 0.2417\n",
      "Epoch 73/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 2.2527 - accuracy: 0.2601\n",
      "Epoch 00073: val_accuracy did not improve from 0.25833\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.2597 - accuracy: 0.2583 - val_loss: 2.2990 - val_accuracy: 0.2458\n",
      "Epoch 74/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 2.2586 - accuracy: 0.2782\n",
      "Epoch 00074: val_accuracy did not improve from 0.25833\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.2618 - accuracy: 0.2758 - val_loss: 2.2924 - val_accuracy: 0.2458\n",
      "Epoch 75/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 2.2705 - accuracy: 0.2289\n",
      "Epoch 00075: val_accuracy did not improve from 0.25833\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.2722 - accuracy: 0.2258 - val_loss: 2.2852 - val_accuracy: 0.2542\n",
      "Epoch 76/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 2.2525 - accuracy: 0.2540\n",
      "Epoch 00076: val_accuracy did not improve from 0.25833\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.2516 - accuracy: 0.2550 - val_loss: 2.2830 - val_accuracy: 0.2542\n",
      "Epoch 77/500\n",
      "37/38 [============================>.] - ETA: 0s - loss: 2.2529 - accuracy: 0.2525\n",
      "Epoch 00077: val_accuracy did not improve from 0.25833\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.2484 - accuracy: 0.2542 - val_loss: 2.2781 - val_accuracy: 0.2542\n",
      "Epoch 78/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 2.2367 - accuracy: 0.2573\n",
      "Epoch 00078: val_accuracy did not improve from 0.25833\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.2349 - accuracy: 0.2592 - val_loss: 2.2698 - val_accuracy: 0.2583\n",
      "Epoch 79/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 2.2158 - accuracy: 0.2635\n",
      "Epoch 00079: val_accuracy did not improve from 0.25833\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.2182 - accuracy: 0.2592 - val_loss: 2.2673 - val_accuracy: 0.2500\n",
      "Epoch 80/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 2.2244 - accuracy: 0.2581\n",
      "Epoch 00080: val_accuracy did not improve from 0.25833\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.2308 - accuracy: 0.2475 - val_loss: 2.2612 - val_accuracy: 0.2500\n",
      "Epoch 81/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 2.2007 - accuracy: 0.2715\n",
      "Epoch 00081: val_accuracy did not improve from 0.25833\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.2184 - accuracy: 0.2625 - val_loss: 2.2570 - val_accuracy: 0.2542\n",
      "Epoch 82/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 2.2177 - accuracy: 0.2520\n",
      "Epoch 00082: val_accuracy improved from 0.25833 to 0.26250, saving model to best_model.hdf5\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.2148 - accuracy: 0.2525 - val_loss: 2.2526 - val_accuracy: 0.2625\n",
      "Epoch 83/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 2.2233 - accuracy: 0.2509\n",
      "Epoch 00083: val_accuracy did not improve from 0.26250\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.2161 - accuracy: 0.2533 - val_loss: 2.2482 - val_accuracy: 0.2625\n",
      "Epoch 84/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 2.2016 - accuracy: 0.2631\n",
      "Epoch 00084: val_accuracy did not improve from 0.26250\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.2002 - accuracy: 0.2642 - val_loss: 2.2379 - val_accuracy: 0.2583\n",
      "Epoch 85/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 2.1919 - accuracy: 0.2739\n",
      "Epoch 00085: val_accuracy did not improve from 0.26250\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.1919 - accuracy: 0.2717 - val_loss: 2.2320 - val_accuracy: 0.2583\n",
      "Epoch 86/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 2.2081 - accuracy: 0.2730\n",
      "Epoch 00086: val_accuracy improved from 0.26250 to 0.26667, saving model to best_model.hdf5\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.2092 - accuracy: 0.2700 - val_loss: 2.2218 - val_accuracy: 0.2667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 2.1945 - accuracy: 0.2638\n",
      "Epoch 00087: val_accuracy improved from 0.26667 to 0.27083, saving model to best_model.hdf5\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.1879 - accuracy: 0.2675 - val_loss: 2.2203 - val_accuracy: 0.2708\n",
      "Epoch 88/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 2.1661 - accuracy: 0.2619\n",
      "Epoch 00088: val_accuracy did not improve from 0.27083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.1647 - accuracy: 0.2608 - val_loss: 2.2118 - val_accuracy: 0.2625\n",
      "Epoch 89/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 2.1702 - accuracy: 0.2623\n",
      "Epoch 00089: val_accuracy did not improve from 0.27083\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.1795 - accuracy: 0.2567 - val_loss: 2.2080 - val_accuracy: 0.2625\n",
      "Epoch 90/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 2.1659 - accuracy: 0.2562\n",
      "Epoch 00090: val_accuracy did not improve from 0.27083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.1709 - accuracy: 0.2583 - val_loss: 2.2017 - val_accuracy: 0.2667\n",
      "Epoch 91/500\n",
      "28/38 [=====================>........] - ETA: 0s - loss: 2.1689 - accuracy: 0.2533\n",
      "Epoch 00091: val_accuracy did not improve from 0.27083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.1626 - accuracy: 0.2558 - val_loss: 2.2010 - val_accuracy: 0.2625\n",
      "Epoch 92/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 2.1645 - accuracy: 0.2764\n",
      "Epoch 00092: val_accuracy did not improve from 0.27083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.1613 - accuracy: 0.2742 - val_loss: 2.1966 - val_accuracy: 0.2583\n",
      "Epoch 93/500\n",
      "36/38 [===========================>..] - ETA: 0s - loss: 2.1376 - accuracy: 0.2847\n",
      "Epoch 00093: val_accuracy did not improve from 0.27083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.1407 - accuracy: 0.2808 - val_loss: 2.1897 - val_accuracy: 0.2625\n",
      "Epoch 94/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 2.1547 - accuracy: 0.2831\n",
      "Epoch 00094: val_accuracy did not improve from 0.27083\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.1481 - accuracy: 0.2825 - val_loss: 2.1861 - val_accuracy: 0.2708\n",
      "Epoch 95/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 2.1248 - accuracy: 0.2782\n",
      "Epoch 00095: val_accuracy did not improve from 0.27083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.1217 - accuracy: 0.2833 - val_loss: 2.1795 - val_accuracy: 0.2625\n",
      "Epoch 96/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 2.1275 - accuracy: 0.2823\n",
      "Epoch 00096: val_accuracy improved from 0.27083 to 0.27917, saving model to best_model.hdf5\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.1360 - accuracy: 0.2792 - val_loss: 2.1593 - val_accuracy: 0.2792\n",
      "Epoch 97/500\n",
      "29/38 [=====================>........] - ETA: 0s - loss: 2.1152 - accuracy: 0.2899\n",
      "Epoch 00097: val_accuracy did not improve from 0.27917\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.1268 - accuracy: 0.2842 - val_loss: 2.1619 - val_accuracy: 0.2792\n",
      "Epoch 98/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 2.1155 - accuracy: 0.2746\n",
      "Epoch 00098: val_accuracy did not improve from 0.27917\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.1117 - accuracy: 0.2783 - val_loss: 2.1571 - val_accuracy: 0.2667\n",
      "Epoch 99/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 2.1208 - accuracy: 0.2871\n",
      "Epoch 00099: val_accuracy did not improve from 0.27917\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.1214 - accuracy: 0.2783 - val_loss: 2.1470 - val_accuracy: 0.2792\n",
      "Epoch 100/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 2.1298 - accuracy: 0.2891\n",
      "Epoch 00100: val_accuracy did not improve from 0.27917\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.1268 - accuracy: 0.2842 - val_loss: 2.1566 - val_accuracy: 0.2750\n",
      "Epoch 101/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 2.1138 - accuracy: 0.2702\n",
      "Epoch 00101: val_accuracy did not improve from 0.27917\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.1042 - accuracy: 0.2800 - val_loss: 2.1427 - val_accuracy: 0.2750\n",
      "Epoch 102/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 2.1042 - accuracy: 0.2923\n",
      "Epoch 00102: val_accuracy did not improve from 0.27917\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.0970 - accuracy: 0.2883 - val_loss: 2.1386 - val_accuracy: 0.2708\n",
      "Epoch 103/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 2.0999 - accuracy: 0.2853\n",
      "Epoch 00103: val_accuracy did not improve from 0.27917\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.0994 - accuracy: 0.2792 - val_loss: 2.1343 - val_accuracy: 0.2625\n",
      "Epoch 104/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 2.1023 - accuracy: 0.2746\n",
      "Epoch 00104: val_accuracy did not improve from 0.27917\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.1024 - accuracy: 0.2808 - val_loss: 2.1347 - val_accuracy: 0.2667\n",
      "Epoch 105/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 2.0646 - accuracy: 0.2904\n",
      "Epoch 00105: val_accuracy did not improve from 0.27917\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.0640 - accuracy: 0.2858 - val_loss: 2.1379 - val_accuracy: 0.2667\n",
      "Epoch 106/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 2.0647 - accuracy: 0.3068\n",
      "Epoch 00106: val_accuracy did not improve from 0.27917\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.0715 - accuracy: 0.3067 - val_loss: 2.1249 - val_accuracy: 0.2417\n",
      "Epoch 107/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 2.0821 - accuracy: 0.2979\n",
      "Epoch 00107: val_accuracy did not improve from 0.27917\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.0833 - accuracy: 0.2917 - val_loss: 2.1244 - val_accuracy: 0.2708\n",
      "Epoch 108/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 2.0768 - accuracy: 0.2812\n",
      "Epoch 00108: val_accuracy did not improve from 0.27917\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.0767 - accuracy: 0.2792 - val_loss: 2.1237 - val_accuracy: 0.2542\n",
      "Epoch 109/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 2.0734 - accuracy: 0.2886\n",
      "Epoch 00109: val_accuracy did not improve from 0.27917\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.0683 - accuracy: 0.2967 - val_loss: 2.1165 - val_accuracy: 0.2708\n",
      "Epoch 110/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 2.0889 - accuracy: 0.2785\n",
      "Epoch 00110: val_accuracy did not improve from 0.27917\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.0854 - accuracy: 0.2800 - val_loss: 2.1262 - val_accuracy: 0.2750\n",
      "Epoch 111/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 2.0516 - accuracy: 0.2785\n",
      "Epoch 00111: val_accuracy did not improve from 0.27917\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.0552 - accuracy: 0.2800 - val_loss: 2.1192 - val_accuracy: 0.2625\n",
      "Epoch 112/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 2.0645 - accuracy: 0.2841\n",
      "Epoch 00112: val_accuracy did not improve from 0.27917\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.0594 - accuracy: 0.2883 - val_loss: 2.1188 - val_accuracy: 0.2667\n",
      "Epoch 113/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 2.0763 - accuracy: 0.2803\n",
      "Epoch 00113: val_accuracy did not improve from 0.27917\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.0703 - accuracy: 0.2875 - val_loss: 2.1194 - val_accuracy: 0.2667\n",
      "Epoch 114/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 2.0604 - accuracy: 0.3042\n",
      "Epoch 00114: val_accuracy did not improve from 0.27917\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.0586 - accuracy: 0.3042 - val_loss: 2.1099 - val_accuracy: 0.2667\n",
      "Epoch 115/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 2.0781 - accuracy: 0.2831\n",
      "Epoch 00115: val_accuracy did not improve from 0.27917\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.0695 - accuracy: 0.2842 - val_loss: 2.1070 - val_accuracy: 0.2750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 2.0347 - accuracy: 0.3070\n",
      "Epoch 00116: val_accuracy did not improve from 0.27917\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.0365 - accuracy: 0.3042 - val_loss: 2.0951 - val_accuracy: 0.2667\n",
      "Epoch 117/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 2.0541 - accuracy: 0.2692\n",
      "Epoch 00117: val_accuracy did not improve from 0.27917\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.0427 - accuracy: 0.2792 - val_loss: 2.0968 - val_accuracy: 0.2667\n",
      "Epoch 118/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 2.0335 - accuracy: 0.2873\n",
      "Epoch 00118: val_accuracy did not improve from 0.27917\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.0406 - accuracy: 0.2858 - val_loss: 2.0911 - val_accuracy: 0.2667\n",
      "Epoch 119/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 2.0407 - accuracy: 0.3051\n",
      "Epoch 00119: val_accuracy did not improve from 0.27917\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.0490 - accuracy: 0.3025 - val_loss: 2.0973 - val_accuracy: 0.2667\n",
      "Epoch 120/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 2.0285 - accuracy: 0.3171\n",
      "Epoch 00120: val_accuracy did not improve from 0.27917\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.0254 - accuracy: 0.3125 - val_loss: 2.0939 - val_accuracy: 0.2667\n",
      "Epoch 121/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 2.0479 - accuracy: 0.2987\n",
      "Epoch 00121: val_accuracy did not improve from 0.27917\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.0401 - accuracy: 0.3075 - val_loss: 2.0844 - val_accuracy: 0.2583\n",
      "Epoch 122/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 2.0337 - accuracy: 0.2822\n",
      "Epoch 00122: val_accuracy did not improve from 0.27917\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.0420 - accuracy: 0.2825 - val_loss: 2.0782 - val_accuracy: 0.2625\n",
      "Epoch 123/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 2.0186 - accuracy: 0.2783\n",
      "Epoch 00123: val_accuracy did not improve from 0.27917\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.0295 - accuracy: 0.2792 - val_loss: 2.0767 - val_accuracy: 0.2625\n",
      "Epoch 124/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 2.0314 - accuracy: 0.2812\n",
      "Epoch 00124: val_accuracy did not improve from 0.27917\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.0325 - accuracy: 0.2917 - val_loss: 2.0721 - val_accuracy: 0.2625\n",
      "Epoch 125/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 2.0101 - accuracy: 0.3116\n",
      "Epoch 00125: val_accuracy did not improve from 0.27917\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.0136 - accuracy: 0.3083 - val_loss: 2.0776 - val_accuracy: 0.2667\n",
      "Epoch 126/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 2.0060 - accuracy: 0.3051\n",
      "Epoch 00126: val_accuracy did not improve from 0.27917\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.0133 - accuracy: 0.3008 - val_loss: 2.0696 - val_accuracy: 0.2750\n",
      "Epoch 127/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 2.0299 - accuracy: 0.3135\n",
      "Epoch 00127: val_accuracy improved from 0.27917 to 0.28333, saving model to best_model.hdf5\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.0237 - accuracy: 0.3175 - val_loss: 2.0649 - val_accuracy: 0.2833\n",
      "Epoch 128/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 2.0094 - accuracy: 0.3015\n",
      "Epoch 00128: val_accuracy did not improve from 0.28333\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.0063 - accuracy: 0.3075 - val_loss: 2.0679 - val_accuracy: 0.2625\n",
      "Epoch 129/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 2.0264 - accuracy: 0.2964\n",
      "Epoch 00129: val_accuracy did not improve from 0.28333\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.0197 - accuracy: 0.2992 - val_loss: 2.0702 - val_accuracy: 0.2625\n",
      "Epoch 130/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.9994 - accuracy: 0.2930\n",
      "Epoch 00130: val_accuracy did not improve from 0.28333\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.0003 - accuracy: 0.2933 - val_loss: 2.0629 - val_accuracy: 0.2792\n",
      "Epoch 131/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.9906 - accuracy: 0.3216\n",
      "Epoch 00131: val_accuracy did not improve from 0.28333\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.9935 - accuracy: 0.3208 - val_loss: 2.0548 - val_accuracy: 0.2750\n",
      "Epoch 132/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 2.0319 - accuracy: 0.2903\n",
      "Epoch 00132: val_accuracy did not improve from 0.28333\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.0187 - accuracy: 0.2950 - val_loss: 2.0539 - val_accuracy: 0.2750\n",
      "Epoch 133/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 2.0087 - accuracy: 0.3066\n",
      "Epoch 00133: val_accuracy did not improve from 0.28333\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.0101 - accuracy: 0.3017 - val_loss: 2.0612 - val_accuracy: 0.2750\n",
      "Epoch 134/500\n",
      "29/38 [=====================>........] - ETA: 0s - loss: 2.0124 - accuracy: 0.3125\n",
      "Epoch 00134: val_accuracy did not improve from 0.28333\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.0112 - accuracy: 0.3100 - val_loss: 2.0573 - val_accuracy: 0.2667\n",
      "Epoch 135/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.9863 - accuracy: 0.2994\n",
      "Epoch 00135: val_accuracy did not improve from 0.28333\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.9838 - accuracy: 0.3017 - val_loss: 2.0520 - val_accuracy: 0.2792\n",
      "Epoch 136/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.9879 - accuracy: 0.3052\n",
      "Epoch 00136: val_accuracy did not improve from 0.28333\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.9850 - accuracy: 0.3075 - val_loss: 2.0378 - val_accuracy: 0.2750\n",
      "Epoch 137/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.9790 - accuracy: 0.2974\n",
      "Epoch 00137: val_accuracy did not improve from 0.28333\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.9804 - accuracy: 0.3033 - val_loss: 2.0420 - val_accuracy: 0.2708\n",
      "Epoch 138/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 1.9850 - accuracy: 0.3144\n",
      "Epoch 00138: val_accuracy did not improve from 0.28333\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.9878 - accuracy: 0.3083 - val_loss: 2.0382 - val_accuracy: 0.2833\n",
      "Epoch 139/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 1.9827 - accuracy: 0.3097\n",
      "Epoch 00139: val_accuracy did not improve from 0.28333\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.9839 - accuracy: 0.3050 - val_loss: 2.0388 - val_accuracy: 0.2833\n",
      "Epoch 140/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 1.9711 - accuracy: 0.2969\n",
      "Epoch 00140: val_accuracy did not improve from 0.28333\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.9793 - accuracy: 0.2975 - val_loss: 2.0413 - val_accuracy: 0.2708\n",
      "Epoch 141/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.9790 - accuracy: 0.2863\n",
      "Epoch 00141: val_accuracy did not improve from 0.28333\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.9747 - accuracy: 0.2975 - val_loss: 2.0433 - val_accuracy: 0.2750\n",
      "Epoch 142/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.9678 - accuracy: 0.3226\n",
      "Epoch 00142: val_accuracy improved from 0.28333 to 0.30000, saving model to best_model.hdf5\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.9671 - accuracy: 0.3250 - val_loss: 2.0377 - val_accuracy: 0.3000\n",
      "Epoch 143/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 1.9582 - accuracy: 0.3162\n",
      "Epoch 00143: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.9674 - accuracy: 0.3133 - val_loss: 2.0339 - val_accuracy: 0.2958\n",
      "Epoch 144/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 1.9758 - accuracy: 0.3309\n",
      "Epoch 00144: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.9740 - accuracy: 0.3325 - val_loss: 2.0331 - val_accuracy: 0.2875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 1.9546 - accuracy: 0.3229\n",
      "Epoch 00145: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.9523 - accuracy: 0.3225 - val_loss: 2.0328 - val_accuracy: 0.2750\n",
      "Epoch 146/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.9571 - accuracy: 0.3105\n",
      "Epoch 00146: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.9560 - accuracy: 0.3050 - val_loss: 2.0346 - val_accuracy: 0.2708\n",
      "Epoch 147/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 1.9692 - accuracy: 0.3134\n",
      "Epoch 00147: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.9609 - accuracy: 0.3183 - val_loss: 2.0314 - val_accuracy: 0.2875\n",
      "Epoch 148/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 1.9807 - accuracy: 0.3051\n",
      "Epoch 00148: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.9780 - accuracy: 0.3025 - val_loss: 2.0260 - val_accuracy: 0.2708\n",
      "Epoch 149/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.9306 - accuracy: 0.3167\n",
      "Epoch 00149: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.9435 - accuracy: 0.3175 - val_loss: 2.0169 - val_accuracy: 0.2792\n",
      "Epoch 150/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 1.9465 - accuracy: 0.3191\n",
      "Epoch 00150: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.9574 - accuracy: 0.3125 - val_loss: 2.0222 - val_accuracy: 0.2875\n",
      "Epoch 151/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 1.9656 - accuracy: 0.3281\n",
      "Epoch 00151: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.9630 - accuracy: 0.3325 - val_loss: 2.0279 - val_accuracy: 0.2875\n",
      "Epoch 152/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.9515 - accuracy: 0.3203\n",
      "Epoch 00152: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.9448 - accuracy: 0.3267 - val_loss: 2.0182 - val_accuracy: 0.2833\n",
      "Epoch 153/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.9468 - accuracy: 0.3125\n",
      "Epoch 00153: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.9289 - accuracy: 0.3200 - val_loss: 2.0085 - val_accuracy: 0.2917\n",
      "Epoch 154/500\n",
      "28/38 [=====================>........] - ETA: 0s - loss: 1.9336 - accuracy: 0.3371\n",
      "Epoch 00154: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.9347 - accuracy: 0.3367 - val_loss: 2.0094 - val_accuracy: 0.2708\n",
      "Epoch 155/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 1.9762 - accuracy: 0.3199\n",
      "Epoch 00155: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.9737 - accuracy: 0.3192 - val_loss: 2.0079 - val_accuracy: 0.2875\n",
      "Epoch 156/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.9400 - accuracy: 0.3408\n",
      "Epoch 00156: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.9420 - accuracy: 0.3367 - val_loss: 2.0113 - val_accuracy: 0.2958\n",
      "Epoch 157/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.9398 - accuracy: 0.3175\n",
      "Epoch 00157: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.9438 - accuracy: 0.3100 - val_loss: 2.0049 - val_accuracy: 0.2833\n",
      "Epoch 158/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 1.9013 - accuracy: 0.3400\n",
      "Epoch 00158: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.9184 - accuracy: 0.3333 - val_loss: 2.0023 - val_accuracy: 0.2917\n",
      "Epoch 159/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 1.9119 - accuracy: 0.3143\n",
      "Epoch 00159: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.9113 - accuracy: 0.3200 - val_loss: 2.0046 - val_accuracy: 0.2917\n",
      "Epoch 160/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.9356 - accuracy: 0.3330\n",
      "Epoch 00160: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.9423 - accuracy: 0.3217 - val_loss: 2.0054 - val_accuracy: 0.2833\n",
      "Epoch 161/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 1.9486 - accuracy: 0.3281\n",
      "Epoch 00161: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.9407 - accuracy: 0.3283 - val_loss: 2.0121 - val_accuracy: 0.2792\n",
      "Epoch 162/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 1.9300 - accuracy: 0.3180\n",
      "Epoch 00162: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.9381 - accuracy: 0.3117 - val_loss: 2.0044 - val_accuracy: 0.2833\n",
      "Epoch 163/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.9178 - accuracy: 0.3135\n",
      "Epoch 00163: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.9280 - accuracy: 0.3142 - val_loss: 2.0024 - val_accuracy: 0.2917\n",
      "Epoch 164/500\n",
      "38/38 [==============================] - ETA: 0s - loss: 1.9359 - accuracy: 0.3042\n",
      "Epoch 00164: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.9359 - accuracy: 0.3042 - val_loss: 2.0051 - val_accuracy: 0.2917\n",
      "Epoch 165/500\n",
      "29/38 [=====================>........] - ETA: 0s - loss: 1.9289 - accuracy: 0.3276\n",
      "Epoch 00165: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.9211 - accuracy: 0.3258 - val_loss: 2.0025 - val_accuracy: 0.2917\n",
      "Epoch 166/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.9164 - accuracy: 0.3125\n",
      "Epoch 00166: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.9199 - accuracy: 0.3100 - val_loss: 1.9952 - val_accuracy: 0.2958\n",
      "Epoch 167/500\n",
      "28/38 [=====================>........] - ETA: 0s - loss: 1.9383 - accuracy: 0.3147\n",
      "Epoch 00167: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.9394 - accuracy: 0.3250 - val_loss: 1.9988 - val_accuracy: 0.3000\n",
      "Epoch 168/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.9111 - accuracy: 0.3266\n",
      "Epoch 00168: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.9179 - accuracy: 0.3283 - val_loss: 1.9949 - val_accuracy: 0.2917\n",
      "Epoch 169/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.9029 - accuracy: 0.3286\n",
      "Epoch 00169: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.9117 - accuracy: 0.3225 - val_loss: 1.9874 - val_accuracy: 0.2958\n",
      "Epoch 170/500\n",
      "38/38 [==============================] - ETA: 0s - loss: 1.9164 - accuracy: 0.3200\n",
      "Epoch 00170: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.9164 - accuracy: 0.3200 - val_loss: 1.9970 - val_accuracy: 0.2958\n",
      "Epoch 171/500\n",
      "38/38 [==============================] - ETA: 0s - loss: 1.9053 - accuracy: 0.3367\n",
      "Epoch 00171: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.9053 - accuracy: 0.3367 - val_loss: 1.9882 - val_accuracy: 0.2875\n",
      "Epoch 172/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.9094 - accuracy: 0.3417\n",
      "Epoch 00172: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.9038 - accuracy: 0.3450 - val_loss: 2.0007 - val_accuracy: 0.2833\n",
      "Epoch 173/500\n",
      "38/38 [==============================] - ETA: 0s - loss: 1.9193 - accuracy: 0.3133\n",
      "Epoch 00173: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.9193 - accuracy: 0.3133 - val_loss: 1.9873 - val_accuracy: 0.2875\n",
      "Epoch 174/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/38 [=======================>......] - ETA: 0s - loss: 1.9163 - accuracy: 0.3034\n",
      "Epoch 00174: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.9074 - accuracy: 0.3208 - val_loss: 1.9774 - val_accuracy: 0.2917\n",
      "Epoch 175/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.8859 - accuracy: 0.3397\n",
      "Epoch 00175: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.8929 - accuracy: 0.3350 - val_loss: 1.9805 - val_accuracy: 0.2875\n",
      "Epoch 176/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 1.8979 - accuracy: 0.3172\n",
      "Epoch 00176: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.9029 - accuracy: 0.3192 - val_loss: 1.9753 - val_accuracy: 0.2833\n",
      "Epoch 177/500\n",
      "29/38 [=====================>........] - ETA: 0s - loss: 1.9150 - accuracy: 0.3254\n",
      "Epoch 00177: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.9117 - accuracy: 0.3133 - val_loss: 1.9850 - val_accuracy: 0.2875\n",
      "Epoch 178/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.8880 - accuracy: 0.3347\n",
      "Epoch 00178: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.8970 - accuracy: 0.3325 - val_loss: 1.9775 - val_accuracy: 0.2958\n",
      "Epoch 179/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.8945 - accuracy: 0.3417\n",
      "Epoch 00179: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.9046 - accuracy: 0.3333 - val_loss: 1.9757 - val_accuracy: 0.2958\n",
      "Epoch 180/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 1.8733 - accuracy: 0.3355\n",
      "Epoch 00180: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.8675 - accuracy: 0.3375 - val_loss: 1.9699 - val_accuracy: 0.2958\n",
      "Epoch 181/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 1.9082 - accuracy: 0.3281\n",
      "Epoch 00181: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.9135 - accuracy: 0.3242 - val_loss: 1.9710 - val_accuracy: 0.2917\n",
      "Epoch 182/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.8734 - accuracy: 0.3359\n",
      "Epoch 00182: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.8864 - accuracy: 0.3308 - val_loss: 1.9677 - val_accuracy: 0.2917\n",
      "Epoch 183/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 1.8980 - accuracy: 0.3355\n",
      "Epoch 00183: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.8967 - accuracy: 0.3350 - val_loss: 1.9726 - val_accuracy: 0.2958\n",
      "Epoch 184/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 1.8959 - accuracy: 0.3295\n",
      "Epoch 00184: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.8961 - accuracy: 0.3317 - val_loss: 1.9711 - val_accuracy: 0.2875\n",
      "Epoch 185/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.9019 - accuracy: 0.3448\n",
      "Epoch 00185: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.8934 - accuracy: 0.3592 - val_loss: 1.9695 - val_accuracy: 0.2917\n",
      "Epoch 186/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 1.8810 - accuracy: 0.3300\n",
      "Epoch 00186: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.8910 - accuracy: 0.3242 - val_loss: 1.9690 - val_accuracy: 0.3000\n",
      "Epoch 187/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.8957 - accuracy: 0.3076\n",
      "Epoch 00187: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.8866 - accuracy: 0.3133 - val_loss: 1.9654 - val_accuracy: 0.2958\n",
      "Epoch 188/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 1.8916 - accuracy: 0.3410\n",
      "Epoch 00188: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.8895 - accuracy: 0.3375 - val_loss: 1.9634 - val_accuracy: 0.2917\n",
      "Epoch 189/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 1.8711 - accuracy: 0.3529\n",
      "Epoch 00189: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.8788 - accuracy: 0.3458 - val_loss: 1.9718 - val_accuracy: 0.2958\n",
      "Epoch 190/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 1.8726 - accuracy: 0.3419\n",
      "Epoch 00190: val_accuracy did not improve from 0.30000\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.8790 - accuracy: 0.3375 - val_loss: 1.9708 - val_accuracy: 0.2875\n",
      "Epoch 191/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.9020 - accuracy: 0.3387\n",
      "Epoch 00191: val_accuracy improved from 0.30000 to 0.30417, saving model to best_model.hdf5\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 1.9051 - accuracy: 0.3350 - val_loss: 1.9613 - val_accuracy: 0.3042\n",
      "Epoch 192/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.8719 - accuracy: 0.3427\n",
      "Epoch 00192: val_accuracy did not improve from 0.30417\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.8837 - accuracy: 0.3300 - val_loss: 1.9577 - val_accuracy: 0.3042\n",
      "Epoch 193/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 1.8750 - accuracy: 0.3281\n",
      "Epoch 00193: val_accuracy did not improve from 0.30417\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.8631 - accuracy: 0.3325 - val_loss: 1.9578 - val_accuracy: 0.3042\n",
      "Epoch 194/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.8925 - accuracy: 0.3320\n",
      "Epoch 00194: val_accuracy did not improve from 0.30417\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.8721 - accuracy: 0.3358 - val_loss: 1.9522 - val_accuracy: 0.3000\n",
      "Epoch 195/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 1.8598 - accuracy: 0.3566\n",
      "Epoch 00195: val_accuracy improved from 0.30417 to 0.30833, saving model to best_model.hdf5\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.8609 - accuracy: 0.3583 - val_loss: 1.9584 - val_accuracy: 0.3083\n",
      "Epoch 196/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 1.8944 - accuracy: 0.3033\n",
      "Epoch 00196: val_accuracy did not improve from 0.30833\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.8885 - accuracy: 0.3100 - val_loss: 1.9570 - val_accuracy: 0.2958\n",
      "Epoch 197/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 1.8512 - accuracy: 0.3438\n",
      "Epoch 00197: val_accuracy did not improve from 0.30833\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.8605 - accuracy: 0.3392 - val_loss: 1.9505 - val_accuracy: 0.3083\n",
      "Epoch 198/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.8619 - accuracy: 0.3398\n",
      "Epoch 00198: val_accuracy improved from 0.30833 to 0.31667, saving model to best_model.hdf5\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.8727 - accuracy: 0.3283 - val_loss: 1.9544 - val_accuracy: 0.3167\n",
      "Epoch 199/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.8548 - accuracy: 0.3478\n",
      "Epoch 00199: val_accuracy did not improve from 0.31667\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.8608 - accuracy: 0.3508 - val_loss: 1.9495 - val_accuracy: 0.3000\n",
      "Epoch 200/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 1.8689 - accuracy: 0.3201\n",
      "Epoch 00200: val_accuracy did not improve from 0.31667\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 1.8643 - accuracy: 0.3267 - val_loss: 1.9485 - val_accuracy: 0.3125\n",
      "Epoch 201/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 1.8609 - accuracy: 0.3617\n",
      "Epoch 00201: val_accuracy did not improve from 0.31667\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.8680 - accuracy: 0.3500 - val_loss: 1.9466 - val_accuracy: 0.3083\n",
      "Epoch 202/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 1.8720 - accuracy: 0.3504\n",
      "Epoch 00202: val_accuracy did not improve from 0.31667\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.8659 - accuracy: 0.3492 - val_loss: 1.9465 - val_accuracy: 0.3042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 203/500\n",
      "35/38 [==========================>...] - ETA: 0s - loss: 1.8789 - accuracy: 0.3357\n",
      "Epoch 00203: val_accuracy did not improve from 0.31667\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 1.8698 - accuracy: 0.3375 - val_loss: 1.9432 - val_accuracy: 0.3083\n",
      "Epoch 204/500\n",
      "38/38 [==============================] - ETA: 0s - loss: 1.8544 - accuracy: 0.3308\n",
      "Epoch 00204: val_accuracy did not improve from 0.31667\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.8544 - accuracy: 0.3308 - val_loss: 1.9457 - val_accuracy: 0.3125\n",
      "Epoch 205/500\n",
      "36/38 [===========================>..] - ETA: 0s - loss: 1.8632 - accuracy: 0.3368\n",
      "Epoch 00205: val_accuracy did not improve from 0.31667\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 1.8661 - accuracy: 0.3350 - val_loss: 1.9499 - val_accuracy: 0.3042\n",
      "Epoch 206/500\n",
      "36/38 [===========================>..] - ETA: 0s - loss: 1.8762 - accuracy: 0.3342\n",
      "Epoch 00206: val_accuracy did not improve from 0.31667\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 1.8723 - accuracy: 0.3342 - val_loss: 1.9408 - val_accuracy: 0.3000\n",
      "Epoch 207/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 1.8330 - accuracy: 0.3493\n",
      "Epoch 00207: val_accuracy did not improve from 0.31667\n",
      "38/38 [==============================] - 0s 13ms/step - loss: 1.8321 - accuracy: 0.3542 - val_loss: 1.9475 - val_accuracy: 0.2917\n",
      "Epoch 208/500\n",
      "38/38 [==============================] - ETA: 0s - loss: 1.8567 - accuracy: 0.3400\n",
      "Epoch 00208: val_accuracy did not improve from 0.31667\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 1.8567 - accuracy: 0.3400 - val_loss: 1.9444 - val_accuracy: 0.3125\n",
      "Epoch 209/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.8603 - accuracy: 0.3337\n",
      "Epoch 00209: val_accuracy did not improve from 0.31667\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.8548 - accuracy: 0.3325 - val_loss: 1.9356 - val_accuracy: 0.3167\n",
      "Epoch 210/500\n",
      "29/38 [=====================>........] - ETA: 0s - loss: 1.8317 - accuracy: 0.3491\n",
      "Epoch 00210: val_accuracy did not improve from 0.31667\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.8501 - accuracy: 0.3442 - val_loss: 1.9375 - val_accuracy: 0.2917\n",
      "Epoch 211/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 1.8547 - accuracy: 0.3551\n",
      "Epoch 00211: val_accuracy did not improve from 0.31667\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.8457 - accuracy: 0.3617 - val_loss: 1.9352 - val_accuracy: 0.2917\n",
      "Epoch 212/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.8506 - accuracy: 0.3196\n",
      "Epoch 00212: val_accuracy did not improve from 0.31667\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.8368 - accuracy: 0.3292 - val_loss: 1.9398 - val_accuracy: 0.2917\n",
      "Epoch 213/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.8395 - accuracy: 0.3271\n",
      "Epoch 00213: val_accuracy did not improve from 0.31667\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.8241 - accuracy: 0.3325 - val_loss: 1.9380 - val_accuracy: 0.2792\n",
      "Epoch 214/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.8527 - accuracy: 0.3478\n",
      "Epoch 00214: val_accuracy did not improve from 0.31667\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.8506 - accuracy: 0.3517 - val_loss: 1.9346 - val_accuracy: 0.3167\n",
      "Epoch 215/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.8450 - accuracy: 0.3438\n",
      "Epoch 00215: val_accuracy did not improve from 0.31667\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.8377 - accuracy: 0.3417 - val_loss: 1.9453 - val_accuracy: 0.3042\n",
      "Epoch 216/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.8382 - accuracy: 0.3542\n",
      "Epoch 00216: val_accuracy did not improve from 0.31667\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.8427 - accuracy: 0.3500 - val_loss: 1.9445 - val_accuracy: 0.3000\n",
      "Epoch 217/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.8255 - accuracy: 0.3296\n",
      "Epoch 00217: val_accuracy did not improve from 0.31667\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.8416 - accuracy: 0.3283 - val_loss: 1.9310 - val_accuracy: 0.3000\n",
      "Epoch 218/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 1.8213 - accuracy: 0.3419\n",
      "Epoch 00218: val_accuracy did not improve from 0.31667\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 1.8219 - accuracy: 0.3442 - val_loss: 1.9321 - val_accuracy: 0.2875\n",
      "Epoch 219/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.8413 - accuracy: 0.3406\n",
      "Epoch 00219: val_accuracy did not improve from 0.31667\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.8502 - accuracy: 0.3400 - val_loss: 1.9255 - val_accuracy: 0.3083\n",
      "Epoch 220/500\n",
      "36/38 [===========================>..] - ETA: 0s - loss: 1.8505 - accuracy: 0.3151\n",
      "Epoch 00220: val_accuracy did not improve from 0.31667\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 1.8484 - accuracy: 0.3158 - val_loss: 1.9353 - val_accuracy: 0.3083\n",
      "Epoch 221/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 1.8475 - accuracy: 0.3327\n",
      "Epoch 00221: val_accuracy did not improve from 0.31667\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 1.8439 - accuracy: 0.3342 - val_loss: 1.9308 - val_accuracy: 0.3167\n",
      "Epoch 222/500\n",
      "36/38 [===========================>..] - ETA: 0s - loss: 1.8234 - accuracy: 0.3507\n",
      "Epoch 00222: val_accuracy did not improve from 0.31667\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 1.8268 - accuracy: 0.3475 - val_loss: 1.9346 - val_accuracy: 0.3125\n",
      "Epoch 223/500\n",
      "37/38 [============================>.] - ETA: 0s - loss: 1.8175 - accuracy: 0.3412\n",
      "Epoch 00223: val_accuracy did not improve from 0.31667\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.8200 - accuracy: 0.3383 - val_loss: 1.9209 - val_accuracy: 0.3167\n",
      "Epoch 224/500\n",
      "29/38 [=====================>........] - ETA: 0s - loss: 1.8223 - accuracy: 0.3534\n",
      "Epoch 00224: val_accuracy did not improve from 0.31667\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.8131 - accuracy: 0.3533 - val_loss: 1.9219 - val_accuracy: 0.2958\n",
      "Epoch 225/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.8536 - accuracy: 0.3306\n",
      "Epoch 00225: val_accuracy did not improve from 0.31667\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.8440 - accuracy: 0.3350 - val_loss: 1.9255 - val_accuracy: 0.3125\n",
      "Epoch 226/500\n",
      "29/38 [=====================>........] - ETA: 0s - loss: 1.8234 - accuracy: 0.3610\n",
      "Epoch 00226: val_accuracy did not improve from 0.31667\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.8505 - accuracy: 0.3450 - val_loss: 1.9180 - val_accuracy: 0.3167\n",
      "Epoch 227/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.8213 - accuracy: 0.3498\n",
      "Epoch 00227: val_accuracy did not improve from 0.31667\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.8276 - accuracy: 0.3425 - val_loss: 1.9245 - val_accuracy: 0.3125\n",
      "Epoch 228/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.8422 - accuracy: 0.3291\n",
      "Epoch 00228: val_accuracy did not improve from 0.31667\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.8479 - accuracy: 0.3208 - val_loss: 1.9379 - val_accuracy: 0.3083\n",
      "Epoch 229/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.8324 - accuracy: 0.3542\n",
      "Epoch 00229: val_accuracy did not improve from 0.31667\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.8277 - accuracy: 0.3617 - val_loss: 1.9173 - val_accuracy: 0.3125\n",
      "Epoch 230/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.8182 - accuracy: 0.3427\n",
      "Epoch 00230: val_accuracy did not improve from 0.31667\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.8169 - accuracy: 0.3433 - val_loss: 1.9250 - val_accuracy: 0.3000\n",
      "Epoch 231/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.8048 - accuracy: 0.3438\n",
      "Epoch 00231: val_accuracy did not improve from 0.31667\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.8093 - accuracy: 0.3400 - val_loss: 1.9217 - val_accuracy: 0.3042\n",
      "Epoch 232/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/38 [========================>.....] - ETA: 0s - loss: 1.8165 - accuracy: 0.3555\n",
      "Epoch 00232: val_accuracy did not improve from 0.31667\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.8164 - accuracy: 0.3608 - val_loss: 1.9257 - val_accuracy: 0.3125\n",
      "Epoch 233/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.8068 - accuracy: 0.3417\n",
      "Epoch 00233: val_accuracy did not improve from 0.31667\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.8205 - accuracy: 0.3417 - val_loss: 1.9235 - val_accuracy: 0.3000\n",
      "Epoch 234/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.8374 - accuracy: 0.3311\n",
      "Epoch 00234: val_accuracy did not improve from 0.31667\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.8206 - accuracy: 0.3500 - val_loss: 1.9195 - val_accuracy: 0.3125\n",
      "Epoch 235/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 1.8082 - accuracy: 0.3636\n",
      "Epoch 00235: val_accuracy did not improve from 0.31667\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.8161 - accuracy: 0.3608 - val_loss: 1.9254 - val_accuracy: 0.3083\n",
      "Epoch 236/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.8006 - accuracy: 0.3594\n",
      "Epoch 00236: val_accuracy did not improve from 0.31667\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.8049 - accuracy: 0.3558 - val_loss: 1.9309 - val_accuracy: 0.3083\n",
      "Epoch 237/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.8172 - accuracy: 0.3458\n",
      "Epoch 00237: val_accuracy did not improve from 0.31667\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.8074 - accuracy: 0.3508 - val_loss: 1.9208 - val_accuracy: 0.3125\n",
      "Epoch 238/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.8153 - accuracy: 0.3347\n",
      "Epoch 00238: val_accuracy did not improve from 0.31667\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.8313 - accuracy: 0.3267 - val_loss: 1.9213 - val_accuracy: 0.3167\n",
      "Epoch 239/500\n",
      "38/38 [==============================] - ETA: 0s - loss: 1.8000 - accuracy: 0.3675\n",
      "Epoch 00239: val_accuracy did not improve from 0.31667\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.8000 - accuracy: 0.3675 - val_loss: 1.9146 - val_accuracy: 0.2958\n",
      "Epoch 240/500\n",
      "37/38 [============================>.] - ETA: 0s - loss: 1.8056 - accuracy: 0.3514\n",
      "Epoch 00240: val_accuracy did not improve from 0.31667\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 1.8043 - accuracy: 0.3517 - val_loss: 1.9148 - val_accuracy: 0.2875\n",
      "Epoch 241/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.8262 - accuracy: 0.3498\n",
      "Epoch 00241: val_accuracy did not improve from 0.31667\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.8271 - accuracy: 0.3475 - val_loss: 1.9142 - val_accuracy: 0.3167\n",
      "Epoch 242/500\n",
      "38/38 [==============================] - ETA: 0s - loss: 1.8339 - accuracy: 0.3500\n",
      "Epoch 00242: val_accuracy improved from 0.31667 to 0.32917, saving model to best_model.hdf5\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 1.8339 - accuracy: 0.3500 - val_loss: 1.9225 - val_accuracy: 0.3292\n",
      "Epoch 243/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.8177 - accuracy: 0.3623\n",
      "Epoch 00243: val_accuracy did not improve from 0.32917\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 1.8086 - accuracy: 0.3692 - val_loss: 1.9190 - val_accuracy: 0.3125\n",
      "Epoch 244/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.7971 - accuracy: 0.3740\n",
      "Epoch 00244: val_accuracy did not improve from 0.32917\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7970 - accuracy: 0.3675 - val_loss: 1.9118 - val_accuracy: 0.2958\n",
      "Epoch 245/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.8389 - accuracy: 0.3417\n",
      "Epoch 00245: val_accuracy did not improve from 0.32917\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.8309 - accuracy: 0.3475 - val_loss: 1.9301 - val_accuracy: 0.3208\n",
      "Epoch 246/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 1.8224 - accuracy: 0.3438\n",
      "Epoch 00246: val_accuracy did not improve from 0.32917\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.8219 - accuracy: 0.3433 - val_loss: 1.9228 - val_accuracy: 0.3292\n",
      "Epoch 247/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.8020 - accuracy: 0.3604\n",
      "Epoch 00247: val_accuracy did not improve from 0.32917\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 1.7954 - accuracy: 0.3625 - val_loss: 1.9175 - val_accuracy: 0.3250\n",
      "Epoch 248/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 1.8029 - accuracy: 0.3392\n",
      "Epoch 00248: val_accuracy did not improve from 0.32917\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.8040 - accuracy: 0.3408 - val_loss: 1.9057 - val_accuracy: 0.3125\n",
      "Epoch 249/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 1.7883 - accuracy: 0.3594\n",
      "Epoch 00249: val_accuracy did not improve from 0.32917\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.8008 - accuracy: 0.3500 - val_loss: 1.9088 - val_accuracy: 0.3042\n",
      "Epoch 250/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 1.7999 - accuracy: 0.3575\n",
      "Epoch 00250: val_accuracy did not improve from 0.32917\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.8063 - accuracy: 0.3542 - val_loss: 1.9092 - val_accuracy: 0.3083\n",
      "Epoch 251/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 1.7768 - accuracy: 0.3712\n",
      "Epoch 00251: val_accuracy did not improve from 0.32917\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.7651 - accuracy: 0.3833 - val_loss: 1.9088 - val_accuracy: 0.3167\n",
      "Epoch 252/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 1.7793 - accuracy: 0.3483\n",
      "Epoch 00252: val_accuracy did not improve from 0.32917\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7817 - accuracy: 0.3492 - val_loss: 1.9058 - val_accuracy: 0.3042\n",
      "Epoch 253/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 1.7899 - accuracy: 0.3665\n",
      "Epoch 00253: val_accuracy did not improve from 0.32917\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.7932 - accuracy: 0.3667 - val_loss: 1.9181 - val_accuracy: 0.3083\n",
      "Epoch 254/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 1.7974 - accuracy: 0.3529\n",
      "Epoch 00254: val_accuracy did not improve from 0.32917\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.8034 - accuracy: 0.3500 - val_loss: 1.9091 - val_accuracy: 0.3292\n",
      "Epoch 255/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.7855 - accuracy: 0.3594\n",
      "Epoch 00255: val_accuracy did not improve from 0.32917\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.7876 - accuracy: 0.3742 - val_loss: 1.9139 - val_accuracy: 0.3167\n",
      "Epoch 256/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.8033 - accuracy: 0.3337\n",
      "Epoch 00256: val_accuracy did not improve from 0.32917\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.8066 - accuracy: 0.3383 - val_loss: 1.9127 - val_accuracy: 0.3125\n",
      "Epoch 257/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.7863 - accuracy: 0.3652\n",
      "Epoch 00257: val_accuracy did not improve from 0.32917\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7828 - accuracy: 0.3700 - val_loss: 1.9107 - val_accuracy: 0.3208\n",
      "Epoch 258/500\n",
      "37/38 [============================>.] - ETA: 0s - loss: 1.8133 - accuracy: 0.3547\n",
      "Epoch 00258: val_accuracy did not improve from 0.32917\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.8116 - accuracy: 0.3575 - val_loss: 1.9057 - val_accuracy: 0.3208\n",
      "Epoch 259/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 1.7821 - accuracy: 0.3640\n",
      "Epoch 00259: val_accuracy did not improve from 0.32917\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.7909 - accuracy: 0.3617 - val_loss: 1.9104 - val_accuracy: 0.3167\n",
      "Epoch 260/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.7681 - accuracy: 0.3690\n",
      "Epoch 00260: val_accuracy did not improve from 0.32917\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7777 - accuracy: 0.3633 - val_loss: 1.8990 - val_accuracy: 0.3250\n",
      "Epoch 261/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/38 [========================>.....] - ETA: 0s - loss: 1.8155 - accuracy: 0.3408\n",
      "Epoch 00261: val_accuracy did not improve from 0.32917\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.8123 - accuracy: 0.3508 - val_loss: 1.9106 - val_accuracy: 0.3083\n",
      "Epoch 262/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 1.7837 - accuracy: 0.3557\n",
      "Epoch 00262: val_accuracy improved from 0.32917 to 0.33750, saving model to best_model.hdf5\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7896 - accuracy: 0.3600 - val_loss: 1.9053 - val_accuracy: 0.3375\n",
      "Epoch 263/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 1.7617 - accuracy: 0.3636\n",
      "Epoch 00263: val_accuracy did not improve from 0.33750\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.7781 - accuracy: 0.3608 - val_loss: 1.9080 - val_accuracy: 0.3125\n",
      "Epoch 264/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.7783 - accuracy: 0.3467\n",
      "Epoch 00264: val_accuracy did not improve from 0.33750\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7953 - accuracy: 0.3425 - val_loss: 1.9080 - val_accuracy: 0.3208\n",
      "Epoch 265/500\n",
      "35/38 [==========================>...] - ETA: 0s - loss: 1.7822 - accuracy: 0.3652\n",
      "Epoch 00265: val_accuracy did not improve from 0.33750\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 1.7815 - accuracy: 0.3608 - val_loss: 1.9148 - val_accuracy: 0.3042\n",
      "Epoch 266/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 1.7571 - accuracy: 0.3759\n",
      "Epoch 00266: val_accuracy did not improve from 0.33750\n",
      "38/38 [==============================] - 0s 13ms/step - loss: 1.7609 - accuracy: 0.3708 - val_loss: 1.9122 - val_accuracy: 0.3042\n",
      "Epoch 267/500\n",
      "38/38 [==============================] - ETA: 0s - loss: 1.7759 - accuracy: 0.3650\n",
      "Epoch 00267: val_accuracy did not improve from 0.33750\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 1.7759 - accuracy: 0.3650 - val_loss: 1.9215 - val_accuracy: 0.3125\n",
      "Epoch 268/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 1.7810 - accuracy: 0.3475\n",
      "Epoch 00268: val_accuracy did not improve from 0.33750\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 1.7906 - accuracy: 0.3450 - val_loss: 1.9030 - val_accuracy: 0.3167\n",
      "Epoch 269/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.7739 - accuracy: 0.3721\n",
      "Epoch 00269: val_accuracy did not improve from 0.33750\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7832 - accuracy: 0.3675 - val_loss: 1.9084 - val_accuracy: 0.3333\n",
      "Epoch 270/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.8063 - accuracy: 0.3619\n",
      "Epoch 00270: val_accuracy did not improve from 0.33750\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.8043 - accuracy: 0.3558 - val_loss: 1.9080 - val_accuracy: 0.3250\n",
      "Epoch 271/500\n",
      "36/38 [===========================>..] - ETA: 0s - loss: 1.7769 - accuracy: 0.3594\n",
      "Epoch 00271: val_accuracy did not improve from 0.33750\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 1.7784 - accuracy: 0.3617 - val_loss: 1.9086 - val_accuracy: 0.3292\n",
      "Epoch 272/500\n",
      "37/38 [============================>.] - ETA: 0s - loss: 1.7593 - accuracy: 0.3758\n",
      "Epoch 00272: val_accuracy did not improve from 0.33750\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7562 - accuracy: 0.3783 - val_loss: 1.9046 - val_accuracy: 0.3167\n",
      "Epoch 273/500\n",
      "37/38 [============================>.] - ETA: 0s - loss: 1.7865 - accuracy: 0.3691\n",
      "Epoch 00273: val_accuracy did not improve from 0.33750\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 1.7873 - accuracy: 0.3667 - val_loss: 1.8954 - val_accuracy: 0.3167\n",
      "Epoch 274/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 1.7867 - accuracy: 0.3523\n",
      "Epoch 00274: val_accuracy did not improve from 0.33750\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 1.7793 - accuracy: 0.3575 - val_loss: 1.9048 - val_accuracy: 0.3375\n",
      "Epoch 275/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.7545 - accuracy: 0.3799\n",
      "Epoch 00275: val_accuracy did not improve from 0.33750\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 1.7560 - accuracy: 0.3767 - val_loss: 1.9092 - val_accuracy: 0.3167\n",
      "Epoch 276/500\n",
      "37/38 [============================>.] - ETA: 0s - loss: 1.7732 - accuracy: 0.3573\n",
      "Epoch 00276: val_accuracy did not improve from 0.33750\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7722 - accuracy: 0.3583 - val_loss: 1.8979 - val_accuracy: 0.3208\n",
      "Epoch 277/500\n",
      "36/38 [===========================>..] - ETA: 0s - loss: 1.7696 - accuracy: 0.3464\n",
      "Epoch 00277: val_accuracy did not improve from 0.33750\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 1.7782 - accuracy: 0.3450 - val_loss: 1.8948 - val_accuracy: 0.3250\n",
      "Epoch 278/500\n",
      "37/38 [============================>.] - ETA: 0s - loss: 1.7706 - accuracy: 0.3471\n",
      "Epoch 00278: val_accuracy did not improve from 0.33750\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7698 - accuracy: 0.3483 - val_loss: 1.9016 - val_accuracy: 0.3375\n",
      "Epoch 279/500\n",
      "37/38 [============================>.] - ETA: 0s - loss: 1.7901 - accuracy: 0.3514\n",
      "Epoch 00279: val_accuracy improved from 0.33750 to 0.34583, saving model to best_model.hdf5\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 1.7901 - accuracy: 0.3517 - val_loss: 1.9011 - val_accuracy: 0.3458\n",
      "Epoch 280/500\n",
      "37/38 [============================>.] - ETA: 0s - loss: 1.7574 - accuracy: 0.3615\n",
      "Epoch 00280: val_accuracy did not improve from 0.34583\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7556 - accuracy: 0.3608 - val_loss: 1.9042 - val_accuracy: 0.3292\n",
      "Epoch 281/500\n",
      "29/38 [=====================>........] - ETA: 0s - loss: 1.7859 - accuracy: 0.3653\n",
      "Epoch 00281: val_accuracy did not improve from 0.34583\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7927 - accuracy: 0.3583 - val_loss: 1.8994 - val_accuracy: 0.3333\n",
      "Epoch 282/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.7805 - accuracy: 0.3569\n",
      "Epoch 00282: val_accuracy did not improve from 0.34583\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7913 - accuracy: 0.3475 - val_loss: 1.9001 - val_accuracy: 0.3292\n",
      "Epoch 283/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.7624 - accuracy: 0.3659\n",
      "Epoch 00283: val_accuracy did not improve from 0.34583\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7564 - accuracy: 0.3725 - val_loss: 1.9124 - val_accuracy: 0.3167\n",
      "Epoch 284/500\n",
      "35/38 [==========================>...] - ETA: 0s - loss: 1.7409 - accuracy: 0.3616\n",
      "Epoch 00284: val_accuracy did not improve from 0.34583\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 1.7361 - accuracy: 0.3683 - val_loss: 1.9041 - val_accuracy: 0.3292\n",
      "Epoch 285/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.7828 - accuracy: 0.3531\n",
      "Epoch 00285: val_accuracy did not improve from 0.34583\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7706 - accuracy: 0.3617 - val_loss: 1.9079 - val_accuracy: 0.3333\n",
      "Epoch 286/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.7744 - accuracy: 0.3669\n",
      "Epoch 00286: val_accuracy did not improve from 0.34583\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7707 - accuracy: 0.3708 - val_loss: 1.9071 - val_accuracy: 0.3292\n",
      "Epoch 287/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.7634 - accuracy: 0.3896\n",
      "Epoch 00287: val_accuracy did not improve from 0.34583\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.7563 - accuracy: 0.3917 - val_loss: 1.8989 - val_accuracy: 0.3375\n",
      "Epoch 288/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.7495 - accuracy: 0.3652\n",
      "Epoch 00288: val_accuracy did not improve from 0.34583\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7386 - accuracy: 0.3800 - val_loss: 1.8950 - val_accuracy: 0.3333\n",
      "Epoch 289/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.7755 - accuracy: 0.3639\n",
      "Epoch 00289: val_accuracy did not improve from 0.34583\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7667 - accuracy: 0.3675 - val_loss: 1.9076 - val_accuracy: 0.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 290/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.7717 - accuracy: 0.3760\n",
      "Epoch 00290: val_accuracy did not improve from 0.34583\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.7532 - accuracy: 0.3875 - val_loss: 1.9052 - val_accuracy: 0.3333\n",
      "Epoch 291/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.7486 - accuracy: 0.3730\n",
      "Epoch 00291: val_accuracy did not improve from 0.34583\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.7542 - accuracy: 0.3717 - val_loss: 1.8912 - val_accuracy: 0.3375\n",
      "Epoch 292/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 1.7558 - accuracy: 0.3703\n",
      "Epoch 00292: val_accuracy did not improve from 0.34583\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.7603 - accuracy: 0.3700 - val_loss: 1.8927 - val_accuracy: 0.3333\n",
      "Epoch 293/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.7712 - accuracy: 0.3792\n",
      "Epoch 00293: val_accuracy did not improve from 0.34583\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7544 - accuracy: 0.3758 - val_loss: 1.8964 - val_accuracy: 0.3375\n",
      "Epoch 294/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.7411 - accuracy: 0.3800\n",
      "Epoch 00294: val_accuracy did not improve from 0.34583\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7437 - accuracy: 0.3783 - val_loss: 1.8987 - val_accuracy: 0.3333\n",
      "Epoch 295/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.7699 - accuracy: 0.3701\n",
      "Epoch 00295: val_accuracy did not improve from 0.34583\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7667 - accuracy: 0.3650 - val_loss: 1.8967 - val_accuracy: 0.3375\n",
      "Epoch 296/500\n",
      "29/38 [=====================>........] - ETA: 0s - loss: 1.7387 - accuracy: 0.3772\n",
      "Epoch 00296: val_accuracy did not improve from 0.34583\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7390 - accuracy: 0.3808 - val_loss: 1.9057 - val_accuracy: 0.3250\n",
      "Epoch 297/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.7679 - accuracy: 0.3691\n",
      "Epoch 00297: val_accuracy did not improve from 0.34583\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7660 - accuracy: 0.3583 - val_loss: 1.8952 - val_accuracy: 0.3292\n",
      "Epoch 298/500\n",
      "37/38 [============================>.] - ETA: 0s - loss: 1.7596 - accuracy: 0.3708\n",
      "Epoch 00298: val_accuracy did not improve from 0.34583\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7645 - accuracy: 0.3700 - val_loss: 1.8976 - val_accuracy: 0.3292\n",
      "Epoch 299/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.7383 - accuracy: 0.3975\n",
      "Epoch 00299: val_accuracy did not improve from 0.34583\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.7384 - accuracy: 0.4000 - val_loss: 1.9014 - val_accuracy: 0.3333\n",
      "Epoch 300/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.7459 - accuracy: 0.3721\n",
      "Epoch 00300: val_accuracy did not improve from 0.34583\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7429 - accuracy: 0.3792 - val_loss: 1.8999 - val_accuracy: 0.3292\n",
      "Epoch 301/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.7531 - accuracy: 0.3564\n",
      "Epoch 00301: val_accuracy improved from 0.34583 to 0.35000, saving model to best_model.hdf5\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7675 - accuracy: 0.3550 - val_loss: 1.8943 - val_accuracy: 0.3500\n",
      "Epoch 302/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.7539 - accuracy: 0.3812\n",
      "Epoch 00302: val_accuracy did not improve from 0.35000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7446 - accuracy: 0.3825 - val_loss: 1.8968 - val_accuracy: 0.3333\n",
      "Epoch 303/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.7391 - accuracy: 0.3659\n",
      "Epoch 00303: val_accuracy did not improve from 0.35000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7465 - accuracy: 0.3633 - val_loss: 1.8920 - val_accuracy: 0.3458\n",
      "Epoch 304/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 1.7809 - accuracy: 0.3494\n",
      "Epoch 00304: val_accuracy did not improve from 0.35000\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.7727 - accuracy: 0.3517 - val_loss: 1.8918 - val_accuracy: 0.3500\n",
      "Epoch 305/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 1.7562 - accuracy: 0.3686\n",
      "Epoch 00305: val_accuracy did not improve from 0.35000\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.7507 - accuracy: 0.3683 - val_loss: 1.8969 - val_accuracy: 0.3375\n",
      "Epoch 306/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.7421 - accuracy: 0.3833\n",
      "Epoch 00306: val_accuracy did not improve from 0.35000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7380 - accuracy: 0.3867 - val_loss: 1.8933 - val_accuracy: 0.3417\n",
      "Epoch 307/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.7690 - accuracy: 0.3730\n",
      "Epoch 00307: val_accuracy did not improve from 0.35000\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.7782 - accuracy: 0.3792 - val_loss: 1.8909 - val_accuracy: 0.3375\n",
      "Epoch 308/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.7350 - accuracy: 0.3810\n",
      "Epoch 00308: val_accuracy did not improve from 0.35000\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.7454 - accuracy: 0.3775 - val_loss: 1.8975 - val_accuracy: 0.3458\n",
      "Epoch 309/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.7473 - accuracy: 0.3750\n",
      "Epoch 00309: val_accuracy did not improve from 0.35000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7367 - accuracy: 0.3850 - val_loss: 1.8906 - val_accuracy: 0.3292\n",
      "Epoch 310/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.7040 - accuracy: 0.3945\n",
      "Epoch 00310: val_accuracy did not improve from 0.35000\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.7148 - accuracy: 0.3892 - val_loss: 1.8973 - val_accuracy: 0.3167\n",
      "Epoch 311/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.7388 - accuracy: 0.3750\n",
      "Epoch 00311: val_accuracy did not improve from 0.35000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7255 - accuracy: 0.3825 - val_loss: 1.8871 - val_accuracy: 0.3417\n",
      "Epoch 312/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.7565 - accuracy: 0.3574\n",
      "Epoch 00312: val_accuracy did not improve from 0.35000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7506 - accuracy: 0.3650 - val_loss: 1.8782 - val_accuracy: 0.3458\n",
      "Epoch 313/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.7458 - accuracy: 0.3740\n",
      "Epoch 00313: val_accuracy did not improve from 0.35000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7355 - accuracy: 0.3817 - val_loss: 1.8847 - val_accuracy: 0.3333\n",
      "Epoch 314/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 1.7269 - accuracy: 0.3778\n",
      "Epoch 00314: val_accuracy did not improve from 0.35000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 1.7320 - accuracy: 0.3733 - val_loss: 1.8937 - val_accuracy: 0.3250\n",
      "Epoch 315/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.7298 - accuracy: 0.3906\n",
      "Epoch 00315: val_accuracy did not improve from 0.35000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 1.7261 - accuracy: 0.3958 - val_loss: 1.8869 - val_accuracy: 0.3208\n",
      "Epoch 316/500\n",
      "37/38 [============================>.] - ETA: 0s - loss: 1.7254 - accuracy: 0.3851\n",
      "Epoch 00316: val_accuracy did not improve from 0.35000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7269 - accuracy: 0.3825 - val_loss: 1.8778 - val_accuracy: 0.3458\n",
      "Epoch 317/500\n",
      "38/38 [==============================] - ETA: 0s - loss: 1.7676 - accuracy: 0.3517\n",
      "Epoch 00317: val_accuracy did not improve from 0.35000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7676 - accuracy: 0.3517 - val_loss: 1.8859 - val_accuracy: 0.3292\n",
      "Epoch 318/500\n",
      "36/38 [===========================>..] - ETA: 0s - loss: 1.7440 - accuracy: 0.3828\n",
      "Epoch 00318: val_accuracy did not improve from 0.35000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 1.7467 - accuracy: 0.3833 - val_loss: 1.8902 - val_accuracy: 0.3333\n",
      "Epoch 319/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/38 [=========================>....] - ETA: 0s - loss: 1.7447 - accuracy: 0.3888\n",
      "Epoch 00319: val_accuracy did not improve from 0.35000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 1.7416 - accuracy: 0.3933 - val_loss: 1.8769 - val_accuracy: 0.3458\n",
      "Epoch 320/500\n",
      "37/38 [============================>.] - ETA: 0s - loss: 1.7414 - accuracy: 0.3606\n",
      "Epoch 00320: val_accuracy did not improve from 0.35000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7410 - accuracy: 0.3600 - val_loss: 1.8822 - val_accuracy: 0.3500\n",
      "Epoch 321/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 1.7139 - accuracy: 0.3996\n",
      "Epoch 00321: val_accuracy did not improve from 0.35000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 1.7087 - accuracy: 0.4017 - val_loss: 1.8831 - val_accuracy: 0.3333\n",
      "Epoch 322/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 1.7436 - accuracy: 0.3805\n",
      "Epoch 00322: val_accuracy did not improve from 0.35000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 1.7441 - accuracy: 0.3783 - val_loss: 1.8910 - val_accuracy: 0.3458\n",
      "Epoch 323/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 1.7541 - accuracy: 0.3854\n",
      "Epoch 00323: val_accuracy did not improve from 0.35000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 1.7563 - accuracy: 0.3817 - val_loss: 1.8926 - val_accuracy: 0.3292\n",
      "Epoch 324/500\n",
      "36/38 [===========================>..] - ETA: 0s - loss: 1.7390 - accuracy: 0.3767\n",
      "Epoch 00324: val_accuracy did not improve from 0.35000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 1.7407 - accuracy: 0.3750 - val_loss: 1.8833 - val_accuracy: 0.3292\n",
      "Epoch 325/500\n",
      "35/38 [==========================>...] - ETA: 0s - loss: 1.7389 - accuracy: 0.3830\n",
      "Epoch 00325: val_accuracy did not improve from 0.35000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 1.7314 - accuracy: 0.3850 - val_loss: 1.8864 - val_accuracy: 0.3417\n",
      "Epoch 326/500\n",
      "37/38 [============================>.] - ETA: 0s - loss: 1.7251 - accuracy: 0.3809\n",
      "Epoch 00326: val_accuracy did not improve from 0.35000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7267 - accuracy: 0.3808 - val_loss: 1.8826 - val_accuracy: 0.3417\n",
      "Epoch 327/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.7338 - accuracy: 0.3771\n",
      "Epoch 00327: val_accuracy did not improve from 0.35000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7425 - accuracy: 0.3758 - val_loss: 1.8873 - val_accuracy: 0.3417\n",
      "Epoch 328/500\n",
      "29/38 [=====================>........] - ETA: 0s - loss: 1.7474 - accuracy: 0.3588\n",
      "Epoch 00328: val_accuracy did not improve from 0.35000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7206 - accuracy: 0.3825 - val_loss: 1.8847 - val_accuracy: 0.3417\n",
      "Epoch 329/500\n",
      "37/38 [============================>.] - ETA: 0s - loss: 1.7492 - accuracy: 0.3733\n",
      "Epoch 00329: val_accuracy did not improve from 0.35000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7491 - accuracy: 0.3733 - val_loss: 1.8873 - val_accuracy: 0.3417\n",
      "Epoch 330/500\n",
      "37/38 [============================>.] - ETA: 0s - loss: 1.7422 - accuracy: 0.3801\n",
      "Epoch 00330: val_accuracy improved from 0.35000 to 0.35833, saving model to best_model.hdf5\n",
      "38/38 [==============================] - 1s 13ms/step - loss: 1.7442 - accuracy: 0.3800 - val_loss: 1.8834 - val_accuracy: 0.3583\n",
      "Epoch 331/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.7333 - accuracy: 0.3838\n",
      "Epoch 00331: val_accuracy did not improve from 0.35833\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 1.7279 - accuracy: 0.3817 - val_loss: 1.8885 - val_accuracy: 0.3333\n",
      "Epoch 332/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.7304 - accuracy: 0.3911\n",
      "Epoch 00332: val_accuracy did not improve from 0.35833\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 1.7162 - accuracy: 0.3917 - val_loss: 1.8916 - val_accuracy: 0.3292\n",
      "Epoch 333/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 1.7224 - accuracy: 0.3842\n",
      "Epoch 00333: val_accuracy did not improve from 0.35833\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 1.7326 - accuracy: 0.3767 - val_loss: 1.8813 - val_accuracy: 0.3417\n",
      "Epoch 334/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.7256 - accuracy: 0.3792\n",
      "Epoch 00334: val_accuracy did not improve from 0.35833\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7299 - accuracy: 0.3800 - val_loss: 1.8697 - val_accuracy: 0.3500\n",
      "Epoch 335/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.7166 - accuracy: 0.3885\n",
      "Epoch 00335: val_accuracy did not improve from 0.35833\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7131 - accuracy: 0.3958 - val_loss: 1.8774 - val_accuracy: 0.3542\n",
      "Epoch 336/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.7339 - accuracy: 0.3667\n",
      "Epoch 00336: val_accuracy did not improve from 0.35833\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7325 - accuracy: 0.3725 - val_loss: 1.8938 - val_accuracy: 0.3208\n",
      "Epoch 337/500\n",
      "29/38 [=====================>........] - ETA: 0s - loss: 1.7024 - accuracy: 0.3976\n",
      "Epoch 00337: val_accuracy did not improve from 0.35833\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7177 - accuracy: 0.3867 - val_loss: 1.8866 - val_accuracy: 0.3208\n",
      "Epoch 338/500\n",
      "37/38 [============================>.] - ETA: 0s - loss: 1.7250 - accuracy: 0.3809\n",
      "Epoch 00338: val_accuracy did not improve from 0.35833\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7219 - accuracy: 0.3833 - val_loss: 1.8756 - val_accuracy: 0.3458\n",
      "Epoch 339/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.7311 - accuracy: 0.3906\n",
      "Epoch 00339: val_accuracy did not improve from 0.35833\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7245 - accuracy: 0.3925 - val_loss: 1.8937 - val_accuracy: 0.3292\n",
      "Epoch 340/500\n",
      "29/38 [=====================>........] - ETA: 0s - loss: 1.7155 - accuracy: 0.3847\n",
      "Epoch 00340: val_accuracy improved from 0.35833 to 0.36250, saving model to best_model.hdf5\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 1.7056 - accuracy: 0.3858 - val_loss: 1.8837 - val_accuracy: 0.3625\n",
      "Epoch 341/500\n",
      "29/38 [=====================>........] - ETA: 0s - loss: 1.7329 - accuracy: 0.3782\n",
      "Epoch 00341: val_accuracy did not improve from 0.36250\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7466 - accuracy: 0.3717 - val_loss: 1.8841 - val_accuracy: 0.3375\n",
      "Epoch 342/500\n",
      "38/38 [==============================] - ETA: 0s - loss: 1.7318 - accuracy: 0.3875\n",
      "Epoch 00342: val_accuracy did not improve from 0.36250\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7318 - accuracy: 0.3875 - val_loss: 1.8876 - val_accuracy: 0.3375\n",
      "Epoch 343/500\n",
      "38/38 [==============================] - ETA: 0s - loss: 1.7336 - accuracy: 0.3758\n",
      "Epoch 00343: val_accuracy did not improve from 0.36250\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7336 - accuracy: 0.3758 - val_loss: 1.8849 - val_accuracy: 0.3333\n",
      "Epoch 344/500\n",
      "29/38 [=====================>........] - ETA: 0s - loss: 1.7181 - accuracy: 0.3782\n",
      "Epoch 00344: val_accuracy did not improve from 0.36250\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7155 - accuracy: 0.3775 - val_loss: 1.8840 - val_accuracy: 0.3500\n",
      "Epoch 345/500\n",
      "38/38 [==============================] - ETA: 0s - loss: 1.7302 - accuracy: 0.3783\n",
      "Epoch 00345: val_accuracy did not improve from 0.36250\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7302 - accuracy: 0.3783 - val_loss: 1.8854 - val_accuracy: 0.3542\n",
      "Epoch 346/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.7322 - accuracy: 0.3881\n",
      "Epoch 00346: val_accuracy did not improve from 0.36250\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7221 - accuracy: 0.3892 - val_loss: 1.8751 - val_accuracy: 0.3625\n",
      "Epoch 347/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.7139 - accuracy: 0.3916\n",
      "Epoch 00347: val_accuracy did not improve from 0.36250\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7288 - accuracy: 0.3867 - val_loss: 1.8716 - val_accuracy: 0.3458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 348/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 1.6784 - accuracy: 0.3996\n",
      "Epoch 00348: val_accuracy did not improve from 0.36250\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.6942 - accuracy: 0.3950 - val_loss: 1.8754 - val_accuracy: 0.3458\n",
      "Epoch 349/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.7173 - accuracy: 0.3885\n",
      "Epoch 00349: val_accuracy did not improve from 0.36250\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7210 - accuracy: 0.3842 - val_loss: 1.8827 - val_accuracy: 0.3542\n",
      "Epoch 350/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.7186 - accuracy: 0.3789\n",
      "Epoch 00350: val_accuracy did not improve from 0.36250\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7223 - accuracy: 0.3800 - val_loss: 1.8758 - val_accuracy: 0.3500\n",
      "Epoch 351/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 1.7005 - accuracy: 0.3873\n",
      "Epoch 00351: val_accuracy did not improve from 0.36250\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.7085 - accuracy: 0.3842 - val_loss: 1.8799 - val_accuracy: 0.3292\n",
      "Epoch 352/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.7420 - accuracy: 0.3770\n",
      "Epoch 00352: val_accuracy did not improve from 0.36250\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7304 - accuracy: 0.3817 - val_loss: 1.8866 - val_accuracy: 0.3292\n",
      "Epoch 353/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6956 - accuracy: 0.4042\n",
      "Epoch 00353: val_accuracy did not improve from 0.36250\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6971 - accuracy: 0.4083 - val_loss: 1.8758 - val_accuracy: 0.3458\n",
      "Epoch 354/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.7039 - accuracy: 0.4146\n",
      "Epoch 00354: val_accuracy did not improve from 0.36250\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7100 - accuracy: 0.4050 - val_loss: 1.8705 - val_accuracy: 0.3375\n",
      "Epoch 355/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.7237 - accuracy: 0.3865\n",
      "Epoch 00355: val_accuracy did not improve from 0.36250\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7188 - accuracy: 0.3842 - val_loss: 1.8759 - val_accuracy: 0.3542\n",
      "Epoch 356/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.7241 - accuracy: 0.3891\n",
      "Epoch 00356: val_accuracy did not improve from 0.36250\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7189 - accuracy: 0.3900 - val_loss: 1.8806 - val_accuracy: 0.3333\n",
      "Epoch 357/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.7113 - accuracy: 0.3750\n",
      "Epoch 00357: val_accuracy did not improve from 0.36250\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7053 - accuracy: 0.3758 - val_loss: 1.8766 - val_accuracy: 0.3500\n",
      "Epoch 358/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.7226 - accuracy: 0.3921\n",
      "Epoch 00358: val_accuracy did not improve from 0.36250\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7207 - accuracy: 0.3975 - val_loss: 1.8804 - val_accuracy: 0.3458\n",
      "Epoch 359/500\n",
      "35/38 [==========================>...] - ETA: 0s - loss: 1.6832 - accuracy: 0.3839\n",
      "Epoch 00359: val_accuracy did not improve from 0.36250\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 1.6811 - accuracy: 0.3817 - val_loss: 1.8874 - val_accuracy: 0.3375\n",
      "Epoch 360/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.7192 - accuracy: 0.4031\n",
      "Epoch 00360: val_accuracy did not improve from 0.36250\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7105 - accuracy: 0.3975 - val_loss: 1.8717 - val_accuracy: 0.3500\n",
      "Epoch 361/500\n",
      "37/38 [============================>.] - ETA: 0s - loss: 1.7032 - accuracy: 0.3885\n",
      "Epoch 00361: val_accuracy did not improve from 0.36250\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 1.7028 - accuracy: 0.3883 - val_loss: 1.8756 - val_accuracy: 0.3333\n",
      "Epoch 362/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 1.7008 - accuracy: 0.4025\n",
      "Epoch 00362: val_accuracy improved from 0.36250 to 0.37083, saving model to best_model.hdf5\n",
      "38/38 [==============================] - 0s 11ms/step - loss: 1.7064 - accuracy: 0.3942 - val_loss: 1.8642 - val_accuracy: 0.3708\n",
      "Epoch 363/500\n",
      "36/38 [===========================>..] - ETA: 0s - loss: 1.6955 - accuracy: 0.3993\n",
      "Epoch 00363: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 1.6964 - accuracy: 0.4008 - val_loss: 1.8762 - val_accuracy: 0.3417\n",
      "Epoch 364/500\n",
      "38/38 [==============================] - ETA: 0s - loss: 1.6940 - accuracy: 0.4083\n",
      "Epoch 00364: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6940 - accuracy: 0.4083 - val_loss: 1.8720 - val_accuracy: 0.3583\n",
      "Epoch 365/500\n",
      "35/38 [==========================>...] - ETA: 0s - loss: 1.7107 - accuracy: 0.3946\n",
      "Epoch 00365: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 1.7078 - accuracy: 0.3958 - val_loss: 1.8738 - val_accuracy: 0.3375\n",
      "Epoch 366/500\n",
      "29/38 [=====================>........] - ETA: 0s - loss: 1.7125 - accuracy: 0.3858\n",
      "Epoch 00366: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 1.7094 - accuracy: 0.3850 - val_loss: 1.8865 - val_accuracy: 0.3333\n",
      "Epoch 367/500\n",
      "37/38 [============================>.] - ETA: 0s - loss: 1.6785 - accuracy: 0.4046\n",
      "Epoch 00367: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6775 - accuracy: 0.4050 - val_loss: 1.8826 - val_accuracy: 0.3417\n",
      "Epoch 368/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.6936 - accuracy: 0.3896\n",
      "Epoch 00368: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6931 - accuracy: 0.3883 - val_loss: 1.8736 - val_accuracy: 0.3500\n",
      "Epoch 369/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.6870 - accuracy: 0.4010\n",
      "Epoch 00369: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6985 - accuracy: 0.3958 - val_loss: 1.8726 - val_accuracy: 0.3542\n",
      "Epoch 370/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.7179 - accuracy: 0.3931\n",
      "Epoch 00370: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7193 - accuracy: 0.3817 - val_loss: 1.8759 - val_accuracy: 0.3125\n",
      "Epoch 371/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.7171 - accuracy: 0.3771\n",
      "Epoch 00371: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7188 - accuracy: 0.3817 - val_loss: 1.8845 - val_accuracy: 0.3083\n",
      "Epoch 372/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.7352 - accuracy: 0.3865\n",
      "Epoch 00372: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7409 - accuracy: 0.3883 - val_loss: 1.8729 - val_accuracy: 0.3125\n",
      "Epoch 373/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 1.6908 - accuracy: 0.4017\n",
      "Epoch 00373: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 1.6874 - accuracy: 0.4008 - val_loss: 1.8732 - val_accuracy: 0.3250\n",
      "Epoch 374/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.7150 - accuracy: 0.3831\n",
      "Epoch 00374: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7086 - accuracy: 0.3875 - val_loss: 1.8779 - val_accuracy: 0.3417\n",
      "Epoch 375/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.7007 - accuracy: 0.4000\n",
      "Epoch 00375: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7027 - accuracy: 0.3917 - val_loss: 1.8731 - val_accuracy: 0.3458\n",
      "Epoch 376/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.7123 - accuracy: 0.4042\n",
      "Epoch 00376: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6970 - accuracy: 0.4133 - val_loss: 1.8694 - val_accuracy: 0.3458\n",
      "Epoch 377/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/38 [=====================>........] - ETA: 0s - loss: 1.6659 - accuracy: 0.4192\n",
      "Epoch 00377: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6824 - accuracy: 0.4108 - val_loss: 1.8679 - val_accuracy: 0.3458\n",
      "Epoch 378/500\n",
      "37/38 [============================>.] - ETA: 0s - loss: 1.6928 - accuracy: 0.4029\n",
      "Epoch 00378: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6977 - accuracy: 0.3983 - val_loss: 1.8737 - val_accuracy: 0.3625\n",
      "Epoch 379/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.7099 - accuracy: 0.3851\n",
      "Epoch 00379: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7011 - accuracy: 0.3850 - val_loss: 1.8746 - val_accuracy: 0.3375\n",
      "Epoch 380/500\n",
      "37/38 [============================>.] - ETA: 0s - loss: 1.7044 - accuracy: 0.3877\n",
      "Epoch 00380: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7052 - accuracy: 0.3867 - val_loss: 1.8728 - val_accuracy: 0.3500\n",
      "Epoch 381/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.6933 - accuracy: 0.3948\n",
      "Epoch 00381: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6987 - accuracy: 0.3908 - val_loss: 1.8765 - val_accuracy: 0.3708\n",
      "Epoch 382/500\n",
      "36/38 [===========================>..] - ETA: 0s - loss: 1.6916 - accuracy: 0.3845\n",
      "Epoch 00382: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6855 - accuracy: 0.3883 - val_loss: 1.8801 - val_accuracy: 0.3250\n",
      "Epoch 383/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.6800 - accuracy: 0.4010\n",
      "Epoch 00383: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6820 - accuracy: 0.4075 - val_loss: 1.8797 - val_accuracy: 0.3292\n",
      "Epoch 384/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6965 - accuracy: 0.4032\n",
      "Epoch 00384: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7034 - accuracy: 0.3892 - val_loss: 1.8671 - val_accuracy: 0.3417\n",
      "Epoch 385/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6869 - accuracy: 0.4062\n",
      "Epoch 00385: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6873 - accuracy: 0.4050 - val_loss: 1.8706 - val_accuracy: 0.3208\n",
      "Epoch 386/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.6944 - accuracy: 0.3945\n",
      "Epoch 00386: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6913 - accuracy: 0.3958 - val_loss: 1.8731 - val_accuracy: 0.3292\n",
      "Epoch 387/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.6698 - accuracy: 0.4135\n",
      "Epoch 00387: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6721 - accuracy: 0.4083 - val_loss: 1.8693 - val_accuracy: 0.3375\n",
      "Epoch 388/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6823 - accuracy: 0.3871\n",
      "Epoch 00388: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6940 - accuracy: 0.3817 - val_loss: 1.8682 - val_accuracy: 0.3333\n",
      "Epoch 389/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.7083 - accuracy: 0.4043\n",
      "Epoch 00389: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7013 - accuracy: 0.4067 - val_loss: 1.8676 - val_accuracy: 0.3500\n",
      "Epoch 390/500\n",
      "29/38 [=====================>........] - ETA: 0s - loss: 1.6606 - accuracy: 0.4235\n",
      "Epoch 00390: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6614 - accuracy: 0.4150 - val_loss: 1.8731 - val_accuracy: 0.3375\n",
      "Epoch 391/500\n",
      "36/38 [===========================>..] - ETA: 0s - loss: 1.6949 - accuracy: 0.4028\n",
      "Epoch 00391: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 1.6929 - accuracy: 0.4017 - val_loss: 1.8794 - val_accuracy: 0.3458\n",
      "Epoch 392/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.7132 - accuracy: 0.4135\n",
      "Epoch 00392: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7102 - accuracy: 0.4042 - val_loss: 1.8703 - val_accuracy: 0.3375\n",
      "Epoch 393/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 1.6799 - accuracy: 0.4271\n",
      "Epoch 00393: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.6898 - accuracy: 0.4133 - val_loss: 1.8688 - val_accuracy: 0.3542\n",
      "Epoch 394/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6729 - accuracy: 0.3992\n",
      "Epoch 00394: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6735 - accuracy: 0.4050 - val_loss: 1.8703 - val_accuracy: 0.3333\n",
      "Epoch 395/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.6965 - accuracy: 0.3994\n",
      "Epoch 00395: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6983 - accuracy: 0.4008 - val_loss: 1.8676 - val_accuracy: 0.3458\n",
      "Epoch 396/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6780 - accuracy: 0.4173\n",
      "Epoch 00396: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6780 - accuracy: 0.4167 - val_loss: 1.8694 - val_accuracy: 0.3500\n",
      "Epoch 397/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6910 - accuracy: 0.3740\n",
      "Epoch 00397: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6916 - accuracy: 0.3758 - val_loss: 1.8754 - val_accuracy: 0.3208\n",
      "Epoch 398/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6831 - accuracy: 0.4083\n",
      "Epoch 00398: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7009 - accuracy: 0.4008 - val_loss: 1.8779 - val_accuracy: 0.3000\n",
      "Epoch 399/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.7072 - accuracy: 0.4189\n",
      "Epoch 00399: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7089 - accuracy: 0.4133 - val_loss: 1.8656 - val_accuracy: 0.3458\n",
      "Epoch 400/500\n",
      "29/38 [=====================>........] - ETA: 0s - loss: 1.6833 - accuracy: 0.3933\n",
      "Epoch 00400: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6687 - accuracy: 0.3992 - val_loss: 1.8695 - val_accuracy: 0.3417\n",
      "Epoch 401/500\n",
      "29/38 [=====================>........] - ETA: 0s - loss: 1.6757 - accuracy: 0.3933\n",
      "Epoch 00401: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6843 - accuracy: 0.4042 - val_loss: 1.8612 - val_accuracy: 0.3458\n",
      "Epoch 402/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6906 - accuracy: 0.3931\n",
      "Epoch 00402: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6966 - accuracy: 0.3967 - val_loss: 1.8745 - val_accuracy: 0.3458\n",
      "Epoch 403/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6561 - accuracy: 0.4214\n",
      "Epoch 00403: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6619 - accuracy: 0.4208 - val_loss: 1.8734 - val_accuracy: 0.3375\n",
      "Epoch 404/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.6787 - accuracy: 0.4146\n",
      "Epoch 00404: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6910 - accuracy: 0.4108 - val_loss: 1.8726 - val_accuracy: 0.3458\n",
      "Epoch 405/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.6709 - accuracy: 0.3958\n",
      "Epoch 00405: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6763 - accuracy: 0.3950 - val_loss: 1.8729 - val_accuracy: 0.3458\n",
      "Epoch 406/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/38 [======================>.......] - ETA: 0s - loss: 1.6868 - accuracy: 0.4000\n",
      "Epoch 00406: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6876 - accuracy: 0.3983 - val_loss: 1.8719 - val_accuracy: 0.3375\n",
      "Epoch 407/500\n",
      "38/38 [==============================] - ETA: 0s - loss: 1.6809 - accuracy: 0.3892\n",
      "Epoch 00407: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6809 - accuracy: 0.3892 - val_loss: 1.8695 - val_accuracy: 0.3292\n",
      "Epoch 408/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6766 - accuracy: 0.4133\n",
      "Epoch 00408: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6725 - accuracy: 0.4125 - val_loss: 1.8637 - val_accuracy: 0.3500\n",
      "Epoch 409/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6736 - accuracy: 0.4113\n",
      "Epoch 00409: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6748 - accuracy: 0.4058 - val_loss: 1.8665 - val_accuracy: 0.3500\n",
      "Epoch 410/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6970 - accuracy: 0.4093\n",
      "Epoch 00410: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6893 - accuracy: 0.4108 - val_loss: 1.8689 - val_accuracy: 0.3458\n",
      "Epoch 411/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 1.6919 - accuracy: 0.3989\n",
      "Epoch 00411: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.6957 - accuracy: 0.3992 - val_loss: 1.8665 - val_accuracy: 0.3500\n",
      "Epoch 412/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.6673 - accuracy: 0.3979\n",
      "Epoch 00412: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6526 - accuracy: 0.4108 - val_loss: 1.8701 - val_accuracy: 0.3458\n",
      "Epoch 413/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6906 - accuracy: 0.4042\n",
      "Epoch 00413: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6819 - accuracy: 0.4042 - val_loss: 1.8703 - val_accuracy: 0.3375\n",
      "Epoch 414/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.6832 - accuracy: 0.4000\n",
      "Epoch 00414: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6830 - accuracy: 0.4017 - val_loss: 1.8654 - val_accuracy: 0.3375\n",
      "Epoch 415/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6829 - accuracy: 0.4163\n",
      "Epoch 00415: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6884 - accuracy: 0.4142 - val_loss: 1.8656 - val_accuracy: 0.3500\n",
      "Epoch 416/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.6513 - accuracy: 0.4131\n",
      "Epoch 00416: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.6627 - accuracy: 0.4108 - val_loss: 1.8662 - val_accuracy: 0.3333\n",
      "Epoch 417/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 1.6845 - accuracy: 0.4108\n",
      "Epoch 00417: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.6759 - accuracy: 0.4125 - val_loss: 1.8752 - val_accuracy: 0.3417\n",
      "Epoch 418/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.7218 - accuracy: 0.3838\n",
      "Epoch 00418: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.7135 - accuracy: 0.3883 - val_loss: 1.8784 - val_accuracy: 0.3333\n",
      "Epoch 419/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.6671 - accuracy: 0.4271\n",
      "Epoch 00419: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6625 - accuracy: 0.4267 - val_loss: 1.8672 - val_accuracy: 0.3417\n",
      "Epoch 420/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 1.6708 - accuracy: 0.4081\n",
      "Epoch 00420: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.6611 - accuracy: 0.4100 - val_loss: 1.8723 - val_accuracy: 0.3333\n",
      "Epoch 421/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.6450 - accuracy: 0.4170\n",
      "Epoch 00421: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6454 - accuracy: 0.4092 - val_loss: 1.8689 - val_accuracy: 0.3542\n",
      "Epoch 422/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6771 - accuracy: 0.4012\n",
      "Epoch 00422: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6714 - accuracy: 0.3983 - val_loss: 1.8690 - val_accuracy: 0.3292\n",
      "Epoch 423/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 1.6907 - accuracy: 0.4025\n",
      "Epoch 00423: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.6890 - accuracy: 0.4033 - val_loss: 1.8660 - val_accuracy: 0.3417\n",
      "Epoch 424/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 1.6689 - accuracy: 0.4182\n",
      "Epoch 00424: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.6707 - accuracy: 0.4150 - val_loss: 1.8730 - val_accuracy: 0.3250\n",
      "Epoch 425/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 1.7011 - accuracy: 0.4034\n",
      "Epoch 00425: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.6916 - accuracy: 0.4083 - val_loss: 1.8692 - val_accuracy: 0.3375\n",
      "Epoch 426/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6876 - accuracy: 0.3972\n",
      "Epoch 00426: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6806 - accuracy: 0.4008 - val_loss: 1.8788 - val_accuracy: 0.3167\n",
      "Epoch 427/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.6778 - accuracy: 0.4177\n",
      "Epoch 00427: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6739 - accuracy: 0.4133 - val_loss: 1.8709 - val_accuracy: 0.3333\n",
      "Epoch 428/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6947 - accuracy: 0.3931\n",
      "Epoch 00428: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6809 - accuracy: 0.3942 - val_loss: 1.8672 - val_accuracy: 0.3375\n",
      "Epoch 429/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.6986 - accuracy: 0.3828\n",
      "Epoch 00429: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.6977 - accuracy: 0.3892 - val_loss: 1.8736 - val_accuracy: 0.3333\n",
      "Epoch 430/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.6739 - accuracy: 0.4141\n",
      "Epoch 00430: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.6845 - accuracy: 0.4117 - val_loss: 1.8665 - val_accuracy: 0.3542\n",
      "Epoch 431/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 1.6829 - accuracy: 0.4129\n",
      "Epoch 00431: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.6797 - accuracy: 0.4133 - val_loss: 1.8688 - val_accuracy: 0.3458\n",
      "Epoch 432/500\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 1.6688 - accuracy: 0.3989\n",
      "Epoch 00432: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 1.6670 - accuracy: 0.3983 - val_loss: 1.8625 - val_accuracy: 0.3417\n",
      "Epoch 433/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.6886 - accuracy: 0.4010\n",
      "Epoch 00433: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6662 - accuracy: 0.4033 - val_loss: 1.8657 - val_accuracy: 0.3292\n",
      "Epoch 434/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6663 - accuracy: 0.4002\n",
      "Epoch 00434: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6689 - accuracy: 0.4025 - val_loss: 1.8714 - val_accuracy: 0.3542\n",
      "Epoch 435/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/38 [========================>.....] - ETA: 0s - loss: 1.6674 - accuracy: 0.3984\n",
      "Epoch 00435: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6722 - accuracy: 0.3992 - val_loss: 1.8708 - val_accuracy: 0.3417\n",
      "Epoch 436/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.6857 - accuracy: 0.4083\n",
      "Epoch 00436: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6855 - accuracy: 0.3992 - val_loss: 1.8760 - val_accuracy: 0.3375\n",
      "Epoch 437/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6654 - accuracy: 0.4032\n",
      "Epoch 00437: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6777 - accuracy: 0.3992 - val_loss: 1.8756 - val_accuracy: 0.3500\n",
      "Epoch 438/500\n",
      "37/38 [============================>.] - ETA: 0s - loss: 1.6988 - accuracy: 0.4012\n",
      "Epoch 00438: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6937 - accuracy: 0.4042 - val_loss: 1.8812 - val_accuracy: 0.3458\n",
      "Epoch 439/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.6766 - accuracy: 0.4125\n",
      "Epoch 00439: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6731 - accuracy: 0.4075 - val_loss: 1.8709 - val_accuracy: 0.3458\n",
      "Epoch 440/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6793 - accuracy: 0.4214\n",
      "Epoch 00440: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6860 - accuracy: 0.4192 - val_loss: 1.8689 - val_accuracy: 0.3417\n",
      "Epoch 441/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6359 - accuracy: 0.4143\n",
      "Epoch 00441: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6394 - accuracy: 0.4075 - val_loss: 1.8631 - val_accuracy: 0.3292\n",
      "Epoch 442/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6866 - accuracy: 0.4012\n",
      "Epoch 00442: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6673 - accuracy: 0.4100 - val_loss: 1.8662 - val_accuracy: 0.3208\n",
      "Epoch 443/500\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 1.6938 - accuracy: 0.4072\n",
      "Epoch 00443: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.6929 - accuracy: 0.4058 - val_loss: 1.8677 - val_accuracy: 0.3125\n",
      "Epoch 444/500\n",
      "38/38 [==============================] - ETA: 0s - loss: 1.6734 - accuracy: 0.4008\n",
      "Epoch 00444: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6734 - accuracy: 0.4008 - val_loss: 1.8634 - val_accuracy: 0.3250\n",
      "Epoch 445/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6459 - accuracy: 0.4335\n",
      "Epoch 00445: val_accuracy did not improve from 0.37083\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6477 - accuracy: 0.4258 - val_loss: 1.8635 - val_accuracy: 0.3375\n",
      "Epoch 446/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6813 - accuracy: 0.4052\n",
      "Epoch 00446: val_accuracy improved from 0.37083 to 0.37500, saving model to best_model.hdf5\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6745 - accuracy: 0.3983 - val_loss: 1.8570 - val_accuracy: 0.3750\n",
      "Epoch 447/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6602 - accuracy: 0.4042\n",
      "Epoch 00447: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6547 - accuracy: 0.4067 - val_loss: 1.8635 - val_accuracy: 0.3292\n",
      "Epoch 448/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.6917 - accuracy: 0.4062\n",
      "Epoch 00448: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6935 - accuracy: 0.4050 - val_loss: 1.8630 - val_accuracy: 0.3542\n",
      "Epoch 449/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.6694 - accuracy: 0.3984\n",
      "Epoch 00449: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6691 - accuracy: 0.3983 - val_loss: 1.8698 - val_accuracy: 0.3292\n",
      "Epoch 450/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.6614 - accuracy: 0.4092\n",
      "Epoch 00450: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.6639 - accuracy: 0.3992 - val_loss: 1.8581 - val_accuracy: 0.3458\n",
      "Epoch 451/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6481 - accuracy: 0.4173\n",
      "Epoch 00451: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6532 - accuracy: 0.4075 - val_loss: 1.8658 - val_accuracy: 0.3250\n",
      "Epoch 452/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6699 - accuracy: 0.3881\n",
      "Epoch 00452: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6537 - accuracy: 0.4050 - val_loss: 1.8591 - val_accuracy: 0.3500\n",
      "Epoch 453/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.6654 - accuracy: 0.4268\n",
      "Epoch 00453: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6720 - accuracy: 0.4225 - val_loss: 1.8518 - val_accuracy: 0.3750\n",
      "Epoch 454/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.6291 - accuracy: 0.4208\n",
      "Epoch 00454: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6418 - accuracy: 0.4192 - val_loss: 1.8654 - val_accuracy: 0.3375\n",
      "Epoch 455/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6787 - accuracy: 0.3972\n",
      "Epoch 00455: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6848 - accuracy: 0.3883 - val_loss: 1.8677 - val_accuracy: 0.3625\n",
      "Epoch 456/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6998 - accuracy: 0.4042\n",
      "Epoch 00456: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6958 - accuracy: 0.4033 - val_loss: 1.8677 - val_accuracy: 0.3375\n",
      "Epoch 457/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6518 - accuracy: 0.4224\n",
      "Epoch 00457: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6535 - accuracy: 0.4150 - val_loss: 1.8720 - val_accuracy: 0.3208\n",
      "Epoch 458/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6753 - accuracy: 0.3901\n",
      "Epoch 00458: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6649 - accuracy: 0.3883 - val_loss: 1.8735 - val_accuracy: 0.3167\n",
      "Epoch 459/500\n",
      "29/38 [=====================>........] - ETA: 0s - loss: 1.6520 - accuracy: 0.4030\n",
      "Epoch 00459: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6452 - accuracy: 0.4042 - val_loss: 1.8639 - val_accuracy: 0.3583\n",
      "Epoch 460/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6695 - accuracy: 0.4103\n",
      "Epoch 00460: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6609 - accuracy: 0.4117 - val_loss: 1.8694 - val_accuracy: 0.3292\n",
      "Epoch 461/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.6624 - accuracy: 0.4229\n",
      "Epoch 00461: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.6589 - accuracy: 0.4192 - val_loss: 1.8631 - val_accuracy: 0.3458\n",
      "Epoch 462/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.6652 - accuracy: 0.4094\n",
      "Epoch 00462: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6610 - accuracy: 0.4092 - val_loss: 1.8642 - val_accuracy: 0.3292\n",
      "Epoch 463/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.6479 - accuracy: 0.4156\n",
      "Epoch 00463: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6519 - accuracy: 0.4142 - val_loss: 1.8684 - val_accuracy: 0.3583\n",
      "Epoch 464/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6496 - accuracy: 0.4143\n",
      "Epoch 00464: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6610 - accuracy: 0.4108 - val_loss: 1.8655 - val_accuracy: 0.3583\n",
      "Epoch 465/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6497 - accuracy: 0.4113\n",
      "Epoch 00465: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6521 - accuracy: 0.4133 - val_loss: 1.8679 - val_accuracy: 0.3542\n",
      "Epoch 466/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6602 - accuracy: 0.3982\n",
      "Epoch 00466: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6654 - accuracy: 0.3933 - val_loss: 1.8699 - val_accuracy: 0.3542\n",
      "Epoch 467/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.6511 - accuracy: 0.4083\n",
      "Epoch 00467: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6452 - accuracy: 0.4108 - val_loss: 1.8708 - val_accuracy: 0.3375\n",
      "Epoch 468/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6436 - accuracy: 0.4355\n",
      "Epoch 00468: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6388 - accuracy: 0.4308 - val_loss: 1.8727 - val_accuracy: 0.3333\n",
      "Epoch 469/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6636 - accuracy: 0.4153\n",
      "Epoch 00469: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6507 - accuracy: 0.4217 - val_loss: 1.8738 - val_accuracy: 0.3208\n",
      "Epoch 470/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.6512 - accuracy: 0.4427\n",
      "Epoch 00470: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6492 - accuracy: 0.4375 - val_loss: 1.8691 - val_accuracy: 0.3458\n",
      "Epoch 471/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6546 - accuracy: 0.4224\n",
      "Epoch 00471: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6575 - accuracy: 0.4233 - val_loss: 1.8692 - val_accuracy: 0.3458\n",
      "Epoch 472/500\n",
      "29/38 [=====================>........] - ETA: 0s - loss: 1.6529 - accuracy: 0.4062\n",
      "Epoch 00472: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6521 - accuracy: 0.4150 - val_loss: 1.8596 - val_accuracy: 0.3583\n",
      "Epoch 473/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6139 - accuracy: 0.4325\n",
      "Epoch 00473: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6239 - accuracy: 0.4242 - val_loss: 1.8686 - val_accuracy: 0.3500\n",
      "Epoch 474/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6576 - accuracy: 0.4022\n",
      "Epoch 00474: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6492 - accuracy: 0.4025 - val_loss: 1.8652 - val_accuracy: 0.3292\n",
      "Epoch 475/500\n",
      "38/38 [==============================] - ETA: 0s - loss: 1.6407 - accuracy: 0.4092\n",
      "Epoch 00475: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6407 - accuracy: 0.4092 - val_loss: 1.8613 - val_accuracy: 0.3458\n",
      "Epoch 476/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.6360 - accuracy: 0.4219\n",
      "Epoch 00476: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6313 - accuracy: 0.4233 - val_loss: 1.8684 - val_accuracy: 0.3458\n",
      "Epoch 477/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6299 - accuracy: 0.4254\n",
      "Epoch 00477: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6400 - accuracy: 0.4167 - val_loss: 1.8715 - val_accuracy: 0.3458\n",
      "Epoch 478/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6245 - accuracy: 0.4335\n",
      "Epoch 00478: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6190 - accuracy: 0.4367 - val_loss: 1.8700 - val_accuracy: 0.3417\n",
      "Epoch 479/500\n",
      "29/38 [=====================>........] - ETA: 0s - loss: 1.6246 - accuracy: 0.4106\n",
      "Epoch 00479: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6486 - accuracy: 0.3983 - val_loss: 1.8721 - val_accuracy: 0.3500\n",
      "Epoch 480/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6610 - accuracy: 0.4093\n",
      "Epoch 00480: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6564 - accuracy: 0.4117 - val_loss: 1.8813 - val_accuracy: 0.3083\n",
      "Epoch 481/500\n",
      "32/38 [========================>.....] - ETA: 0s - loss: 1.6199 - accuracy: 0.4150\n",
      "Epoch 00481: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6418 - accuracy: 0.4083 - val_loss: 1.8765 - val_accuracy: 0.3250\n",
      "Epoch 482/500\n",
      "29/38 [=====================>........] - ETA: 0s - loss: 1.6325 - accuracy: 0.4073\n",
      "Epoch 00482: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6368 - accuracy: 0.4125 - val_loss: 1.8786 - val_accuracy: 0.3292\n",
      "Epoch 483/500\n",
      "29/38 [=====================>........] - ETA: 0s - loss: 1.6436 - accuracy: 0.4192\n",
      "Epoch 00483: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6288 - accuracy: 0.4258 - val_loss: 1.8732 - val_accuracy: 0.3333\n",
      "Epoch 484/500\n",
      "38/38 [==============================] - ETA: 0s - loss: 1.6346 - accuracy: 0.4217\n",
      "Epoch 00484: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6346 - accuracy: 0.4217 - val_loss: 1.8775 - val_accuracy: 0.3292\n",
      "Epoch 485/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6542 - accuracy: 0.4234\n",
      "Epoch 00485: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6498 - accuracy: 0.4200 - val_loss: 1.8658 - val_accuracy: 0.3417\n",
      "Epoch 486/500\n",
      "29/38 [=====================>........] - ETA: 0s - loss: 1.6115 - accuracy: 0.4375\n",
      "Epoch 00486: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6336 - accuracy: 0.4183 - val_loss: 1.8767 - val_accuracy: 0.3292\n",
      "Epoch 487/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6472 - accuracy: 0.4365\n",
      "Epoch 00487: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6591 - accuracy: 0.4342 - val_loss: 1.8749 - val_accuracy: 0.3292\n",
      "Epoch 488/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6358 - accuracy: 0.4214\n",
      "Epoch 00488: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6354 - accuracy: 0.4250 - val_loss: 1.8665 - val_accuracy: 0.3292\n",
      "Epoch 489/500\n",
      "37/38 [============================>.] - ETA: 0s - loss: 1.6427 - accuracy: 0.4274\n",
      "Epoch 00489: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6447 - accuracy: 0.4267 - val_loss: 1.8670 - val_accuracy: 0.3333\n",
      "Epoch 490/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6326 - accuracy: 0.4204\n",
      "Epoch 00490: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6210 - accuracy: 0.4283 - val_loss: 1.8692 - val_accuracy: 0.3375\n",
      "Epoch 491/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6521 - accuracy: 0.4073\n",
      "Epoch 00491: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6516 - accuracy: 0.4100 - val_loss: 1.8731 - val_accuracy: 0.3458\n",
      "Epoch 492/500\n",
      "37/38 [============================>.] - ETA: 0s - loss: 1.6621 - accuracy: 0.4181\n",
      "Epoch 00492: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6603 - accuracy: 0.4192 - val_loss: 1.8759 - val_accuracy: 0.3375\n",
      "Epoch 493/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/38 [=====================>........] - ETA: 0s - loss: 1.6617 - accuracy: 0.4106\n",
      "Epoch 00493: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6531 - accuracy: 0.4175 - val_loss: 1.8714 - val_accuracy: 0.3333\n",
      "Epoch 494/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.6267 - accuracy: 0.4167\n",
      "Epoch 00494: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6444 - accuracy: 0.4158 - val_loss: 1.8685 - val_accuracy: 0.3458\n",
      "Epoch 495/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6255 - accuracy: 0.4183\n",
      "Epoch 00495: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6141 - accuracy: 0.4275 - val_loss: 1.8636 - val_accuracy: 0.3458\n",
      "Epoch 496/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6228 - accuracy: 0.4335\n",
      "Epoch 00496: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6292 - accuracy: 0.4342 - val_loss: 1.8697 - val_accuracy: 0.3375\n",
      "Epoch 497/500\n",
      "29/38 [=====================>........] - ETA: 0s - loss: 1.6299 - accuracy: 0.4203\n",
      "Epoch 00497: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6200 - accuracy: 0.4392 - val_loss: 1.8663 - val_accuracy: 0.3417\n",
      "Epoch 498/500\n",
      "30/38 [======================>.......] - ETA: 0s - loss: 1.6077 - accuracy: 0.4448\n",
      "Epoch 00498: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6165 - accuracy: 0.4367 - val_loss: 1.8643 - val_accuracy: 0.3458\n",
      "Epoch 499/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6131 - accuracy: 0.4466\n",
      "Epoch 00499: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6251 - accuracy: 0.4433 - val_loss: 1.8639 - val_accuracy: 0.3667\n",
      "Epoch 500/500\n",
      "31/38 [=======================>......] - ETA: 0s - loss: 1.6227 - accuracy: 0.4385\n",
      "Epoch 00500: val_accuracy did not improve from 0.37500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.6324 - accuracy: 0.4333 - val_loss: 1.8579 - val_accuracy: 0.3542\n",
      "MODEL TRAINING COMPLETE!\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(np.array(train_x), np.array(train_y), epochs=500, callbacks=[ mc], batch_size=32, validation_data=(np.array(test_x),np.array(test_y)))\n",
    "print(\"MODEL TRAINING COMPLETE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Visualization\n",
    "\n",
    "Now we can take a look at how successful our model is and can easily find where overfitting takes place (if at all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(history.history['loss'], label='train') \n",
    "plt.plot(history.history['val_loss'], label='test') \n",
    "plt.legend() \n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='train') \n",
    "plt.plot(history.history['val_accuracy'], label='test') \n",
    "plt.legend() \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
