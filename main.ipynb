{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries\n",
    "\n",
    "For any issues running these modules, use python -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data, separate into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Building Train/Test Data ---\n",
      "Test data is 0.21% of the overall data\n",
      "Train has 1200 files.\n",
      "Test has 240 files.\n",
      "Split is 5:1 train:test\n"
     ]
    }
   ],
   "source": [
    "from fractions import Fraction\n",
    "import random\n",
    "\n",
    "path = \"./data/audio_speech_actors_01-24/\"\n",
    "actors = os.listdir(path)\n",
    "\n",
    "# We need to categorize the data files according to their emotion. Since the dataset is labelled by emotion (which is encoded into their filenames), we need to break that down\n",
    "# Filename identifiers:\n",
    "# Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
    "#\n",
    "# Vocal channel (01 = speech, 02 = song).\n",
    "#\n",
    "# Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
    "#\n",
    "# Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n",
    "#\n",
    "# Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n",
    "#\n",
    "# Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
    "#\n",
    "# Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n",
    "\n",
    "# We are only focusing on the emotion, so we categorize by the third number (01-08)\n",
    "# according to the dataset's site\n",
    "mapping = {1:\"neutral\", 2:\"calm\", 3:\"happy\", 4:\"sad\", 5:\"angry\", 6:\"fearful\", 7:\"disgust\", 8:\"surprised\"}\n",
    "\n",
    "def load_data(path, return_train_test=False, test_percentage=0.20):\n",
    "    # 1 slot for each of the emotions\n",
    "    emot = []\n",
    "    paths = []\n",
    "    train_test_labels = []\n",
    "    \n",
    "    # Custom making our train/test split\n",
    "    test_threshold = int(len(os.listdir(path)) * test_percentage + 1)    # (At least) 20% of data reserved for testing (important that we do this by actor to prevent data leakage)\n",
    "    print(\"Test data is {:0.2f}% of the overall data\".format(1 / (len(os.listdir(path))/test_threshold)))\n",
    "    \n",
    "    # Make a list of actors so we can shuffle order (last few actors will not always be test data each time we load data)\n",
    "    actors = []\n",
    "    for directory in os.listdir(path):\n",
    "        actors.append(directory)\n",
    "    random.shuffle(actors)\n",
    "    \n",
    "    count = 0\n",
    "    data_label = \"test\"\n",
    "    for directory in actors:\n",
    "        count += 1\n",
    "        if (count == test_threshold):\n",
    "            data_label = \"train\"\n",
    "        files = os.path.join(path, directory)\n",
    "        for file in os.listdir(files):\n",
    "            em_num = int(file.split(\"-\")[2])\n",
    "            emot.append(em_num)\n",
    "            train_test_labels.append(data_label)\n",
    "            paths.append(path + directory + \"/\" + file)\n",
    "    \n",
    "    tts = pd.DataFrame(train_test_labels, columns=[\"train_test\"])\n",
    "    ems = pd.DataFrame(emot, columns=['emotion']).replace(mapping)\n",
    "    pths = pd.DataFrame(paths, columns = [\"path\"])\n",
    "    data_file = pd.concat(\n",
    "        [\n",
    "            tts.reset_index(drop=True),\n",
    "            ems.reset_index(drop=True),\n",
    "            pths.reset_index(drop=True)\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    \n",
    "    if return_train_test:\n",
    "        return get_train_test(data_file)\n",
    "    return data_file\n",
    "\n",
    "def get_train_test(data):\n",
    "    grouped = data.groupby(data.train_test)\n",
    "    train = grouped.get_group(\"train\")\n",
    "    test = grouped.get_group(\"test\")\n",
    "    return train, test\n",
    "\n",
    "print(\"--- Building Train/Test Data ---\")\n",
    "data = load_data(path)\n",
    "train, test = get_train_test(data)\n",
    "train_size = train.train_test.value_counts().train\n",
    "test_size = test.train_test.value_counts().test\n",
    "print(\"Train has\", train_size, \"files.\")\n",
    "print(\"Test has\", test_size, \"files.\")\n",
    "\n",
    "ratio = Fraction(train_size, test_size)\n",
    "print(\"Split is\", str(ratio.numerator)+\":\"+str(ratio.denominator), \"train:test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# for m in mapping:\n",
    "#     for i in train[m]:\n",
    "#         X.append(i)\n",
    "#         y.append(m)\n",
    "\n",
    "def gen_mfccs(data, NUM_MFCCs=13):\n",
    "    mfccs = pd.DataFrame(columns=['mfccs'])\n",
    "    \n",
    "    # Get mfccs from each audio file\n",
    "    count=0\n",
    "    for i, j in data.iterrows():\n",
    "        for item in j.items():\n",
    "            if item[0] == 'path':\n",
    "                # Sample rate and duration taken from the kaggle dataset description\n",
    "                file, sample_rate = librosa.load(item[1], res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)\n",
    "                sample_rate = np.array(sample_rate)\n",
    "                mfcc = np.mean(librosa.feature.mfcc(y=file, sr=sample_rate, n_mfcc=NUM_MFCCs), axis=0)\n",
    "                mfccs.loc[count] = [mfcc]\n",
    "                count+=1\n",
    "                break\n",
    "    \n",
    "    # Gen list of mfccs as a dataframe to **manually** concatenate onto data\n",
    "#     mfccs = pd.DataFrame(mfccs, columns = [(\"mfcc_\" + str(num)) for num in range(len(mfccs[0]))])\n",
    "#     data = pd.concat(\n",
    "#         [\n",
    "#             data.reset_index(drop=True),\n",
    "#             mfccs.reset_index(drop=True)\n",
    "#         ],\n",
    "#         axis=1\n",
    "#     )\n",
    "\n",
    "    # Add on these mfccs to the data DataFrame\n",
    "    return pd.concat([data.reset_index(drop=True), pd.DataFrame(mfccs[\"mfccs\"].values.tolist())], axis=1)\n",
    "        \n",
    "data = gen_mfccs(data)\n",
    "data = data.fillna(0)\n",
    "train, test = get_train_test(data)\n",
    "\n",
    "# Save train + test DataFrame file as a csv\n",
    "data.to_csv(\"extracted_data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     train_test    emotion                                               path  \\\n",
      "240       train    neutral  ./data/audio_speech_actors_01-24/Actor_08/03-0...   \n",
      "241       train    neutral  ./data/audio_speech_actors_01-24/Actor_08/03-0...   \n",
      "242       train    neutral  ./data/audio_speech_actors_01-24/Actor_08/03-0...   \n",
      "243       train    neutral  ./data/audio_speech_actors_01-24/Actor_08/03-0...   \n",
      "244       train       calm  ./data/audio_speech_actors_01-24/Actor_08/03-0...   \n",
      "...         ...        ...                                                ...   \n",
      "1435      train  surprised  ./data/audio_speech_actors_01-24/Actor_06/03-0...   \n",
      "1436      train  surprised  ./data/audio_speech_actors_01-24/Actor_06/03-0...   \n",
      "1437      train  surprised  ./data/audio_speech_actors_01-24/Actor_06/03-0...   \n",
      "1438      train  surprised  ./data/audio_speech_actors_01-24/Actor_06/03-0...   \n",
      "1439      train  surprised  ./data/audio_speech_actors_01-24/Actor_06/03-0...   \n",
      "\n",
      "              0          1          2          3          4          5  \\\n",
      "240  -58.452599 -54.180382 -49.728397 -51.274837 -51.939064 -52.804298   \n",
      "241  -56.635319 -53.801495 -48.778366 -49.389030 -52.919968 -52.927910   \n",
      "242  -56.513313 -52.982422 -53.046795 -54.998184 -55.053368 -55.839298   \n",
      "243  -53.718319 -55.116398 -54.395218 -54.102036 -53.421829 -52.040806   \n",
      "244  -54.715317 -55.716309 -57.206558 -53.522114 -51.687614 -51.324093   \n",
      "...         ...        ...        ...        ...        ...        ...   \n",
      "1435 -46.555317 -47.138985 -49.506126 -50.332485 -49.198311 -48.018578   \n",
      "1436 -49.720791 -48.585789 -49.626892 -50.109943 -50.939846 -50.507210   \n",
      "1437 -57.263977 -57.263977 -56.639229 -56.887520 -57.263977 -57.263977   \n",
      "1438 -56.783100 -57.903156 -57.313747 -55.658054 -54.677284 -53.925579   \n",
      "1439 -50.460850 -51.438293 -52.842278 -52.935017 -52.631477 -52.050552   \n",
      "\n",
      "              6  ...        206        207        208        209        210  \\\n",
      "240  -54.458553  ... -54.011433 -51.501019 -52.596626 -52.921890 -52.165344   \n",
      "241  -52.374519  ... -48.259182 -48.500546 -51.232071 -51.535114 -51.117180   \n",
      "242  -56.142193  ... -52.229752 -51.346794 -48.552353 -48.913780 -50.868160   \n",
      "243  -52.316238  ... -50.883095 -50.105244 -50.378658 -52.548416 -52.499649   \n",
      "244  -50.385330  ... -53.763222 -55.250263 -56.162598 -54.456821 -51.828842   \n",
      "...         ...  ...        ...        ...        ...        ...        ...   \n",
      "1435 -48.555622  ... -52.494144 -51.593437 -53.318974 -55.786404 -54.555508   \n",
      "1436 -49.388775  ... -66.464607 -65.734299 -65.762421 -64.644012 -63.755638   \n",
      "1437 -57.263977  ... -57.225266 -57.263977 -57.263977 -56.305752 -56.134872   \n",
      "1438 -50.057102  ... -59.508606 -59.329849 -58.667759 -59.280014 -59.818378   \n",
      "1439 -52.375511  ... -56.148605 -57.138523 -56.890938 -57.386909 -57.918270   \n",
      "\n",
      "            211        212        213        214        215  \n",
      "240  -53.209713 -55.509731 -55.962559 -54.742577 -51.877007  \n",
      "241  -50.224464 -50.940475 -51.356316 -52.761215 -56.884457  \n",
      "242  -51.494400 -51.384605 -52.178658 -54.717129 -55.563065  \n",
      "243  -52.258755 -54.717014 -54.552670 -52.712013 -51.445766  \n",
      "244  -51.606987 -51.162624 -53.035416 -53.412766 -53.346176  \n",
      "...         ...        ...        ...        ...        ...  \n",
      "1435 -53.245281 -55.746098 -57.643330 -54.877254 -54.076523  \n",
      "1436 -64.084137 -64.584000 -64.644936 -65.611000 -67.399963  \n",
      "1437 -55.761616 -55.964268 -57.088032 -56.611214 -56.862770  \n",
      "1438 -59.246090 -58.021259 -58.988949 -59.660332 -59.822720  \n",
      "1439 -58.591789 -57.985630 -57.143559 -57.047550 -58.122089  \n",
      "\n",
      "[1200 rows x 435 columns]\n"
     ]
    }
   ],
   "source": [
    "def get_x_y(data):\n",
    "    rev_mapping = {emotion: num for num, emotion in mapping.items()}\n",
    "    x, y = [], []\n",
    "    for i, j in data.iterrows():\n",
    "        col = (label for label in j.items() if label[0] == 'emotion')\n",
    "        for item in col:\n",
    "            y.append(rev_mapping[item[1]])\n",
    "        count = 0\n",
    "        xs = []\n",
    "        for k in j.items():\n",
    "            if count > 5:\n",
    "                xs.append(k)\n",
    "#                 print(xs)\n",
    "            count += 1\n",
    "#             print(k)\n",
    "        x.append(xs)\n",
    "    return x, y\n",
    "\n",
    "# The numbered columns are mfccs\n",
    "print(train)\n",
    "train_x, trian_y = get_x_y(train)\n",
    "test_x, test_y = get_x_y(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest frequency baseline: 0.13333333333333333\n",
      "Random baseline 0.11666666666666667\n",
      "Random uniform baseline 0.13333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# train a dummy classifier to make predictions based on the most_frequent class value\n",
    "frequent_dummy_classifier = DummyClassifier(strategy=\"most_frequent\")\n",
    "frequent_dummy_classifier.fit(train_x, trian_y)\n",
    "\n",
    "print(\"Highest frequency baseline:\", frequent_dummy_classifier.score(test_x, test_y))\n",
    "\n",
    "# train a dummy classifier to make predictions based on the class values\n",
    "stratified_dummy_classifier = DummyClassifier(strategy=\"stratified\")\n",
    "stratified_dummy_classifier.fit(train_x,trian_y)\n",
    "\n",
    "print(\"Random baseline\", stratified_dummy_classifier.score(test_x, test_y))\n",
    "\n",
    "# train a dummy classifier to make predictions based on uniform selection\n",
    "uniform_dummy_classifier = DummyClassifier(strategy=\"uniform\")\n",
    "uniform_dummy_classifier.fit(train_x,trian_y)\n",
    "\n",
    "print(\"Random uniform baseline\", uniform_dummy_classifier.score(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore dataset\n",
    "hist.head()\n",
    "hist.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Data\n",
    "\n",
    "Visualize the data that we have extracted to better understand trends to expect and routes to go for changing parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO\n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax1.set_title('Visualization Title')\n",
    "ax1.set_xlabel('xLabel')\n",
    "ax1.set_ylabel('yLabel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "If we want to transform the data in any way, we can do it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "\n",
    "labels=[\"01/20\", \"01/21\", \"01/22\"]\n",
    "# Generates an int for each label\n",
    "y=le.fit_transform(labels)\n",
    "\n",
    "# Prints out each date with its int mapping\n",
    "for c in list(le.classes_):\n",
    "    print(le.transform([c])[0], c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reshape data here as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create train set and test set\n",
    "train_set_size = 5000\n",
    "data = np.array(hist)\n",
    "train_data = data[:train_set_size]\n",
    "test_data = data[train_set_size:]\n",
    "x_train = data[:train_set_size,:-1]\n",
    "y_train = data[:train_set_size,-1]\n",
    "x_test = data[train_set_size:,:-1]\n",
    "y_test = data[train_set_size:,-1]\n",
    "x_train = torch.from_numpy(x_train).type(torch.Tensor)\n",
    "x_test = torch.from_numpy(x_test).type(torch.Tensor)\n",
    "y_train = torch.from_numpy(y_train).type(torch.Tensor)\n",
    "y_test = torch.from_numpy(y_test).type(torch.Tensor)\n",
    "\n",
    "# We could try this as it might be a bit easier to use\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_tr, x_val, y_tr, y_val = train_test_split(np.array(data_to_be_added),np.array(y),stratify=y,test_size = 0.2,random_state=777,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Dense, Dropout, Flatten, Conv1D, Input, MaxPooling1D\n",
    "# from keras.models import Model\n",
    "# from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K.clear_session()\n",
    "\n",
    "# inputs = Input(shape=(8000,1))\n",
    "\n",
    "# #First Conv1D layer\n",
    "# conv = Conv1D(8,13, padding='valid', activation='relu', strides=1)(inputs)\n",
    "# conv = MaxPooling1D(3)(conv)\n",
    "# conv = Dropout(0.3)(conv)\n",
    "\n",
    "# #Second Conv1D layer\n",
    "# conv = Conv1D(16, 11, padding='valid', activation='relu', strides=1)(conv)\n",
    "# conv = MaxPooling1D(3)(conv)\n",
    "# conv = Dropout(0.3)(conv)\n",
    "\n",
    "# #Third Conv1D layer\n",
    "# conv = Conv1D(32, 9, padding='valid', activation='relu', strides=1)(conv)\n",
    "# conv = MaxPooling1D(3)(conv)\n",
    "# conv = Dropout(0.3)(conv)\n",
    "\n",
    "# #Flatten layer\n",
    "# conv = Flatten()(conv)\n",
    "\n",
    "# #Dense Layer 1\n",
    "# conv = Dense(128, activation='relu')(conv)\n",
    "# conv = Dropout(0.3)(conv)\n",
    "\n",
    "# # Output layer\n",
    "# outputs = Dense(len(labels), activation='softmax')(conv)\n",
    "\n",
    "# model = Model(inputs, outputs)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "# es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, min_delta=0.0001) \n",
    "# mc = ModelCheckpoint('best_model.hdf5', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(x_tr, y_tr ,epochs=10, callbacks=[es,mc], batch_size=32, validation_data=(x_val,y_val))\n",
    "\n",
    "print(\"MODEL TRAINING COMPLETE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Visualization\n",
    "\n",
    "Now we can take a look at how successful our model is and can easily find where overfitting takes place (if at all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(history.history['loss'], label='train') \n",
    "plt.plot(history.history['val_loss'], label='test') \n",
    "plt.legend() \n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='train') \n",
    "plt.plot(history.history['val_accuracy'], label='test') \n",
    "plt.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the application part (maybe a separate file), we can use our model to predict the future performance\n",
    "\n",
    "from keras.models import load_model\n",
    "model=load_model('best_model.hdf5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
