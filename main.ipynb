{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries\n",
    "\n",
    "For any issues running these modules, use python -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data, separate into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Building Train/Test Data ---\n",
      "Test data is 0.21% of the overall data\n",
      "Train has 1200 files.\n",
      "Test has 240 files.\n",
      "Split is 5:1 train:test\n"
     ]
    }
   ],
   "source": [
    "from fractions import Fraction\n",
    "import random\n",
    "\n",
    "path = \"./data/audio_speech_actors_01-24/\"\n",
    "actors = os.listdir(path)\n",
    "\n",
    "# We need to categorize the data files according to their emotion. Since the dataset is labelled by emotion (which is encoded into their filenames), we need to break that down\n",
    "# Filename identifiers:\n",
    "# Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
    "#\n",
    "# Vocal channel (01 = speech, 02 = song).\n",
    "#\n",
    "# Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
    "#\n",
    "# Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n",
    "#\n",
    "# Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n",
    "#\n",
    "# Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
    "#\n",
    "# Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n",
    "\n",
    "# We are only focusing on the emotion, so we categorize by the third number (01-08)\n",
    "# according to the dataset's site\n",
    "mapping = {1:\"neutral\", 2:\"calm\", 3:\"happy\", 4:\"sad\", 5:\"angry\", 6:\"fearful\", 7:\"disgust\", 8:\"surprised\"}\n",
    "\n",
    "def load_data(path, return_train_test=False, test_percentage=0.20):\n",
    "    # 1 slot for each of the emotions\n",
    "    emot = []\n",
    "    paths = []\n",
    "    train_test_labels = []\n",
    "    \n",
    "    # Custom making our train/test split\n",
    "    test_threshold = int(len(os.listdir(path)) * test_percentage + 1)    # (At least) 20% of data reserved for testing (important that we do this by actor to prevent data leakage)\n",
    "    print(\"Test data is {:0.2f}% of the overall data\".format(1 / (len(os.listdir(path))/test_threshold)))\n",
    "    \n",
    "    # Make a list of actors so we can shuffle order (last few actors will not always be test data each time we load data)\n",
    "    actors = []\n",
    "    for directory in os.listdir(path):\n",
    "        actors.append(directory)\n",
    "    random.shuffle(actors)\n",
    "    \n",
    "    count = 0\n",
    "    data_label = \"test\"\n",
    "    for directory in actors:\n",
    "        count += 1\n",
    "        if (count == test_threshold):\n",
    "            data_label = \"train\"\n",
    "        files = os.path.join(path, directory)\n",
    "        for file in os.listdir(files):\n",
    "            em_num = int(file.split(\"-\")[2])\n",
    "            emot.append(em_num)\n",
    "            train_test_labels.append(data_label)\n",
    "            paths.append(path + directory + \"/\" + file)\n",
    "    \n",
    "    tts = pd.DataFrame(train_test_labels, columns=[\"train_test\"])\n",
    "    ems = pd.DataFrame(emot, columns=['emotion']).replace(mapping)\n",
    "    pths = pd.DataFrame(paths, columns = [\"path\"])\n",
    "    data_file = pd.concat(\n",
    "        [\n",
    "            tts.reset_index(drop=True),\n",
    "            ems.reset_index(drop=True),\n",
    "            pths.reset_index(drop=True)\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    \n",
    "    if return_train_test:\n",
    "        return get_train_test(data_file)\n",
    "    return data_file\n",
    "\n",
    "def get_train_test(data):\n",
    "    grouped = data.groupby(data.train_test)\n",
    "    train = grouped.get_group(\"train\")\n",
    "    test = grouped.get_group(\"test\")\n",
    "    return train, test\n",
    "\n",
    "print(\"--- Building Train/Test Data ---\")\n",
    "data = load_data(path)\n",
    "train, test = get_train_test(data)\n",
    "train_size = train.train_test.value_counts().train\n",
    "test_size = test.train_test.value_counts().test\n",
    "print(\"Train has\", train_size, \"files.\")\n",
    "print(\"Test has\", test_size, \"files.\")\n",
    "\n",
    "ratio = Fraction(train_size, test_size)\n",
    "print(\"Split is\", str(ratio.numerator)+\":\"+str(ratio.denominator), \"train:test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# for m in mapping:\n",
    "#     for i in train[m]:\n",
    "#         X.append(i)\n",
    "#         y.append(m)\n",
    "\n",
    "def gen_mfccs(data, NUM_MFCCs=13):\n",
    "    mfccs = pd.DataFrame(columns=['mfccs'])\n",
    "    \n",
    "    # Get mfccs from each audio file\n",
    "    count=0\n",
    "    for i, j in data.iterrows():\n",
    "        for item in j.items():\n",
    "            if item[0] == 'path':\n",
    "                # Sample rate and duration taken from the kaggle dataset description\n",
    "                file, sample_rate = librosa.load(item[1], res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)\n",
    "                sample_rate = np.array(sample_rate)\n",
    "                mfcc = np.mean(librosa.feature.mfcc(y=file, sr=sample_rate, n_mfcc=NUM_MFCCs), axis=0)\n",
    "                mfccs.loc[count] = [mfcc]\n",
    "                count+=1\n",
    "                break\n",
    "    \n",
    "    # Gen list of mfccs as a dataframe to **manually** concatenate onto data\n",
    "#     mfccs = pd.DataFrame(mfccs, columns = [(\"mfcc_\" + str(num)) for num in range(len(mfccs[0]))])\n",
    "#     data = pd.concat(\n",
    "#         [\n",
    "#             data.reset_index(drop=True),\n",
    "#             mfccs.reset_index(drop=True)\n",
    "#         ],\n",
    "#         axis=1\n",
    "#     )\n",
    "\n",
    "    # Add on these mfccs to the data DataFrame\n",
    "    return pd.concat([data.reset_index(drop=True), pd.DataFrame(mfccs[\"mfccs\"].values.tolist())], axis=1)\n",
    "        \n",
    "data = gen_mfccs(data)\n",
    "data = data.fillna(0)\n",
    "train, test = get_train_test(data)\n",
    "\n",
    "# Save train + test DataFrame file as a csv\n",
    "data.to_csv(\"extracted_data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     train_test  emotion                                               path  \\\n",
      "240       train    angry  ./data/audio_speech_actors_01-24/Actor_22/03-0...   \n",
      "241       train  fearful  ./data/audio_speech_actors_01-24/Actor_22/03-0...   \n",
      "242       train  fearful  ./data/audio_speech_actors_01-24/Actor_22/03-0...   \n",
      "243       train    angry  ./data/audio_speech_actors_01-24/Actor_22/03-0...   \n",
      "244       train  disgust  ./data/audio_speech_actors_01-24/Actor_22/03-0...   \n",
      "...         ...      ...                                                ...   \n",
      "1435      train    happy  ./data/audio_speech_actors_01-24/Actor_17/03-0...   \n",
      "1436      train    happy  ./data/audio_speech_actors_01-24/Actor_17/03-0...   \n",
      "1437      train     calm  ./data/audio_speech_actors_01-24/Actor_17/03-0...   \n",
      "1438      train     calm  ./data/audio_speech_actors_01-24/Actor_17/03-0...   \n",
      "1439      train  neutral  ./data/audio_speech_actors_01-24/Actor_17/03-0...   \n",
      "\n",
      "              0          1          2          3          4          5  \\\n",
      "240  -53.678326 -54.377914 -55.140835 -56.300232 -55.326923 -55.119808   \n",
      "241  -57.705177 -57.728172 -56.672729 -56.152203 -57.413799 -58.473492   \n",
      "242  -49.289055 -49.289055 -49.048580 -48.872910 -49.150307 -49.238182   \n",
      "243  -47.356522 -47.379227 -47.379227 -47.395439 -47.436260 -47.356297   \n",
      "244  -53.010777 -54.685703 -56.436161 -57.489353 -56.092094 -55.378746   \n",
      "...         ...        ...        ...        ...        ...        ...   \n",
      "1435 -42.217430 -40.560471 -40.231331 -41.971317 -41.413189 -41.679756   \n",
      "1436 -50.683445 -49.582844 -49.573154 -49.530697 -51.778385 -49.248268   \n",
      "1437 -53.676575 -52.471817 -51.828735 -52.794479 -54.578403 -54.932976   \n",
      "1438 -55.115253 -54.340874 -53.995773 -54.535549 -56.042793 -53.741886   \n",
      "1439 -50.432739 -51.713955 -55.933186 -57.709980 -54.208328 -53.605801   \n",
      "\n",
      "              6  ...        206        207        208        209        210  \\\n",
      "240  -56.282532  ... -49.583542 -50.354843 -51.740112 -53.151653 -53.596394   \n",
      "241  -58.040920  ... -52.963810 -52.148178 -52.575092 -52.733963 -53.780277   \n",
      "242  -46.144737  ... -49.017906 -49.289055 -49.289055 -49.289055 -49.289055   \n",
      "243  -47.264126  ... -32.538971 -31.666105 -32.830318 -33.882835 -33.577568   \n",
      "244  -55.773678  ... -47.018372 -48.827934 -49.922104 -52.423866 -51.783352   \n",
      "...         ...  ...        ...        ...        ...        ...        ...   \n",
      "1435 -40.859455  ... -30.497267 -31.011204 -31.791348 -34.396542 -34.452412   \n",
      "1436 -49.240765  ... -49.726692 -50.108318 -50.828194 -51.525120 -50.562866   \n",
      "1437 -55.668983  ... -45.076759 -43.427559 -44.171726 -44.658108 -44.492882   \n",
      "1438 -51.131588  ... -54.914558 -53.693851 -54.099697 -53.585266 -53.993038   \n",
      "1439 -53.940762  ... -52.116783 -53.097797 -51.577583 -50.862648 -51.355446   \n",
      "\n",
      "            211        212        213        214        215  \n",
      "240  -53.313469 -53.512512 -55.573833 -56.696701 -56.354916  \n",
      "241  -54.433163 -57.540977 -57.138042 -55.262573 -54.643867  \n",
      "242  -49.289055 -49.289055 -49.289055 -49.057945 -48.911900  \n",
      "243  -32.613457 -31.279625 -30.565348 -19.475845 -12.500349  \n",
      "244  -51.101242 -49.003166 -50.278866 -53.998123 -57.619953  \n",
      "...         ...        ...        ...        ...        ...  \n",
      "1435 -34.094193 -35.161083 -35.943985 -37.234013 -38.118156  \n",
      "1436 -49.983562 -52.057095 -50.964302 -50.562965 -51.980812  \n",
      "1437 -47.181416 -43.806145 -41.568722 -44.150291 -48.124912  \n",
      "1438 -52.828178 -51.828327 -52.234890 -51.609539 -51.023682  \n",
      "1439 -53.824326 -55.785744 -55.169132 -55.552544 -57.285320  \n",
      "\n",
      "[1200 rows x 1083 columns]\n"
     ]
    }
   ],
   "source": [
    "def get_x_y(data):\n",
    "    rev_mapping = {emotion: num for num, emotion in mapping.items()}\n",
    "    x, y = [], []\n",
    "    for i, j in data.iterrows():\n",
    "        col = (label for label in j.items() if label[0] == 'emotion')\n",
    "        for item in col:\n",
    "            y.append(rev_mapping[item[1]])\n",
    "        count = 0\n",
    "        xs = []\n",
    "        for k in j.items():\n",
    "            if count > 5:\n",
    "                xs.append(k[1])\n",
    "            count += 1\n",
    "        x.append(xs)\n",
    "    return x, y\n",
    "\n",
    "# The numbered columns are mfccs\n",
    "print(train)\n",
    "train_x, train_y = get_x_y(train)\n",
    "test_x, test_y = get_x_y(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest frequency baseline: 0.13333333333333333\n",
      "Random baseline 0.08333333333333333\n",
      "Random uniform baseline 0.10833333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# train a dummy classifier to make predictions based on the most_frequent class value\n",
    "frequent_dummy_classifier = DummyClassifier(strategy=\"most_frequent\")\n",
    "frequent_dummy_classifier.fit(train_x, train_y)\n",
    "\n",
    "print(\"Highest frequency baseline:\", frequent_dummy_classifier.score(test_x, test_y))\n",
    "\n",
    "# train a dummy classifier to make predictions based on the class values\n",
    "stratified_dummy_classifier = DummyClassifier(strategy=\"stratified\")\n",
    "stratified_dummy_classifier.fit(train_x,train_y)\n",
    "\n",
    "print(\"Random baseline\", stratified_dummy_classifier.score(test_x, test_y))\n",
    "\n",
    "# train a dummy classifier to make predictions based on uniform selection\n",
    "uniform_dummy_classifier = DummyClassifier(strategy=\"uniform\")\n",
    "uniform_dummy_classifier.fit(train_x,train_y)\n",
    "\n",
    "print(\"Random uniform baseline\", uniform_dummy_classifier.score(test_x, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Data\n",
    "\n",
    "Visualize the data that we have extracted to better understand trends to expect and routes to go for changing parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'yLabel')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## TO DO\n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax1.set_title('Visualization Title')\n",
    "ax1.set_xlabel('xLabel')\n",
    "ax1.set_ylabel('yLabel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "If we want to transform the data in any way, we can do it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 1077, 1)\n",
      "[[-56.30023193]\n",
      " [-55.32692337]\n",
      " [-55.1198082 ]\n",
      " ...\n",
      " [-55.57383347]\n",
      " [-56.69670105]\n",
      " [-56.35491562]]\n",
      "\n",
      "(240, 1077, 1)\n",
      "[[-39.79786682]\n",
      " [-40.15006256]\n",
      " [-41.48541641]\n",
      " ...\n",
      " [-41.06079483]\n",
      " [-42.71106339]\n",
      " [-42.36263657]]\n"
     ]
    }
   ],
   "source": [
    "train_length = len(train_x[0])\n",
    "train_x = np.array(train_x).reshape(-1,length,1)\n",
    "print(np.shape(train_x))\n",
    "print(train_x[0])\n",
    "print()\n",
    "\n",
    "\n",
    "test_length = len(test_x[0])\n",
    "test_x = np.array(test_x).reshape(-1,length,1)\n",
    "print(np.shape(test_x))\n",
    "print(test_x[0])\n",
    "\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# le = LabelEncoder()\n",
    "\n",
    "\n",
    "# # \n",
    "# # Generates an int for each label\n",
    "# y=le.fit_transform(labels)\n",
    "\n",
    "# # Prints out each date with its int mapping\n",
    "# for c in list(le.classes_):\n",
    "#     print(le.transform([c])[0], c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 neutral\n",
      "[0 0 0 1 0 0 0 0] neutral\n",
      "2 neutral\n",
      "[0 0 1 0 0 0 0 0] neutral\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.utils import np_utils\n",
    "\n",
    "labels=[\"neutral\", \"calm\", \"happy\", \"sad\", \"angry\", \"fearful\", \"disgust\", \"surprised\"]\n",
    "\n",
    "train_y = [val-1 for val in train_y]\n",
    "train_y = [int(val) for val in train_y]\n",
    "print(train_y[0], labels[0])\n",
    "train_y=np_utils.to_categorical(train_y, num_classes=len(labels), dtype=np.int32)\n",
    "print(train_y[0], labels[0])\n",
    "\n",
    "test_y = [val-1 for val in test_y]\n",
    "print(test_y[0], labels[0])\n",
    "test_y=np_utils.to_categorical(test_y, num_classes=len(labels), dtype=np.int32)\n",
    "print(test_y[0], labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.int32'>\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Dropout, Flatten, Conv1D, Input, MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "\n",
    "print(type(train_y[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1077, 1)]         0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 1065, 8)           112       \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 355, 8)            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 355, 8)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 345, 16)           1424      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 115, 16)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 115, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 107, 32)           4640      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 35, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 35, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1120)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               143488    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 1032      \n",
      "=================================================================\n",
      "Total params: 150,696\n",
      "Trainable params: 150,696\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "\n",
    "inputs = Input(shape=(train_length,1))\n",
    "\n",
    "#First Conv1D layer\n",
    "conv = Conv1D(8,13, padding='valid', activation='relu', strides=1)(inputs)\n",
    "conv = MaxPooling1D(3)(conv)\n",
    "conv = Dropout(0.3)(conv)\n",
    "\n",
    "#Second Conv1D layer\n",
    "conv = Conv1D(16, 11, padding='valid', activation='relu', strides=1)(conv)\n",
    "conv = MaxPooling1D(3)(conv)\n",
    "conv = Dropout(0.3)(conv)\n",
    "\n",
    "#Third Conv1D layer\n",
    "conv = Conv1D(32, 9, padding='valid', activation='relu', strides=1)(conv)\n",
    "conv = MaxPooling1D(3)(conv)\n",
    "conv = Dropout(0.3)(conv)\n",
    "\n",
    "#Flatten layer\n",
    "conv = Flatten()(conv)\n",
    "\n",
    "#Dense Layer 1\n",
    "conv = Dense(128, activation='relu')(conv)\n",
    "conv = Dropout(0.3)(conv)\n",
    "\n",
    "# Output layer\n",
    "outputs = Dense(len(labels), activation='softmax')(conv)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, min_delta=0.0001) \n",
    "mc = ModelCheckpoint('best_model.hdf5', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "37/38 [============================>.] - ETA: 0s - loss: 1.8066 - accuracy: 0.2998\n",
      "Epoch 00001: val_accuracy did not improve from 0.07917\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 1.8027 - accuracy: 0.3008 - val_loss: 2.4074 - val_accuracy: 0.0500\n",
      "Epoch 2/10\n",
      "37/38 [============================>.] - ETA: 0s - loss: 1.8259 - accuracy: 0.2931\n",
      "Epoch 00002: val_accuracy did not improve from 0.07917\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 1.8246 - accuracy: 0.2925 - val_loss: 2.3100 - val_accuracy: 0.0500\n",
      "Epoch 3/10\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 1.7908 - accuracy: 0.3097\n",
      "Epoch 00003: val_accuracy did not improve from 0.07917\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 1.7915 - accuracy: 0.3125 - val_loss: 2.3815 - val_accuracy: 0.0458\n",
      "Epoch 4/10\n",
      "37/38 [============================>.] - ETA: 0s - loss: 1.7744 - accuracy: 0.3057\n",
      "Epoch 00004: val_accuracy did not improve from 0.07917\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 1.7726 - accuracy: 0.3075 - val_loss: 2.3649 - val_accuracy: 0.0333\n",
      "Epoch 5/10\n",
      "35/38 [==========================>...] - ETA: 0s - loss: 1.7678 - accuracy: 0.3170\n",
      "Epoch 00005: val_accuracy improved from 0.07917 to 0.08333, saving model to best_model.hdf5\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 1.7678 - accuracy: 0.3200 - val_loss: 2.5245 - val_accuracy: 0.0833\n",
      "Epoch 6/10\n",
      "37/38 [============================>.] - ETA: 0s - loss: 1.7298 - accuracy: 0.3336\n",
      "Epoch 00006: val_accuracy did not improve from 0.08333\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 1.7304 - accuracy: 0.3325 - val_loss: 2.3504 - val_accuracy: 0.0417\n",
      "Epoch 7/10\n",
      "37/38 [============================>.] - ETA: 0s - loss: 1.7377 - accuracy: 0.3446\n",
      "Epoch 00007: val_accuracy did not improve from 0.08333\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 1.7361 - accuracy: 0.3433 - val_loss: 2.3929 - val_accuracy: 0.0292\n",
      "Epoch 8/10\n",
      "37/38 [============================>.] - ETA: 0s - loss: 1.7176 - accuracy: 0.3497\n",
      "Epoch 00008: val_accuracy did not improve from 0.08333\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 1.7164 - accuracy: 0.3508 - val_loss: 2.4573 - val_accuracy: 0.0542\n",
      "Epoch 9/10\n",
      "36/38 [===========================>..] - ETA: 0s - loss: 1.6991 - accuracy: 0.3385\n",
      "Epoch 00009: val_accuracy did not improve from 0.08333\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 1.6959 - accuracy: 0.3425 - val_loss: 2.3697 - val_accuracy: 0.0292\n",
      "Epoch 10/10\n",
      "36/38 [===========================>..] - ETA: 0s - loss: 1.6582 - accuracy: 0.3594\n",
      "Epoch 00010: val_accuracy did not improve from 0.08333\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 1.6560 - accuracy: 0.3600 - val_loss: 2.5922 - val_accuracy: 0.0417\n",
      "MODEL TRAINING COMPLETE!\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(np.array(train_x), np.array(train_y), epochs=10, callbacks=[es,mc], batch_size=32, validation_data=(np.array(test_x),np.array(test_y)))\n",
    "print(\"MODEL TRAINING COMPLETE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Visualization\n",
    "\n",
    "Now we can take a look at how successful our model is and can easily find where overfitting takes place (if at all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(history.history['loss'], label='train') \n",
    "plt.plot(history.history['val_loss'], label='test') \n",
    "plt.legend() \n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='train') \n",
    "plt.plot(history.history['val_accuracy'], label='test') \n",
    "plt.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the application part (maybe a separate file), we can use our model to predict the future performance\n",
    "\n",
    "from keras.models import load_model\n",
    "model=load_model('best_model.hdf5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
