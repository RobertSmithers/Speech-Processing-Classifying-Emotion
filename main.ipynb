{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries\n",
    "\n",
    "For any issues running these modules, use python -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data, separate into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Building Train/Test Data ---\n",
      "Test data is 0.21% of the overall data\n",
      "Train has 1200 files.\n",
      "Test has 240 files.\n",
      "Split is 5:1 train:test\n"
     ]
    }
   ],
   "source": [
    "from fractions import Fraction\n",
    "import random\n",
    "\n",
    "path = \"./data/audio_speech_actors_01-24/\"\n",
    "actors = os.listdir(path)\n",
    "\n",
    "# We need to categorize the data files according to their emotion. Since the dataset is labelled by emotion (which is encoded into their filenames), we need to break that down\n",
    "# Filename identifiers:\n",
    "# Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
    "#\n",
    "# Vocal channel (01 = speech, 02 = song).\n",
    "#\n",
    "# Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
    "#\n",
    "# Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n",
    "#\n",
    "# Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n",
    "#\n",
    "# Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
    "#\n",
    "# Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n",
    "\n",
    "# We are only focusing on the emotion, so we categorize by the third number (01-08)\n",
    "# according to the dataset's site\n",
    "mapping = {1:\"neutral\", 2:\"calm\", 3:\"happy\", 4:\"sad\", 5:\"angry\", 6:\"fearful\", 7:\"disgust\", 8:\"surprised\"}\n",
    "\n",
    "def load_data(path, return_train_test=False, test_percentage=0.20):\n",
    "    # 1 slot for each of the emotions\n",
    "    emot = []\n",
    "    paths = []\n",
    "    train_test_labels = []\n",
    "    \n",
    "    # Custom making our train/test split\n",
    "    test_threshold = int(len(os.listdir(path)) * test_percentage + 1)    # (At least) 20% of data reserved for testing (important that we do this by actor to prevent data leakage)\n",
    "    print(\"Test data is {:0.2f}% of the overall data\".format(1 / (len(os.listdir(path))/test_threshold)))\n",
    "    \n",
    "    # Make a list of actors so we can shuffle order (last few actors will not always be test data each time we load data)\n",
    "    actors = []\n",
    "    for directory in os.listdir(path):\n",
    "        actors.append(directory)\n",
    "    random.shuffle(actors)\n",
    "    \n",
    "    count = 0\n",
    "    data_label = \"test\"\n",
    "    for directory in actors:\n",
    "        count += 1\n",
    "        if (count == test_threshold):\n",
    "            data_label = \"train\"\n",
    "        files = os.path.join(path, directory)\n",
    "        for file in os.listdir(files):\n",
    "            em_num = int(file.split(\"-\")[2])\n",
    "            emot.append(em_num)\n",
    "            train_test_labels.append(data_label)\n",
    "            paths.append(path + directory + \"/\" + file)\n",
    "    \n",
    "    tts = pd.DataFrame(train_test_labels, columns=[\"train_test\"])\n",
    "    ems = pd.DataFrame(emot, columns=['emotion']).replace(mapping)\n",
    "    pths = pd.DataFrame(paths, columns = [\"path\"])\n",
    "    data_file = pd.concat(\n",
    "        [\n",
    "            tts.reset_index(drop=True),\n",
    "            ems.reset_index(drop=True),\n",
    "            pths.reset_index(drop=True)\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    \n",
    "    if return_train_test:\n",
    "        return get_train_test(data_file)\n",
    "    return data_file\n",
    "\n",
    "def get_train_test(data):\n",
    "    grouped = data.groupby(data.train_test)\n",
    "    train = grouped.get_group(\"train\")\n",
    "    test = grouped.get_group(\"test\")\n",
    "    return train, test\n",
    "\n",
    "print(\"--- Building Train/Test Data ---\")\n",
    "data = load_data(path)\n",
    "train, test = get_train_test(data)\n",
    "train_size = train.train_test.value_counts().train\n",
    "test_size = test.train_test.value_counts().test\n",
    "print(\"Train has\", train_size, \"files.\")\n",
    "print(\"Test has\", test_size, \"files.\")\n",
    "\n",
    "ratio = Fraction(train_size, test_size)\n",
    "print(\"Split is\", str(ratio.numerator)+\":\"+str(ratio.denominator), \"train:test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# for m in mapping:\n",
    "#     for i in train[m]:\n",
    "#         X.append(i)\n",
    "#         y.append(m)\n",
    "\n",
    "def gen_mfccs(data, NUM_MFCCs=13):\n",
    "    mfccs = pd.DataFrame(columns=['mfccs'])\n",
    "    \n",
    "    # Get mfccs from each audio file\n",
    "    count=0\n",
    "    for i, j in data.iterrows():\n",
    "        for item in j.items():\n",
    "            if item[0] == 'path':\n",
    "                # Sample rate and duration taken from the kaggle dataset description\n",
    "                file, sample_rate = librosa.load(item[1], res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)\n",
    "                sample_rate = np.array(sample_rate)\n",
    "                mfcc = np.mean(librosa.feature.mfcc(y=file, sr=sample_rate, n_mfcc=NUM_MFCCs), axis=0)\n",
    "                mfccs.loc[count] = [mfcc]\n",
    "                count+=1\n",
    "                break\n",
    "    \n",
    "    # Gen list of mfccs as a dataframe to **manually** concatenate onto data\n",
    "#     mfccs = pd.DataFrame(mfccs, columns = [(\"mfcc_\" + str(num)) for num in range(len(mfccs[0]))])\n",
    "#     data = pd.concat(\n",
    "#         [\n",
    "#             data.reset_index(drop=True),\n",
    "#             mfccs.reset_index(drop=True)\n",
    "#         ],\n",
    "#         axis=1\n",
    "#     )\n",
    "\n",
    "    # Add on these mfccs to the data DataFrame\n",
    "    return pd.concat([data.reset_index(drop=True), pd.DataFrame(mfccs[\"mfccs\"].values.tolist())], axis=1)\n",
    "        \n",
    "data = gen_mfccs(data)\n",
    "data = data.fillna(0)\n",
    "train, test = get_train_test(data)\n",
    "\n",
    "# Save train + test DataFrame file as a csv\n",
    "data.to_csv(\"extracted_data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     train_test  emotion                                               path  \\\n",
      "240       train  fearful  ./data/audio_speech_actors_01-24/Actor_02/03-0...   \n",
      "241       train    angry  ./data/audio_speech_actors_01-24/Actor_02/03-0...   \n",
      "242       train    angry  ./data/audio_speech_actors_01-24/Actor_02/03-0...   \n",
      "243       train  fearful  ./data/audio_speech_actors_01-24/Actor_02/03-0...   \n",
      "244       train      sad  ./data/audio_speech_actors_01-24/Actor_02/03-0...   \n",
      "...         ...      ...                                                ...   \n",
      "1435      train  neutral  ./data/audio_speech_actors_01-24/Actor_22/03-0...   \n",
      "1436      train     calm  ./data/audio_speech_actors_01-24/Actor_22/03-0...   \n",
      "1437      train     calm  ./data/audio_speech_actors_01-24/Actor_22/03-0...   \n",
      "1438      train    happy  ./data/audio_speech_actors_01-24/Actor_22/03-0...   \n",
      "1439      train    happy  ./data/audio_speech_actors_01-24/Actor_22/03-0...   \n",
      "\n",
      "              0          1          2          3          4          5  \\\n",
      "240  -55.349541 -55.585308 -56.931351 -56.681725 -56.784481 -55.854031   \n",
      "241  -58.940220 -58.940220 -58.940220 -58.940220 -58.940220 -58.940220   \n",
      "242  -46.882404 -46.882404 -46.882404 -46.882404 -46.882404 -46.882404   \n",
      "243  -50.250221 -50.250221 -50.250221 -50.255898 -50.314941 -50.416325   \n",
      "244  -64.798790 -64.798790 -64.798790 -64.798790 -64.798790 -64.798790   \n",
      "...         ...        ...        ...        ...        ...        ...   \n",
      "1435 -62.084328 -60.069756 -58.299805 -54.431618 -54.665321 -54.536171   \n",
      "1436 -52.029785 -49.040154 -49.723259 -53.390762 -56.820419 -60.676460   \n",
      "1437 -60.600342 -60.240757 -56.190895 -51.478291 -46.882469 -45.834343   \n",
      "1438 -63.673378 -63.133614 -60.915764 -56.386559 -54.564926 -52.669125   \n",
      "1439 -53.450066 -52.781845 -53.893433 -55.097374 -53.156521 -52.881020   \n",
      "\n",
      "              6  ...        206        207        208        209        210  \\\n",
      "240  -55.723221  ... -58.246853 -57.985378 -58.166031 -58.036156 -55.960423   \n",
      "241  -58.940220  ... -48.235161 -46.350334 -47.066261 -48.931396 -47.600639   \n",
      "242  -46.882404  ... -33.641357 -34.990402 -34.930424 -37.702354 -38.504726   \n",
      "243  -50.497036  ... -47.858189 -47.587994 -47.472061 -48.161884 -48.074448   \n",
      "244  -64.798790  ... -49.973179 -49.565804 -51.082909 -50.710281 -52.202518   \n",
      "...         ...  ...        ...        ...        ...        ...        ...   \n",
      "1435 -55.455105  ... -61.541897 -59.857841 -56.082497 -54.257488 -55.692223   \n",
      "1436 -62.403404  ... -57.032387 -57.066833 -56.801228 -59.784161 -61.799107   \n",
      "1437 -49.034832  ... -47.162910 -48.214237 -46.509407 -47.161057 -48.834919   \n",
      "1438 -55.094971  ... -58.636715 -59.604427 -62.149086 -61.382118 -64.251472   \n",
      "1439 -52.550095  ... -31.758280 -32.009052 -33.739487 -35.712341 -36.766190   \n",
      "\n",
      "            211        212        213        214        215  \n",
      "240  -56.344688 -57.951897 -54.570671 -51.357929 -52.141319  \n",
      "241  -49.094292 -50.973202 -51.739258 -49.920254 -49.148476  \n",
      "242  -40.004250 -40.235683 -40.575851 -42.321167 -43.132748  \n",
      "243  -47.586262 -47.317574 -47.287376 -46.979465 -47.003113  \n",
      "244  -50.220310 -49.293602 -52.702557 -56.099472 -59.495033  \n",
      "...         ...        ...        ...        ...        ...  \n",
      "1435 -61.619381 -61.093925 -59.269871 -60.922424 -63.810814  \n",
      "1436 -61.976730 -62.287128 -61.755066 -62.131077 -66.521088  \n",
      "1437 -50.404514 -53.548889 -57.053982 -56.385750 -56.411106  \n",
      "1438 -64.135078 -61.190720 -58.856174 -57.789772 -59.030785  \n",
      "1439 -36.789177 -37.869125 -39.358334 -35.731247 -32.711559  \n",
      "\n",
      "[1200 rows x 219 columns]\n"
     ]
    }
   ],
   "source": [
    "def get_x_y(data):\n",
    "    rev_mapping = {emotion: num for num, emotion in mapping.items()}\n",
    "    x, y = [], []\n",
    "    for i, j in data.iterrows():\n",
    "        col = (label for label in j.items() if label[0] == 'emotion')\n",
    "        for item in col:\n",
    "            y.append(rev_mapping[item[1]])\n",
    "        count = 0\n",
    "        xs = []\n",
    "        for k in j.items():\n",
    "            if count > 5:\n",
    "                xs.append(k[1])\n",
    "            count += 1\n",
    "        x.append(xs)\n",
    "    return x, y\n",
    "\n",
    "# The numbered columns are mfccs\n",
    "print(train)\n",
    "train_x, train_y = get_x_y(train)\n",
    "test_x, test_y = get_x_y(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest frequency baseline: 0.13333333333333333\n",
      "Random baseline 0.12083333333333333\n",
      "Random uniform baseline 0.17083333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# train a dummy classifier to make predictions based on the most_frequent class value\n",
    "frequent_dummy_classifier = DummyClassifier(strategy=\"most_frequent\")\n",
    "frequent_dummy_classifier.fit(train_x, train_y)\n",
    "\n",
    "print(\"Highest frequency baseline:\", frequent_dummy_classifier.score(test_x, test_y))\n",
    "\n",
    "# train a dummy classifier to make predictions based on the class values\n",
    "stratified_dummy_classifier = DummyClassifier(strategy=\"stratified\")\n",
    "stratified_dummy_classifier.fit(train_x,train_y)\n",
    "\n",
    "print(\"Random baseline\", stratified_dummy_classifier.score(test_x, test_y))\n",
    "\n",
    "# train a dummy classifier to make predictions based on uniform selection\n",
    "uniform_dummy_classifier = DummyClassifier(strategy=\"uniform\")\n",
    "uniform_dummy_classifier.fit(train_x,train_y)\n",
    "\n",
    "print(\"Random uniform baseline\", uniform_dummy_classifier.score(test_x, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Data\n",
    "\n",
    "Visualize the data that we have extracted to better understand trends to expect and routes to go for changing parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'yLabel')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## TO DO\n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax1.set_title('Visualization Title')\n",
    "ax1.set_xlabel('xLabel')\n",
    "ax1.set_ylabel('yLabel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "If we want to transform the data in any way, we can do it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 213, 1)\n",
      "[[-56.68172455]\n",
      " [-56.78448105]\n",
      " [-55.85403061]\n",
      " [-55.72322083]\n",
      " [-56.15439224]\n",
      " [-55.49872208]\n",
      " [-53.93239594]\n",
      " [-54.44218445]\n",
      " [-54.94053268]\n",
      " [-55.65406799]\n",
      " [-55.58727646]\n",
      " [-55.43637848]\n",
      " [-54.70941925]\n",
      " [-54.01356506]\n",
      " [-54.58927155]\n",
      " [-54.90510941]\n",
      " [-55.49765778]\n",
      " [-56.95895767]\n",
      " [-58.11436081]\n",
      " [-58.26810074]\n",
      " [-58.26529312]\n",
      " [-58.26529312]\n",
      " [-58.26529312]\n",
      " [-58.26529312]\n",
      " [-58.26529312]\n",
      " [-58.26529312]\n",
      " [-58.26529312]\n",
      " [-58.26529312]\n",
      " [-58.26529312]\n",
      " [-58.26529312]\n",
      " [-58.26529312]\n",
      " [-58.26529312]\n",
      " [-58.26529312]\n",
      " [-58.26529312]\n",
      " [-58.26529312]\n",
      " [-58.26529312]\n",
      " [-58.26529312]\n",
      " [-56.07157898]\n",
      " [-53.89182281]\n",
      " [-54.97628021]\n",
      " [-54.32460785]\n",
      " [-48.91162491]\n",
      " [-38.06649017]\n",
      " [-36.1774025 ]\n",
      " [-38.18199921]\n",
      " [-39.871521  ]\n",
      " [-40.43122864]\n",
      " [-39.43253708]\n",
      " [-39.72987747]\n",
      " [-41.036129  ]\n",
      " [-41.92995453]\n",
      " [-41.69484711]\n",
      " [-40.30969238]\n",
      " [-39.76254654]\n",
      " [-42.42000961]\n",
      " [-40.48332214]\n",
      " [-40.49825287]\n",
      " [-41.55001068]\n",
      " [-42.27687454]\n",
      " [-39.40753174]\n",
      " [-40.98705673]\n",
      " [-44.18576431]\n",
      " [-42.61742783]\n",
      " [-41.69299698]\n",
      " [-39.3181572 ]\n",
      " [-38.55437851]\n",
      " [-40.08078384]\n",
      " [-42.52689743]\n",
      " [-41.98071289]\n",
      " [-41.32772446]\n",
      " [-38.39113998]\n",
      " [-38.26531219]\n",
      " [-39.99304962]\n",
      " [-38.96813965]\n",
      " [-39.06599426]\n",
      " [-38.8911705 ]\n",
      " [-38.99573898]\n",
      " [-39.95763397]\n",
      " [-39.94574356]\n",
      " [-37.34635162]\n",
      " [-35.26343918]\n",
      " [-33.92475128]\n",
      " [-33.23410416]\n",
      " [-37.26901245]\n",
      " [-37.30013275]\n",
      " [-38.77664566]\n",
      " [-43.24978256]\n",
      " [-42.70843124]\n",
      " [-42.43146133]\n",
      " [-46.39754105]\n",
      " [-44.47304916]\n",
      " [-40.65948868]\n",
      " [-41.14709854]\n",
      " [-40.43109894]\n",
      " [-38.85546112]\n",
      " [-41.07837296]\n",
      " [-43.85235214]\n",
      " [-46.14085388]\n",
      " [-41.69374847]\n",
      " [-38.2146225 ]\n",
      " [-39.56886673]\n",
      " [-42.60936737]\n",
      " [-44.22920227]\n",
      " [-43.03986359]\n",
      " [-39.81312561]\n",
      " [-41.15753174]\n",
      " [-43.45301056]\n",
      " [-45.05825043]\n",
      " [-44.69160461]\n",
      " [-44.31181335]\n",
      " [-44.70373535]\n",
      " [-44.66752625]\n",
      " [-44.92427063]\n",
      " [-44.76505661]\n",
      " [-43.57550049]\n",
      " [-42.65122986]\n",
      " [-42.54256058]\n",
      " [-34.98577499]\n",
      " [-31.13724518]\n",
      " [-34.48700714]\n",
      " [-37.62778854]\n",
      " [-39.87431335]\n",
      " [-41.24261093]\n",
      " [-41.23531723]\n",
      " [-40.15549469]\n",
      " [-39.70396042]\n",
      " [-40.75291061]\n",
      " [-39.80122375]\n",
      " [-39.34442139]\n",
      " [-38.65707016]\n",
      " [-37.37053299]\n",
      " [-41.60089111]\n",
      " [-40.18787003]\n",
      " [-40.82432175]\n",
      " [-41.77799988]\n",
      " [-39.85761261]\n",
      " [-39.01297379]\n",
      " [-39.80121613]\n",
      " [-40.36965561]\n",
      " [-36.74077606]\n",
      " [-37.47969055]\n",
      " [-41.18682861]\n",
      " [-43.14984512]\n",
      " [-41.99821472]\n",
      " [-43.14196014]\n",
      " [-39.59620667]\n",
      " [-34.76986313]\n",
      " [-33.72585678]\n",
      " [-37.96538925]\n",
      " [-36.71578217]\n",
      " [-36.28126526]\n",
      " [-40.04951477]\n",
      " [-42.72651672]\n",
      " [-44.12403107]\n",
      " [-44.3962059 ]\n",
      " [-46.46526337]\n",
      " [-48.99920654]\n",
      " [-46.8645401 ]\n",
      " [-46.43560028]\n",
      " [-47.53002548]\n",
      " [-46.53978729]\n",
      " [-44.39523697]\n",
      " [-45.26758194]\n",
      " [-44.56618881]\n",
      " [-45.09670639]\n",
      " [-45.55331802]\n",
      " [-44.88056183]\n",
      " [-42.11710739]\n",
      " [-43.79996109]\n",
      " [-44.81925583]\n",
      " [-43.17273712]\n",
      " [-42.01305389]\n",
      " [-45.32010269]\n",
      " [-45.57788849]\n",
      " [-42.74889755]\n",
      " [-42.77533722]\n",
      " [-43.39730453]\n",
      " [-44.29886627]\n",
      " [-42.20369339]\n",
      " [-42.01981735]\n",
      " [-42.26450729]\n",
      " [-42.22782516]\n",
      " [-40.72154236]\n",
      " [-40.43001175]\n",
      " [-39.8919754 ]\n",
      " [-40.83412933]\n",
      " [-40.87749863]\n",
      " [-41.31920624]\n",
      " [-41.77597809]\n",
      " [-43.76420593]\n",
      " [-44.81768036]\n",
      " [-46.58327866]\n",
      " [-46.54956055]\n",
      " [-46.24716568]\n",
      " [-47.55142593]\n",
      " [-49.46786118]\n",
      " [-50.43151093]\n",
      " [-50.53350449]\n",
      " [-53.30804062]\n",
      " [-55.10586166]\n",
      " [-51.33935547]\n",
      " [-53.18019104]\n",
      " [-58.33462143]\n",
      " [-58.24685287]\n",
      " [-57.98537827]\n",
      " [-58.16603088]\n",
      " [-58.0361557 ]\n",
      " [-55.96042252]\n",
      " [-56.34468842]\n",
      " [-57.95189667]\n",
      " [-54.57067108]\n",
      " [-51.35792923]\n",
      " [-52.14131927]]\n",
      "\n",
      "(240, 213, 1)\n",
      "[[-55.97121048]\n",
      " [-60.46885681]\n",
      " [-62.65668488]\n",
      " [-61.16388321]\n",
      " [-58.514328  ]\n",
      " [-56.89417267]\n",
      " [-56.48431396]\n",
      " [-57.92702484]\n",
      " [-61.84336472]\n",
      " [-59.50098038]\n",
      " [-56.62215042]\n",
      " [-51.70220184]\n",
      " [-49.76417923]\n",
      " [-48.80521011]\n",
      " [-49.96627045]\n",
      " [-50.43067169]\n",
      " [-48.6095314 ]\n",
      " [-47.24300003]\n",
      " [-47.78695297]\n",
      " [-48.47755432]\n",
      " [-51.66358185]\n",
      " [-51.85896301]\n",
      " [-51.43377304]\n",
      " [-51.18659973]\n",
      " [-51.80295563]\n",
      " [-49.88924026]\n",
      " [-48.30839157]\n",
      " [-50.67176056]\n",
      " [-53.18868637]\n",
      " [-53.70283508]\n",
      " [-54.26763535]\n",
      " [-56.00315475]\n",
      " [-54.56432724]\n",
      " [-53.78698349]\n",
      " [-51.37833405]\n",
      " [-48.34890366]\n",
      " [-45.85146713]\n",
      " [-42.06009293]\n",
      " [-38.00981522]\n",
      " [-29.93387985]\n",
      " [-24.84127998]\n",
      " [-23.28499031]\n",
      " [-23.91981888]\n",
      " [-23.35139656]\n",
      " [-22.36223602]\n",
      " [-22.11359406]\n",
      " [-22.4719677 ]\n",
      " [-22.54670143]\n",
      " [-23.09140968]\n",
      " [-24.44665146]\n",
      " [-25.56808853]\n",
      " [-27.71555519]\n",
      " [-29.37866974]\n",
      " [-30.00206757]\n",
      " [-28.08336449]\n",
      " [-27.27440262]\n",
      " [-29.99064827]\n",
      " [-29.32711983]\n",
      " [-27.26302147]\n",
      " [-25.90906143]\n",
      " [-25.26727676]\n",
      " [-26.88459206]\n",
      " [-27.09306908]\n",
      " [-27.38261604]\n",
      " [-26.81350708]\n",
      " [-25.3572216 ]\n",
      " [-25.22834396]\n",
      " [-26.53515244]\n",
      " [-29.46054077]\n",
      " [-30.97094345]\n",
      " [-30.2984314 ]\n",
      " [-29.07298851]\n",
      " [-29.93460274]\n",
      " [-31.1118679 ]\n",
      " [-30.879879  ]\n",
      " [-30.63425064]\n",
      " [-25.19258499]\n",
      " [-24.04638863]\n",
      " [-25.85284424]\n",
      " [-26.37075996]\n",
      " [-26.16688919]\n",
      " [-25.95217514]\n",
      " [-25.66161919]\n",
      " [-25.09618187]\n",
      " [-25.0217762 ]\n",
      " [-25.5992527 ]\n",
      " [-24.57825851]\n",
      " [-24.36079597]\n",
      " [-24.84650993]\n",
      " [-25.31825829]\n",
      " [-25.78111458]\n",
      " [-25.40036964]\n",
      " [-24.94909668]\n",
      " [-25.83819199]\n",
      " [-25.0915432 ]\n",
      " [-25.14860725]\n",
      " [-25.10264778]\n",
      " [-25.66749191]\n",
      " [-27.09224129]\n",
      " [-27.36474419]\n",
      " [-26.1758728 ]\n",
      " [-24.59297371]\n",
      " [-22.23234177]\n",
      " [-21.07043457]\n",
      " [-20.81153679]\n",
      " [-20.83697891]\n",
      " [-22.2656765 ]\n",
      " [-22.64437485]\n",
      " [-23.74133492]\n",
      " [-25.16036034]\n",
      " [-25.45825577]\n",
      " [-24.7141304 ]\n",
      " [-25.01635933]\n",
      " [-25.15781593]\n",
      " [-25.0962944 ]\n",
      " [-24.61009789]\n",
      " [-25.05868721]\n",
      " [-28.01211929]\n",
      " [-28.45497322]\n",
      " [-29.85014534]\n",
      " [-31.88729668]\n",
      " [-31.58086014]\n",
      " [-28.08112526]\n",
      " [-24.63383293]\n",
      " [-24.191679  ]\n",
      " [-25.84309959]\n",
      " [-28.2322216 ]\n",
      " [-27.25170708]\n",
      " [-26.16722298]\n",
      " [-25.44343948]\n",
      " [-26.01949883]\n",
      " [-26.9530201 ]\n",
      " [-28.05810738]\n",
      " [-27.35029793]\n",
      " [-27.0046463 ]\n",
      " [-27.84576797]\n",
      " [-26.77794456]\n",
      " [-26.30013657]\n",
      " [-25.05628586]\n",
      " [-25.32529449]\n",
      " [-25.08790779]\n",
      " [-24.92905998]\n",
      " [-24.60385132]\n",
      " [-24.71058083]\n",
      " [-25.85011482]\n",
      " [-26.92076492]\n",
      " [-27.53890038]\n",
      " [-28.05937004]\n",
      " [-29.49814987]\n",
      " [-33.07371521]\n",
      " [-34.58572769]\n",
      " [-34.7444191 ]\n",
      " [-36.92273712]\n",
      " [-37.84653091]\n",
      " [-39.34568787]\n",
      " [-40.13267517]\n",
      " [-41.74264145]\n",
      " [-42.73551178]\n",
      " [-45.03386688]\n",
      " [-45.70555115]\n",
      " [-46.41290283]\n",
      " [-44.45269012]\n",
      " [-42.73999786]\n",
      " [-44.664608  ]\n",
      " [-45.70064926]\n",
      " [-45.51395798]\n",
      " [-44.67724228]\n",
      " [-43.32919693]\n",
      " [-44.31205368]\n",
      " [-47.43514633]\n",
      " [-49.20351791]\n",
      " [-48.53839493]\n",
      " [-47.7577095 ]\n",
      " [-48.63190079]\n",
      " [-47.77526474]\n",
      " [-47.69040298]\n",
      " [-50.83010483]\n",
      " [-51.74717712]\n",
      " [-51.14877701]\n",
      " [-49.00482941]\n",
      " [-49.63993073]\n",
      " [-49.6197319 ]\n",
      " [-50.21946335]\n",
      " [-50.02551651]\n",
      " [-50.76831818]\n",
      " [-51.48252106]\n",
      " [-53.90925598]\n",
      " [-54.96459579]\n",
      " [-53.13075638]\n",
      " [-54.89826202]\n",
      " [-50.7877121 ]\n",
      " [-51.17639923]\n",
      " [-52.38637161]\n",
      " [-54.0450058 ]\n",
      " [-52.09276962]\n",
      " [-50.57141495]\n",
      " [-49.82284927]\n",
      " [-50.18394089]\n",
      " [-51.22629166]\n",
      " [-50.82517242]\n",
      " [-50.77563095]\n",
      " [-50.52364731]\n",
      " [-52.22670364]\n",
      " [-51.60630417]\n",
      " [-52.54749298]\n",
      " [-54.87712097]\n",
      " [-55.2011261 ]\n",
      " [-55.59236908]\n",
      " [-55.28601837]\n",
      " [-55.1697998 ]\n",
      " [-54.78282928]\n",
      " [-58.87726212]\n",
      " [-57.61657715]]\n"
     ]
    }
   ],
   "source": [
    "train_length = len(train_x[0])\n",
    "train_x = np.array(train_x).reshape(-1,train_length,1)\n",
    "print(np.shape(train_x))\n",
    "print(train_x[0])\n",
    "print()\n",
    "\n",
    "\n",
    "test_length = len(test_x[0])\n",
    "test_x = np.array(test_x).reshape(-1,test_length,1)\n",
    "print(np.shape(test_x))\n",
    "print(test_x[0])\n",
    "\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# le = LabelEncoder()\n",
    "\n",
    "\n",
    "# # \n",
    "# # Generates an int for each label\n",
    "# y=le.fit_transform(labels)\n",
    "\n",
    "# # Prints out each date with its int mapping\n",
    "# for c in list(le.classes_):\n",
    "#     print(le.transform([c])[0], c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 neutral\n",
      "[0 0 0 0 0 1 0 0] neutral\n",
      "0 neutral\n",
      "[1 0 0 0 0 0 0 0] neutral\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.utils import np_utils\n",
    "\n",
    "labels=[\"neutral\", \"calm\", \"happy\", \"sad\", \"angry\", \"fearful\", \"disgust\", \"surprised\"]\n",
    "\n",
    "train_y = [val-1 for val in train_y]\n",
    "train_y = [int(val) for val in train_y]\n",
    "print(train_y[0], labels[0])\n",
    "train_y=np_utils.to_categorical(train_y, num_classes=len(labels), dtype=np.int32)\n",
    "print(train_y[0], labels[0])\n",
    "\n",
    "test_y = [val-1 for val in test_y]\n",
    "print(test_y[0], labels[0])\n",
    "test_y=np_utils.to_categorical(test_y, num_classes=len(labels), dtype=np.int32)\n",
    "print(test_y[0], labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.int32'>\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Dropout, Flatten, Conv1D, Input, MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "\n",
    "print(type(train_y[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 213, 1)]          0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 201, 8)            112       \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 67, 8)             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 67, 8)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 57, 16)            1424      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 19, 16)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 19, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 11, 32)            4640      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 3, 32)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 3, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               12416     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 1032      \n",
      "=================================================================\n",
      "Total params: 19,624\n",
      "Trainable params: 19,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "\n",
    "inputs = Input(shape=(train_length,1))\n",
    "\n",
    "#First Conv1D layer\n",
    "conv = Conv1D(8,13, padding='valid', activation='relu', strides=1)(inputs)\n",
    "conv = MaxPooling1D(3)(conv)\n",
    "conv = Dropout(0.3)(conv)\n",
    "\n",
    "#Second Conv1D layer\n",
    "conv = Conv1D(16, 11, padding='valid', activation='relu', strides=1)(conv)\n",
    "conv = MaxPooling1D(3)(conv)\n",
    "conv = Dropout(0.3)(conv)\n",
    "\n",
    "#Third Conv1D layer\n",
    "conv = Conv1D(32, 9, padding='valid', activation='relu', strides=1)(conv)\n",
    "conv = MaxPooling1D(3)(conv)\n",
    "conv = Dropout(0.3)(conv)\n",
    "\n",
    "#Flatten layer\n",
    "conv = Flatten()(conv)\n",
    "\n",
    "#Dense Layer 1\n",
    "conv = Dense(128, activation='relu')(conv)\n",
    "conv = Dropout(0.3)(conv)\n",
    "\n",
    "# Output layer\n",
    "outputs = Dense(len(labels), activation='softmax')(conv)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, min_delta=0.0001) \n",
    "mc = ModelCheckpoint('best_model.hdf5', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "36/38 [===========================>..] - ETA: 0s - loss: 3.2230 - accuracy: 0.1233\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.16250, saving model to best_model.hdf5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 3.1790 - accuracy: 0.1233 - val_loss: 2.0705 - val_accuracy: 0.1625\n",
      "Epoch 2/10\n",
      "35/38 [==========================>...] - ETA: 0s - loss: 2.1002 - accuracy: 0.1509\n",
      "Epoch 00002: val_accuracy did not improve from 0.16250\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.0988 - accuracy: 0.1525 - val_loss: 2.0648 - val_accuracy: 0.1458\n",
      "Epoch 3/10\n",
      "36/38 [===========================>..] - ETA: 0s - loss: 2.0749 - accuracy: 0.1345\n",
      "Epoch 00003: val_accuracy improved from 0.16250 to 0.22917, saving model to best_model.hdf5\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.0758 - accuracy: 0.1342 - val_loss: 2.0570 - val_accuracy: 0.2292\n",
      "Epoch 4/10\n",
      "36/38 [===========================>..] - ETA: 0s - loss: 2.0818 - accuracy: 0.1415\n",
      "Epoch 00004: val_accuracy did not improve from 0.22917\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.0814 - accuracy: 0.1417 - val_loss: 2.0617 - val_accuracy: 0.1333\n",
      "Epoch 5/10\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 2.0722 - accuracy: 0.1154\n",
      "Epoch 00005: val_accuracy did not improve from 0.22917\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.0695 - accuracy: 0.1258 - val_loss: 2.0529 - val_accuracy: 0.2083\n",
      "Epoch 6/10\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 2.0698 - accuracy: 0.1466\n",
      "Epoch 00006: val_accuracy did not improve from 0.22917\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.0653 - accuracy: 0.1500 - val_loss: 2.0549 - val_accuracy: 0.1583\n",
      "Epoch 7/10\n",
      "37/38 [============================>.] - ETA: 0s - loss: 2.0478 - accuracy: 0.1554\n",
      "Epoch 00007: val_accuracy did not improve from 0.22917\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.0470 - accuracy: 0.1550 - val_loss: 2.0352 - val_accuracy: 0.2292\n",
      "Epoch 8/10\n",
      "36/38 [===========================>..] - ETA: 0s - loss: 2.0380 - accuracy: 0.1615\n",
      "Epoch 00008: val_accuracy improved from 0.22917 to 0.25417, saving model to best_model.hdf5\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 2.0382 - accuracy: 0.1650 - val_loss: 2.0329 - val_accuracy: 0.2542\n",
      "Epoch 9/10\n",
      "26/38 [===================>..........] - ETA: 0s - loss: 2.0455 - accuracy: 0.1923\n",
      "Epoch 00009: val_accuracy did not improve from 0.25417\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.0475 - accuracy: 0.1842 - val_loss: 2.0478 - val_accuracy: 0.1708\n",
      "Epoch 10/10\n",
      "38/38 [==============================] - ETA: 0s - loss: 2.0272 - accuracy: 0.1883\n",
      "Epoch 00010: val_accuracy did not improve from 0.25417\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.0272 - accuracy: 0.1883 - val_loss: 1.9977 - val_accuracy: 0.2208\n",
      "MODEL TRAINING COMPLETE!\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(np.array(train_x), np.array(train_y), epochs=10, callbacks=[es,mc], batch_size=32, validation_data=(np.array(test_x),np.array(test_y)))\n",
    "print(\"MODEL TRAINING COMPLETE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Visualization\n",
    "\n",
    "Now we can take a look at how successful our model is and can easily find where overfitting takes place (if at all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(history.history['loss'], label='train') \n",
    "plt.plot(history.history['val_loss'], label='test') \n",
    "plt.legend() \n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='train') \n",
    "plt.plot(history.history['val_accuracy'], label='test') \n",
    "plt.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the application part (maybe a separate file), we can use our model to predict the future performance\n",
    "\n",
    "from keras.models import load_model\n",
    "model=load_model('best_model.hdf5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
