{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries\n",
    "\n",
    "For any issues running these modules, use python -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data, separate into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Building Train/Test Data ---\n",
      "Test data is 0.21% of the overall data\n",
      "Train has 240 files.\n",
      "Test has 1200 files.\n",
      "Split is 1:5 train:test\n"
     ]
    }
   ],
   "source": [
    "from fractions import Fraction\n",
    "import random\n",
    "\n",
    "path = \"./data/audio_speech_actors_01-24/\"\n",
    "actors = os.listdir(path)\n",
    "\n",
    "# We need to categorize the data files according to their emotion. Since the dataset is labelled by emotion (which is encoded into their filenames), we need to break that down\n",
    "# Filename identifiers:\n",
    "# Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
    "#\n",
    "# Vocal channel (01 = speech, 02 = song).\n",
    "#\n",
    "# Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
    "#\n",
    "# Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n",
    "#\n",
    "# Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n",
    "#\n",
    "# Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
    "#\n",
    "# Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n",
    "\n",
    "# We are only focusing on the emotion, so we categorize by the third number (01-08)\n",
    "# according to the dataset's site\n",
    "\n",
    "def load_data(path, test_percentage=0.20):\n",
    "    # 1 slot for each of the emotions\n",
    "    mapping = [\"neutral\", \"calm\", \"happy\", \"sad\", \"angry\", \"fearful\", \"disgust\", \"surprised\"]\n",
    "    \n",
    "    train = {\"neutral\":[], \"calm\":[], \"happy\":[], \"sad\":[], \"angry\":[], \"fearful\":[], \"disgust\":[], \"surprised\":[]}\n",
    "    test = {\"neutral\":[], \"calm\":[], \"happy\":[], \"sad\":[], \"angry\":[], \"fearful\":[], \"disgust\":[], \"surprised\":[]}\n",
    "    \n",
    "    # Custom making our train/test split\n",
    "    test_threshold = int(len(os.listdir(path)) * test_percentage + 1)    # (At least) 20% of data reserved for testing (important that we do this by actor to prevent data leakage)\n",
    "    print(\"Test data is {:0.2f}% of the overall data\".format(1 / (len(os.listdir(path))/test_threshold)))\n",
    "    \n",
    "    # Make a list of actors so we can shuffle order (last few actors will not always be test data each time we load data)\n",
    "    actors = []\n",
    "    for directory in os.listdir(path):\n",
    "        actors.append(directory)\n",
    "    random.shuffle(actors)\n",
    "    \n",
    "    count = 0\n",
    "    current_dict = train\n",
    "    for directory in actors:\n",
    "        count += 1\n",
    "        if (count == test_threshold):\n",
    "            current_dict = test\n",
    "        files = os.path.join(path, directory)\n",
    "        for file in os.listdir(files):\n",
    "            em_num = int(file.split(\"-\")[2])\n",
    "            current_dict[mapping[em_num-1]].append(file)\n",
    "    return train, test\n",
    "\n",
    "def get_file_count(data):\n",
    "    count = 0\n",
    "    for emotion in data:\n",
    "        for _ in data[emotion]:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "print(\"--- Building Train/Test Data ---\")\n",
    "train, test = load_data(path)\n",
    "train_size = get_file_count(train)\n",
    "test_size = get_file_count(test)\n",
    "print(\"Train has\", train_size, \"files.\")\n",
    "print(\"Test has\", test_size, \"files.\")\n",
    "\n",
    "ratio = Fraction(train_size, test_size)\n",
    "print(\"Split is\", str(ratio.numerator)+\":\"+str(ratio.denominator), \"train:test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Explore dataset\n",
    "hist.head()\n",
    "hist.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Data\n",
    "\n",
    "Visualize the data that we have extracted to better understand trends to expect and routes to go for changing parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO\n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax1.set_title('Visualization Title')\n",
    "ax1.set_xlabel('xLabel')\n",
    "ax1.set_ylabel('yLabel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "If we want to transform the data in any way, we can do it here.\n",
    "(Maybe we can transform the date (month/day of the year) to a one-hot array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "\n",
    "labels=[\"01/20\", \"01/21\", \"01/22\"]\n",
    "# Generates an int for each label\n",
    "y=le.fit_transform(labels)\n",
    "\n",
    "# Prints out each date with its int mapping\n",
    "for c in list(le.classes_):\n",
    "    print(le.transform([c])[0], c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reshape data here as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create train set and test set\n",
    "train_set_size = 5000\n",
    "data = np.array(hist)\n",
    "train_data = data[:train_set_size]\n",
    "test_data = data[train_set_size:]\n",
    "x_train = data[:train_set_size,:-1]\n",
    "y_train = data[:train_set_size,-1]\n",
    "x_test = data[train_set_size:,:-1]\n",
    "y_test = data[train_set_size:,-1]\n",
    "x_train = torch.from_numpy(x_train).type(torch.Tensor)\n",
    "x_test = torch.from_numpy(x_test).type(torch.Tensor)\n",
    "y_train = torch.from_numpy(y_train).type(torch.Tensor)\n",
    "y_test = torch.from_numpy(y_test).type(torch.Tensor)\n",
    "\n",
    "# We could try this as it might be a bit easier to use\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_tr, x_val, y_tr, y_val = train_test_split(np.array(data_to_be_added),np.array(y),stratify=y,test_size = 0.2,random_state=777,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Dense, Dropout, Flatten, Conv1D, Input, MaxPooling1D\n",
    "# from keras.models import Model\n",
    "# from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K.clear_session()\n",
    "\n",
    "# inputs = Input(shape=(8000,1))\n",
    "\n",
    "# #First Conv1D layer\n",
    "# conv = Conv1D(8,13, padding='valid', activation='relu', strides=1)(inputs)\n",
    "# conv = MaxPooling1D(3)(conv)\n",
    "# conv = Dropout(0.3)(conv)\n",
    "\n",
    "# #Second Conv1D layer\n",
    "# conv = Conv1D(16, 11, padding='valid', activation='relu', strides=1)(conv)\n",
    "# conv = MaxPooling1D(3)(conv)\n",
    "# conv = Dropout(0.3)(conv)\n",
    "\n",
    "# #Third Conv1D layer\n",
    "# conv = Conv1D(32, 9, padding='valid', activation='relu', strides=1)(conv)\n",
    "# conv = MaxPooling1D(3)(conv)\n",
    "# conv = Dropout(0.3)(conv)\n",
    "\n",
    "# #Flatten layer\n",
    "# conv = Flatten()(conv)\n",
    "\n",
    "# #Dense Layer 1\n",
    "# conv = Dense(128, activation='relu')(conv)\n",
    "# conv = Dropout(0.3)(conv)\n",
    "\n",
    "# # Output layer\n",
    "# outputs = Dense(len(labels), activation='softmax')(conv)\n",
    "\n",
    "# model = Model(inputs, outputs)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "# es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, min_delta=0.0001) \n",
    "# mc = ModelCheckpoint('best_model.hdf5', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(x_tr, y_tr ,epochs=10, callbacks=[es,mc], batch_size=32, validation_data=(x_val,y_val))\n",
    "\n",
    "print(\"MODEL TRAINING COMPLETE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Visualization\n",
    "\n",
    "Now we can take a look at how successful our model is and can easily find where overfitting takes place (if at all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(history.history['loss'], label='train') \n",
    "plt.plot(history.history['val_loss'], label='test') \n",
    "plt.legend() \n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='train') \n",
    "plt.plot(history.history['val_accuracy'], label='test') \n",
    "plt.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the application part (maybe a separate file), we can use our model to predict the future performance\n",
    "\n",
    "from keras.models import load_model\n",
    "model=load_model('best_model.hdf5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
